@misc{AmatRodrigo2017,
author = {{Amat Rodrigo}, Joaqu{\'{i}}n},
title = {{An{\'{a}}lisis de Componentes Principales (Principal Component Analysis, PCA) y t-SNE}},
year = {2017}
}
@article{Barchiesi2015,
abstract = {In this article, we present an account of the state of the art in acoustic scene classification (ASC), the task of classifying environments from the sounds they produce. Starting from a historical review of previous research in this area, we define a general framework for ASC and present different implementations of its components. We then describe a range of different algorithms submitted for a data challenge that was held to provide a general and fair benchmark for ASC techniques. The data set recorded for this purpose is presented along with the performance metrics that are used to evaluate the algorithms and statistical significance tests to compare the submitted methods.},
author = {Barchiesi, Daniele and Giannoulis, D. Dimitrios and Stowell, Dan and Plumbley, Mark D.},
doi = {10.1109/MSP.2014.2326181},
issn = {10535888},
journal = {IEEE Signal Processing Magazine},
title = {{Acoustic Scene Classification: Classifying environments from the sounds they produce}},
year = {2015}
}
@misc{Scikit-learna,
author = {Scikit-learn},
title = {{Cross-validation: evaluating estimator performance}}
}
@misc{Fagerlund2017,
author = {Fagerlund, Eemi and Hiltunen, Aku},
title = {{TUT Rare sound events}},
year = {2017}
}
@inproceedings{Aytar2016,
abstract = {We learn rich natural sound representations by capitalizing on large amounts of unlabeled sound data collected in the wild. We leverage the natural synchronization between vision and sound to learn an acoustic representation using two-million unlabeled videos. Unlabeled video has the advantage that it can be economically acquired at massive scales, yet contains useful signals about natural sound. We propose a student-teacher training procedure which transfers discriminative visual knowledge from well established visual recognition models into the sound modality using unlabeled video as a bridge. Our sound representation yields significant performance improvements over the state-of-the-art results on standard benchmarks for acoustic scene/object classification. Visualizations suggest some high-level semantics automatically emerge in the sound network, even though it is trained without ground truth labels.},
archivePrefix = {arXiv},
arxivId = {1610.09001},
author = {Aytar, Yusuf and Vondrick, Carl and Torralba, Antonio},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1610.09001},
issn = {10495258},
title = {{SoundNet: Learning sound representations from unlabeled video}},
year = {2016}
}
@misc{Heise1999,
author = {Heise, Lori and Ellsberg, Mary and Gottemoeller, Megan},
booktitle = {Population reports. Series L, Issues in world health},
doi = {10.4324/9780429269516-5},
issn = {01975838},
pmid = {11056940},
title = {{Ending violence against women.}},
year = {1999}
}
@article{Kaski2011,
abstract = {Dimensionality reduction is one of the basic operations in the toolbox of data analysts and designers of machine learning and pattern recognition systems. Given a large set of measured variables but few observations, an obvious idea is to reduce the degrees of freedom in the measurements by representing them with a smaller set of more condensed variables. Another reason for reducing the dimensionality is to reduce computational load in further processing. A third reason is visualization. {\textcopyright} 2006 IEEE.},
author = {Kaski, Samuel and Peltonen, Jaakko},
doi = {10.1109/MSP.2010.940003},
issn = {10535888},
journal = {IEEE Signal Processing Magazine},
keywords = {Data models,Data visualization,Information retrieval,Machine learning,Manifolds,Probabilistic logic,Visualization},
title = {{Dimensionality reduction for data visualization}},
year = {2011}
}
@inproceedings{Giannakopoulos2010,
abstract = {In this paper we present our research towards the detection of violent scenes in movies, employing fusion methodologies, based on learning. Towards this goal, a multi-step approach is followed: initially, automated auditory and visual processing and analysis is performed in order to estimate probabilistic measures regarding particular audio and visual related classes. At a second stage, a meta-classification architecture is adopted, which combines the audio and visual information, in order to classify mid-term video segments as "violent" or "non-violent". The proposed scheme has been evaluated on a real dataset from 10 films. {\textcopyright} Springer-Verlag Berlin Heidelberg 2010.},
author = {Giannakopoulos, Theodoros and Makris, Alexandros and Kosmopoulos, Dimitrios and Perantonis, Stavros and Theodoridis, Sergios},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-12842-4_13},
isbn = {3642128416},
issn = {03029743},
keywords = {Multi-modal video classification,Violence detection},
title = {{Audio-visual fusion for detecting violent scenes in videos}},
year = {2010}
}
@book{Wang2006,
abstract = {How can we engineer systems capable of “cocktail party” listening? Human listeners are able to perceptually segregate one sound source from an acoustic mixture, such as a single voice from a mixture of other voices and music at a busy cocktail party. How can we engineer “machine listening” systems that achieve this perceptual feat? Albert Bregmans book Auditory Scene Analysis, published in 1990, drew an analogy between the perception of auditory scenes and visual scenes, and described a coherent framework for understanding the perceptual organization of sound. His account has stimulated much interest in computational studies of hearing. Such studies are motivated in part by the demand for practical sound separation systems, which have many applications including noiserobust automatic speech recognition, hearing prostheses, and automatic music transcription. This emerging field has become known as computational auditory scene analysis (CASA). Computational Auditory Scene Analysis: Principles, Algorithms, and Applications provides a comprehensive and coherent account of the state of the art in CASA, in terms of the underlying principles, the algorithms and system architectures that are employed, and the potential applications of this exciting new technology. With a Foreword by Bregman, its chapters are written by leading researchers and cover a wide range of topics including: Estimation of multiple fundamental frequenciesFeaturebased and modelbased approaches to CASASound separation based on spatial locationProcessing for reverberant environmentsSegregation of speech and musical signalsAutomatic speech recognition in noisy environmentsNeural and perceptual modeling of auditory organizationThe text is written at a level that will be accessible to graduate students and researchers from related science and engineering disciplines. The extensive bibliography accompanying each chapter will also make this book a valuable reference source. A web site accompanying the text, http://www.casabook.org, features software tools and sound demonstrations.},
author = {Wang, Deliang and Brown, Guy J.},
booktitle = {Computational Auditory Scene Analysis: Principles, Algorithms, and Applications},
doi = {10.1109/9780470043387},
isbn = {0470043385},
title = {{Computational auditory scene analysis: Principles, algorithms, and applications}},
year = {2006}
}
@misc{VideoUnderstandingGroup2017,
author = {{Video Understanding Group}},
title = {{YouTube-8M}},
year = {2017}
}
@article{GoogleResearch2015,
abstract = {TensorFlow [1] is an interface for expressing machine learn-ing algorithms, and an implementation for executing such al-gorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of hetero-geneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learn-ing systems into production across more than a dozen areas of computer science and other fields, including speech recogni-tion, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the Ten-sorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
archivePrefix = {arXiv},
arxivId = {arXiv:1603.04467v2},
author = {GoogleResearch},
eprint = {arXiv:1603.04467v2},
journal = {Google Research},
title = {{TensorFlow: Large-scale machine learning on heterogeneous systems}},
year = {2015}
}
@misc{Mapell2012,
author = {Mapell, Val{\'{e}}rie},
title = {{UPC-TALP database of isolated meeting-room acoustic events}},
url = {http://catalog.elra.info/en-us/repository/browse/ELRA-S0268/},
year = {2012}
}
@article{Bogdanov2011,
abstract = {Measuring music similarity is essential for multimedia retrieval. For music items, this task can be regarded as obtaining a suitable distance measurement between songs defined on a certain feature space. In this paper, we propose three of such distance measures based on the audio content: first, a low-level measure based on tempo-related description; second, a high-level semantic measure based on the inference of different musical dimensions by support vector machines. These dimensions include genre, culture, moods, instruments, rhythm, and tempo annotations. Third, a hybrid measure which combines the above-mentioned distance measures with two existing low-level measures: a Euclidean distance based on principal component analysis of timbral, temporal, and tonal descriptors, and a timbral distance based on single Gaussian Mel-frequency cepstral coefficient (MFCC) modeling. We evaluate our proposed measures against a number of baseline measures. We do this objectively based on a comprehensive set of music collections, and subjectively based on listeners' ratings. Results show that the proposed methods achieve accuracies comparable to the baseline approaches in the case of the tempo and classifier-based measures. The highest accuracies are obtained by the hybrid distance. Furthermore, the proposed classifier-based approach opens up the possibility to explore distance measures that are based on semantic notions. {\textcopyright} 2011 IEEE.},
author = {Bogdanov, Dmitry and Serr{\`{a}}, Joan and Wack, Nicolas and Herrera, Perfecto and Serra, Xavier},
doi = {10.1109/TMM.2011.2125784},
issn = {15209210},
journal = {IEEE Transactions on Multimedia},
keywords = {Distance measurement,information retrieval,knowledge acquisition,multimedia computing,multimedia databases,music},
title = {{Unifying low-level and high-level music similarity measures}},
year = {2011}
}
@article{Wattenberg2016,
author = {Wattenberg, Martin and Viegas, Fernanda and Johnson, Ian},
doi = {10.23915/distill.00002},
journal = {Distill},
title = {{How to Use t-SNE Effectively}},
url = {http://distill.pub/2016/misread-tsne},
year = {2016}
}
@misc{Olah2015,
author = {Olah, Christopher},
title = {{Understanding LSTM Networks}},
url = {https://colah.github.io/posts/2015-08-Understanding-LSTMs/},
year = {2015}
}
@inproceedings{Hearst1992,
abstract = {We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexicosyntactic patterns that are easily recognizable, that occur frequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest that other lexical relations will also be acquirable in this way. A subset of the acquisition algorithm is implemented and the results are used to augment and critique the structure of a large hand-built thesaurus. Extensions and applications to areas such as information retrieval are suggested. 1 Introduction  Currently there is much interest in the automatic acquisition of lexical syntax and semantics, with the goal of building up large lexicons for natural language processing. Projects ...},
author = {Hearst, Marti A.},
doi = {10.3115/992133.992154},
title = {{Automatic acquisition of hyponyms from large text corpora}},
year = {1992}
}
@inproceedings{Hinz2016,
author = {Hinz, Tobias and Barros, Pablo and Wermter, Stefan},
doi = {10.1007/978-3-319-44781-0_10},
pages = {80--87},
title = {{The Effects of Regularization on Learning Facial Expressions with Convolutional Neural Networks}},
year = {2016}
}
@article{Beyer2015,
abstract = {Intimate partner violence (IPV) is an important global public health problem, affecting women across the life span and increasing risk for a number of unfavorable health outcomes. Typically conceptualized as a private form of violence, most research has focused on individual-level risk markers. Recently, more scholarly attention has been paid to the role that the residential neighborhood environment may play in influencing the occurrence of IPV. With research accumulating since the 1990s, increasing prominence of the topic, and no comprehensive literature reviews yet undertaken, it is time to take stock of what is known, what remains unknown, and the methods and concepts investigators have considered. In this article, we undertake a comprehensive, systematic review of the literature to date on the relationship between neighborhood environment and IPV, asking, “what is the status of scholarship related to the association between neighborhood environment and IPV occurrence?” Although the literature is young, it is receiving increasing attention from researchers in sociology, public health, criminology, and other fields. Obvious gaps in the literature include limited consideration of nonurban areas, limited theoretical motivation, and limited consideration of the range of potential contributors to environmental effects on IPV—such as built environmental factors or access to services. In addition, explanations of the pathways by which place influences the occurrence of IPV draw mainly from social disorganization theory that was developed in urban settings in the United States and may need to be adapted, especially to be useful in explaining residential environmental correlates of IPV in rural or non-U.S. settings. A more complete theoretical understanding of the relationship between neighborhood environment and IPV, especially considering differences among urban, semiurban, and rural settings and developed and developing country settings, will be necessary to advance research questions and improve policy and intervention responses to reduce the burden of IPV.},
author = {Beyer, Kirsten and Wallis, Anne Baber and Hamberger, L. Kevin},
doi = {10.1177/1524838013515758},
issn = {15528324},
journal = {Trauma, Violence, and Abuse},
keywords = {community violence,cultural contexts,domestic violence},
title = {{Neighborhood Environment and Intimate Partner Violence: A Systematic Review}},
year = {2015}
}
@article{Bahoura2010,
abstract = {Our understanding of etiology of obesity and overweight is incomplete due to lack of objective and accurate methods for monitoring of ingestive behavior (MIB) in the free-living population. Our research has shown that frequency of swallowing may serve as a predictor for detecting food intake, differentiating liquids and solids, and estimating ingested mass. This paper proposes and compares two methods of acoustical swallowing detection from sounds contaminated by motion artifacts, speech, and external noise. Methods based on mel-scale Fourier spectrum, wavelet packets, and support vector machines are studied considering the effects of epoch size, level of decomposition, and lagging on classification accuracy. The methodology was tested on a large dataset (64.5 h with a total of 9966 swallows) collected from 20 human subjects with various degrees of adiposity. Average weighted epoch-recognition accuracy for intravisit individual models was 96.8{\%}, which resulted in 84.7{\%} average weighted accuracy in detection of swallowing events. These results suggest high efficiency of the proposed methodology in separation of swallowing sounds from artifacts that originate from respiration, intrinsic speech, head movements, food ingestion, and ambient noise. The recognition accuracy was not related to body mass index, suggesting that the methodology is suitable for obese individuals. {\textcopyright} 2006 IEEE.},
author = {Sazonov, Edward S. and Makeyev, Oleksandr and Schuckers, Stephanie and Lopez-Meyer, Paulo and Melanson, Edward L. and Neuman, Michael R.},
doi = {10.1109/TBME.2009.2033037},
issn = {00189294},
journal = {IEEE Transactions on Biomedical Engineering},
keywords = {Biomedical signal processing,Obesity,Pattern recognition,Swallowing,Wearable devices},
title = {{Automatic detection of swallowing events by acoustical means for applications of monitoring of ingestive behavior}},
year = {2010}
}
@article{Beranek1950,
author = {Beranek, Leo L. and Rosenblith, Walter A.},
doi = {10.1063/1.3066788},
issn = {0031-9228},
journal = {Physics Today},
title = {{                            Acoustic Measurements                          }},
year = {1950}
}
@inproceedings{Grill2012,
abstract = {This paper describes the construction of computable audio descriptors capable of modeling relevant high-level perceptual qualities of textural sounds. These qualities - all metaphoric bipolar and continuous constructs - have been identified in previous research: high-low, ordered-chaotic, smooth-coarse, tonal-noisy, and homogeneous-heterogeneous, covering timbral, temporal and structural properties of sound. We detail the construction of the descriptors and demonstrate the effects of tuning with respect to individual accuracy or mutual independence. The descriptors are evaluated on a corpus of 100 textural sounds against respective measures of human perception that have been retrieved by use of an online survey. Potential future use of perceptual audio descriptors in music creation is illustrated by a prototypic sound browser application. {\textcopyright} 2012 Thomas Grill et al.},
author = {Grill, Thomas},
booktitle = {Proceedings of the 9th Sound and Music Computing Conference, SMC 2012},
isbn = {9783832531805},
title = {{Constructing high-level perceptual audio descriptors for textural sounds}},
year = {2012}
}
@misc{M2018,
author = {M, Sanjay},
title = {{Why and how to Cross Validate a Model?}},
url = {https://towardsdatascience.com/why-and-how-to-cross-validate-a-model-d6424b45261f},
year = {2018}
}
@inproceedings{Mesaros2010,
abstract = {This paper presents a system for acoustic event detection in recordings from real life environments. The events are modeled using a network of hidden Markov models; their size and topology is chosen based on a study of isolated events recognition. We also studied the effect of ambient background noise on event classification performance. On real life recordings, we tested recognition of isolated sound events and event detection. For event detection, the system performs recognition and temporal positioning of a sequence of events. An accuracy of 24{\%} was obtained in classifying isolated sound events into 61 classes. This corresponds to the accuracy of classifying between 61 events when mixed with ambient background noise at 0dB signal-to-noise ratio. In event detection, the system is capable of recognizing almost one third of the events, and the temporal positioning of the events is not correct for 84{\%} of the time. {\textcopyright} EURASIP, 2010.},
author = {Mesaros, Annamaria and Heittola, Toni and Eronen, Antti and Virtanen, Tuomas},
booktitle = {European Signal Processing Conference},
issn = {22195491},
title = {{Acoustic event detection in real life recordings}},
year = {2010}
}
@misc{Cakir2016,
author = {Cakir, Emre and Heittola, Toni},
title = {{TUT-SED Synthetic}},
year = {2016}
}
@article{Pachet2009,
abstract = {We present a feature generation system designed to create audio features for supervised classification tasks. The main contribution to feature generation studies is the notion of analytical features (AFs), a construct designed to support the representation of knowledge about audio signal processing. We describe the most important aspects of AFs, in particular their dimensional type system, on which are based pattern-based random generators, heuristics, and rewriting rules. We show how AFs generalize or improve previous approaches used in feature generation. We report on several projects using AFs for difficult audio classification tasks, demonstrating their advantage over standard audio features. More generally, we propose analytical features as a paradigm to bring raw signals into the world of symbolic computation.},
author = {Pachet, Fran{\c{c}}ois and Roy, Pierre},
doi = {10.1155/2009/153017},
issn = {16874714},
journal = {Eurasip Journal on Audio, Speech, and Music Processing},
title = {{Analytical features: A knowledge-based approach to audio feature generation}},
year = {2009}
}
@inproceedings{Gemmeke2017,
abstract = {Audio event recognition, the human-like ability to identify and re-late sounds from audio, is a nascent problem in machine percep-tion. Comparable problems such as object detection in images have reaped enormous benefits from comprehensive datasets – principally ImageNet. This paper describes the creation of Audio Set, a large-scale dataset of manually-annotated audio events that endeavors to bridge the gap in data availability between image and audio research. Using a carefully structured hierarchical ontology of 632 audio classes guided by the literature and manual curation, we collect data from human labelers to probe the presence of specific audio classes in 10 second segments of YouTube videos. Segments are proposed for labeling using searches based on metadata, context (e.g., links), and content analysis. The result is a dataset of unprecedented breadth and size that will, we hope, substantially stimulate the de-velopment of high-performance audio event recognizers.},
author = {Gemmeke, Jort F. and Ellis, Daniel P.W. and Freedman, Dylan and Jansen, Aren and Lawrence, Wade and Moore, R. Channing and Plakal, Manoj and Ritter, Marvin},
booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
doi = {10.1109/ICASSP.2017.7952261},
isbn = {9781509041176},
issn = {15206149},
keywords = {Audio event detection,audio databases,data collection,sound ontology},
title = {{Audio Set: An ontology and human-labeled dataset for audio events}},
year = {2017}
}
@article{VanDerMaaten2008,
abstract = {We present a new technique called "t-SNE" that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.},
author = {{Van Der Maaten}, Laurens and Hinton, Geoffrey},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {Dimensionality reduction,Embedding algorithms,Manifold learning,Multidimensional scaling,Visualization},
title = {{Visualizing data using t-SNE}},
year = {2008}
}
@misc{Kaw,
author = {Kaw, Autar},
title = {{Holistic Numerical Methods}},
url = {https://nm.mathforcollege.com/blog/}
}
@inproceedings{Fonseca2017,
abstract = {Openly available datasets are a key factor in the advancement of data-driven research approaches, including many of the ones used in sound and music computing. In the last few years, quite a number of new audio datasets have been made available but there are still major shortcomings in many of them to have a significant research impact. Among the common shortcomings are the lack of transparency in their creation and the difficulty of making them completely open and sharable. They often do not include clear mechanisms to amend errors and many times they are not large enough for current machine learning needs. This paper introduces Freesound Datasets, an online platform for the collaborative creation of open audio datasets based on principles of transparency, openness, dynamic character, and sustainability. As a proof-of-concept, we present an early snapshot of a large-scale audio dataset built using this platform. It consists of audio samples from Freesound organised in a hierarchy based on the AudioSet Ontology. We believe that building and maintaining datasets following the outlined principles and using open tools and collaborative approaches like the ones presented here will have a significant impact in our research community.},
author = {Fonseca, Eduardo and Pons, Jordi and Favory, Xavier and Font, Frederic and Bogdanov, Dmitry and Ferraro, Andres and Oramas, Sergio and Porter, Alastair and Serra, Xavier},
booktitle = {Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017},
isbn = {9789811151798},
title = {{Freesound datasets: A platform for the creation of open audio datasets}},
year = {2017}
}
@inproceedings{Geiger2013,
abstract = {This work describes a system for acoustic scene classification using large-scale audio feature extraction. It is our contribution to the Scene Classification track of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (D-CASE). The system classifies 30 second long recordings of 10 different acoustic scenes. From the highly variable recordings, a large number of spectral, cepstral, energy and voicing-related audio features are extracted. Using a sliding window approach, classification is performed on short windows. SVM are used to classify these short segments, and a majority voting scheme is employed to get a decision for longer recordings. On the official development set of the challenge, an accuracy of 73 {\%} is achieved. SVM are compared with a nearest neighbour classifier and an approach called Latent Perceptual Indexing, whereby SVM achieve the best results. A feature analysis using the t-statistic shows that mainly Mel spectra are the most relevant features. {\textcopyright} 2013 IEEE.},
author = {Geiger, Jurgen T. and Schuller, Bjorn and Rigoll, Gerhard},
booktitle = {IEEE Workshop on Applications of Signal Processing to Audio and Acoustics},
doi = {10.1109/WASPAA.2013.6701857},
isbn = {9781479909728},
keywords = {Computational auditory scene analysis,acoustic scene recognition,feature extraction},
title = {{Large-scale audio feature extraction and SVM for acoustic scene classification}},
year = {2013}
}
@book{Kwon2011,
abstract = {An artificial neural network (ANN) is a type of artificial intelligence technology which implements more complex data-analysis features into existing applications by an intelligent, human-like application of knowledge. ANN can be considered as a mathematical or computational model based on biological (brain) neural networks. ANN is an adaptive system that changes its structure based on external or internal information that is processed within the network during the learning stage. ANNs implement algorithms that attempt to achieve neurologically-related processes and performances such as learning from experience, making generalizations from similar situations and judging states where poor results were achieved in the past. This new and important book gathers the most current research from across the globe in the study of artificial neural networks. {\textcopyright} 2011 by Nova Science Publishers, Inc. All rights reserved.},
author = {Kwon, Seoyun J.},
booktitle = {Artificial Neural Networks},
doi = {10.4324/9781315154282-3},
isbn = {9781617615535},
issn = {0272-989X},
title = {{Artificial neural networks}},
year = {2011}
}
@article{Blanco2004,
abstract = {La gran magnitud de la violencia contra las mujeres llev� a que la Organizaci�n Mundial de la Salud la declarara como un problema prioritario en salud p�blica. Seg�n los datos de la macroencuesta realizada por el Instituto de la Mujer en 1999, este problema est� afectando en Espa�a a una de cada 7 mujeres y da lugar a m�s de medio centenar de muertes cada a�o. Este trabajo tiene como objetivo hacer una revisi�n del origen de sus causas, las consecuencias en la salud de las mujeres y su impacto en los servicios sanitarios. Se analiza por qu� las mujeres maltratadas, a pesar de su alta prevalencia, no son reconocidas habitualmente por los profesionales sanitarios, tanto en las consultas como en los servicios de urgencias. Se revisan las acciones que se han puesto en marcha en los �ltimos a�os en Espa�a desde la Administraci�n sanitaria. Se proponen algunas recomendaciones respecto a las pol�ticas sanitarias y sociales, el papel de los profesionales y la formaci�n e investigaci�n necesarias para avanzar en la erradicaci�n de esta lacra social.},
author = {Blanco, Pilar and Ruiz-Jarabo, Consuelo and {Garc{\'{i}}a de Vinuesa}, Leonor and Mart{\'{i}}n-Garc{\'{i}}a, Mar},
doi = {10.1157/13062524},
issn = {02139111},
journal = {Gaceta Sanitaria},
title = {{La violencia de pareja y la salud de las mujeres}},
year = {2004}
}
@inproceedings{Eronen2006,
abstract = {The aim of this paper is to investigate the feasibility of an audio-based context recognition system. Here, context recognition refers to the automatic classification of the context or an environment around a device. A system is developed and compared to the accuracy of human listeners in the same task. Particular emphasis is placed on the computational complexity of the methods, since the application is of particular interest in resource-constrained portable devices. Simplistic low-dimensional feature vectors are evaluated against more standard spectral features. Using discriminative training, competitive recognition accuracies are achieved with very low-order hidden Markov models (1-3 Gaussian components). Slight improvement in recognition accuracy is observed when linear data-driven feature transformations are applied to mel-cepstral features. The recognition rate of the system as a function of the test sequence length appears to converge only after about 30 to 60 s. Some degree of accuracy can be achieved even with less than 1-s test sequence lengths. The average reaction time of the human listeners was 14 s, i.e., somewhat smaller, but of the same order as that of the system. The average recognition accuracy of the system was 58{\%} against 69{\%}, obtained in the listening tests in recognizing between 24 everyday contexts. The accuracies in recognizing six high-level classes were 82{\%} for the system and 88{\%} for the subjects. {\textcopyright} 2006 IEEE.},
author = {Eronen, Antti J. and Peltonen, Vesa T. and Tuomi, Juha T. and Klapuri, Anssi P. and Fagerlund, Seppo and Sorsa, Timo and Lorho, Ga{\"{e}}tan and Huopaniemi, Jyri},
booktitle = {IEEE Transactions on Audio, Speech and Language Processing},
doi = {10.1109/TSA.2005.854103},
issn = {15587916},
keywords = {Audio classification,Context awareness,Feature extraction,Hidden markov models (hmms)},
title = {{Audio-based context recognition}},
year = {2006}
}
@misc{Browniee2018a,
author = {Browniee, Jason},
title = {{Difference Between a Batch and an Epoch in a Neural Network}},
url = {https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/},
year = {2018}
}
@misc{Levoy2012,
author = {Levoy, Marc and Dektar, Katie and Adams, Andrew},
title = {{Spatial Convolution}},
url = {https://graphics.stanford.edu/courses/cs178/applets/convolution.html},
year = {2012}
}
@misc{SoundUnderstandinggroup2017,
author = {{Sound Understanding group}},
title = {{AudioSet}},
year = {2017}
}
@inproceedings{Salamon2017,
abstract = {Sound event detection (SED) in environmental recordings is a key topic of research in machine listening, with applications in noise monitoring for smart cities, self-driving cars, surveillance, bioa-coustic monitoring, and indexing of large multimedia collections. Developing new solutions for SED often relies on the availability of strongly labeled audio recordings, where the annotation includes the onset, offset and source of every event. Generating such precise annotations manually is very time consuming, and as a result existing datasets for SED with strong labels are scarce and limited in size. To address this issue, we present Scaper, an open-source library for soundscape synthesis and augmentation. Given a collection of iso-lated sound events, Scaper acts as a high-level sequencer that can generate multiple soundscapes from a single, probabilistically defined, 'specification'. To increase the variability of the output, Scaper supports the application of audio transformations such as pitch shifting and time stretching individually to every event. To illustrate the potential of the library, we generate a dataset of 10,000 sound-scapes and use it to compare the performance of two state-of-The-Art algorithms, including a breakdown by soundscape characteristics. We also describe how Scaper was used to generate audio stimuli for an audio labeling crowdsourcing experiment, and conclude with a discussion of Scaper's limitations and potential applications.},
author = {Salamon, Justin and MacConnell, Duncan and Cartwright, Mark and Li, Peter and Bello, Juan Pablo},
booktitle = {IEEE Workshop on Applications of Signal Processing to Audio and Acoustics},
doi = {10.1109/WASPAA.2017.8170052},
isbn = {9781538616321},
keywords = {Soundscape,sound event detection,synthesis},
title = {{Scaper: A library for soundscape synthesis and augmentation}},
year = {2017}
}
@misc{Mahmood2018,
author = {Mahmood, Hamza},
title = {{The Softmax Function, Simplified}},
year = {2018}
}
@inproceedings{Jiang2005,
abstract = {Audio scene classification is very important in audio indexing, retrieval and video content analysis. In this paper we present our approach that uses support vector machine (SVM) for audio scene classification, which classifies audio clips into one of five classes: pure speech, non-pure speech, music, environment sound, and silence. Among of them, non-pure speech may further be divided into speech with music and speech with noise. We also describe two methods to select effective and robust audio feature sets. Based on these feature sets, we have evaluated and compared the performance of two kinds of classification frameworks on a testing database that is composed of about 4-hour audio data. The experimental results have shown that the SVM-based method yields high accuracy with high processing speed. {\textcopyright} 2005 IEEE.},
author = {Jiang, Hongchen and Bai, Junmei and Zhang, Shuwu and Xu, Bo},
booktitle = {Proceedings of 2005 IEEE International Conference on Natural Language Processing and Knowledge Engineering, IEEE NLP-KE'05},
doi = {10.1109/NLPKE.2005.1598721},
isbn = {0780393619},
title = {{SVM-based audio scene classification}},
year = {2005}
}
@article{Marr1982,
abstract = {A text which approaches vision through an information processing framework, with particular attention to the computer processing of visual information. Machine information processing necessitates both a study of computer facilities and of human information processing. An introductory section outlines the philosophical approach using representational theories of the mind. Vision is then studied using theories of representation for images and surfaces, shapes and pattern recognition. Algorithmic approaches and theoretical models are discussed. A final chapter details a 'conversation' about the material undertaken by the author and colleagues. -M.Blakemore},
author = {Marr, D.},
doi = {10.1016/0022-2496(83)90030-5},
isbn = {0716712849},
issn = {00222496},
journal = {Vision: a computational investigation into the human representation and processing of visual information.},
title = {{Vision: a computational investigation into the human representation and processing of visual information.}},
year = {1982}
}
@misc{Stowell2013,
author = {Stowell, Dan and Benetos, Emmanouil},
title = {{IEEE AASP Challenge: Detection and Classification of Acoustic Scenes and Events}},
year = {2013}
}
@misc{Saha2018,
author = {Saha, Sumit},
title = {{No A Comprehensive Guide to Convolutional Neural Networks — the ELI5 way}},
year = {2018}
}
@misc{UnitedNations1989,
author = {{United Nations}},
pages = {11},
title = {{No Title}},
year = {1989}
}
@misc{Abdi2010,
abstract = {Principal component analysis (PCA) is amultivariate technique that analyzes a data table in which observations are described by several inter-correlated quantitative dependent variables. Its goal is to extract the important information from the table, to represent it as a set of new orthogonal variables called principal components, and to display the pattern of similarity of the observations and of the variables as points in maps. The quality of the PCA model can be evaluated using cross-validation techniques such as the bootstrap and the jackknife. PCA can be generalized as correspondence analysis (CA) in order to handle qualitative variables and as multiple factor analysis (MFA) in order to handle heterogeneous sets of variables. Mathematically, PCA depends upon the eigen-decomposition of positive semidefinite matrices and upon the singular value decomposition (SVD) of rectangular matrices. {\textcopyright} 2010 John Wiley {\&} Sons, Inc.},
author = {Abdi, Herv{\'{e}} and Williams, Lynne J.},
booktitle = {Wiley Interdisciplinary Reviews: Computational Statistics},
doi = {10.1002/wics.101},
issn = {19395108},
title = {{Principal component analysis}},
year = {2010}
}
@incollection{Temko2009,
abstract = {The human activity that takes place in meeting-rooms or class-rooms is reflected in a rich variety of acoustic events, either produced by the human body or by objects handled by humans, so the determination of both the identity of sounds and their position in time may help to detect and describe that human activity. Additionally, detection of sounds other than speech may be useful to enhance the robustness of speech technologies like automatic speech recognition. Automatic detection and classification of acoustic events is the objective of this thesis work. It aims at processing the acoustic signals collected by distant microphones in meeting-room or classroom environments to convert them into symbolic descriptions corresponding to a listener's perception of the different sound events that are present in the signals and their sources. First of all, the task of acoustic event classification is faced using Support Vector Machine (SVM) classifiers, which are motivated by the scarcity of training data. A confusion-matrix-based variable-feature-set clustering scheme is developed for the multiclass recognition problem, and tested on the gathered database. With it, a higher classification rate than the GMM-based technique is obtained, arriving to a large relative average error reduction with respect to the best result from the conventional binary tree scheme. Moreover, several ways to extend SVMs to sequence processing are compared, in an attempt to avoid the drawback of SVMs when dealing with audio data, i.e. their restriction to work with fixed-length vectors, observing that the dynamic time warping kernels work well for sounds that show a temporal structure. Furthermore, concepts and tools from the fuzzy theory are used to investigate, first, the importance of and degree of interaction among features, and second, ways to fuse the outputs of several classification systems. The developed AEC systems are tested also by participating in several international evaluations from 2004 to 2006, and the results are reported. The second main contribution of this thesis work is the development of systems for detection of acoustic events. The detection problem is more complex since it includes both classification and determination of the time intervals where the sound takes place. Two system versions are developed and tested on the datasets of the two CLEAR international evaluation campaigns in 2006 and 2007. Two kinds of databases are used: two databases of isolated acoustic events, and a database of interactive seminars containing a significant number of acoustic events of interest. Our developed systems, which consist of SVM-based classification within a sliding window plus post-processing, were the only submissions not using HMMs, and each of them obtained competitive results in the corresponding evaluation. Speech activity detection was also pursued in this thesis since, in fact, it is a – especially important – particular case of acoustic event detection. An enhanced SVM training approach for the speech activity detection task is developed, mainly to cope with the problem of dataset reduction. The resulting SVM-based system is tested with several NIST Rich Transcription (RT) evaluation datasets, and it shows better scores than our GMM-based system, which ranked among the best systems in the RT06 evaluation. Finally, it is worth mentioning a few side outcomes from this thesis work. As it has been carried out in the framework of the CHIL EU project, the author has been responsible for the organization of the above mentioned international evaluations in acoustic event classification and detection, taking a leading role in the specification of acoustic event classes, databases, and evaluation protocols, and, especially, in the proposal and implementation of the various metrics that have been used. Moreover, the detection systems have been implemented in the UPC's smart-room and work in real time for purposes of testing and demonstration.},
author = {Temko, Andrey and Nadeu, Climent and Macho, Du{\v{s}}an and Malkin, Robert and Zieger, Christian and Omologo, Maurizio},
booktitle = {Computers in the Human Interaction Loop},
chapter = {Part II, 7},
doi = {10.1007/978-1-84882-054-8_7},
title = {{Acoustic Event Detection and Classification}},
year = {2009}
}
@inproceedings{Chua2014,
abstract = {Modern elevators are equipped with closed-circuit television (CCTV) cameras to record videos for post-incident investigation rather than providing proactive event monitoring. While there are some attempts at automated video surveillance, events such as urinating, vandalism, and crimes that involved vulnerable targets may not exhibit significant visual cues. On contrary, such events are more discerning from audio cues. In this work, we propose a hierarchical audio-visual surveillance framework for elevators. Audio analytic module acts as the front line detector to monitor for such events. This means audio cue is the main determining source to infer the event occurrence. The secondary inference process involves queries to visual analytic module to build-up the evidences leading to event detection. We validate the performance of our system at a residential trial site and the initial results are promising. {\textcopyright} 2014 Springer International Publishing.},
author = {Chua, Teck Wee and Leman, Karianto and Gao, Feng},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-04117-9_5},
isbn = {9783319041162},
issn = {03029743},
title = {{Hierarchical audio-visual surveillance for passenger elevators}},
year = {2014}
}
@book{Giannakopoulos2014,
abstract = {Introduction to Audio Analysis serves as a standalone introduction to audio analysis, providing theoretical background to many state-of-the-art techniques. It covers the essential theory necessary to develop audio engineering applications, but also uses programming techniques, notably MATLAB{\textregistered}, to take a more applied approach to the topic. Basic theory and reproducible experiments are combined to demonstrate theoretical concepts from a practical point of view and provide a solid foundation in the field of audio analysis. Audio feature extraction, audio classification, audio segmentation, and music information retrieval are all addressed in detail, along with material on basic audio processing and frequency domain representations and filtering. Throughout the text, reproducible MATLAB{\textregistered} examples are accompanied by theoretical descriptions, illustrating how concepts and equations can be applied to the development of audio analysis systems and components. A blend of reproducible MATLAB{\textregistered} code and essential theory provides enable the reader to delve into the world of audio signals and develop real-world audio applications in various domains. {\textcopyright} 2014 Elsevier Ltd. All rights reserved.},
author = {Giannakopoulos, Theodoros and Pikrakis, Aggelos},
booktitle = {Introduction to Audio Analysis: A MATLAB Approach},
doi = {10.1016/C2012-0-03524-7},
isbn = {9780080993881},
title = {{Introduction to Audio Analysis: A MATLAB Approach}},
year = {2014}
}
@inproceedings{Hershey2017,
abstract = {Convolutional Neural Networks (CNNs) have proven very effective in image classification and show promise for audio. We use various CNN architectures to classify the soundtracks of a dataset of 70M training videos (5.24 million hours) with 30,871 video-level labels. We examine fully connected Deep Neural Networks (DNNs), AlexNet [1], VGG [2], Inception [3], and ResNet [4]. We investigate varying the size of both training set and label vocabulary, finding that analogs of the CNNs used in image classification do well on our audio classification task, and larger training and label sets help up to a point. A model using embeddings from these classifiers does much better than raw features on the Audio Set [5] Acoustic Event Detection (AED) classification task.},
archivePrefix = {arXiv},
arxivId = {1609.09430},
author = {Hershey, Shawn and Chaudhuri, Sourish and Ellis, Daniel P.W. and Gemmeke, Jort F. and Jansen, Aren and Moore, R. Channing and Plakal, Manoj and Platt, Devin and Saurous, Rif A. and Seybold, Bryan and Slaney, Malcolm and Weiss, Ron J. and Wilson, Kevin},
booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
doi = {10.1109/ICASSP.2017.7952132},
eprint = {1609.09430},
isbn = {9781509041176},
issn = {15206149},
keywords = {Acoustic Event Detection,Acoustic Scene Classification,Convolutional Neural Networks,Deep Neural Networks,Video Classification},
title = {{CNN architectures for large-scale audio classification}},
year = {2017}
}
@inproceedings{Henaff2011,
abstract = {In this work we present a system to automatically learn features from audio in an unsupervised manner. Our method first learns an overcomplete dictionary which can be used to sparsely decompose log-scaled spectrograms. It then trains an efficient encoder which quickly maps new inputs to approximations of their sparse representations using the learned dictionary. This avoids expensive iterative procedures usually required to infer sparse codes. We then use these sparse codes as inputs for a linear Support Vector Machine (SVM). Our system achieves 83.4{\%} accuracy in predicting genres on the GTZAN dataset, which is competitive with current state-of-the-art approaches. Furthermore, the use of a simple linear classifier combined with a fast feature extraction system allows our approach to scale well to large datasets. {\textcopyright} 2011 International Society for Music Information Retrieval.},
author = {Henaff, Mikael and Jarrett, Kevin and Kavukcuoglu, Koray and Lecun, Yann},
booktitle = {Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011},
isbn = {9780615548654},
title = {{Unsupervised learning of sparse features for scalable audio classification}},
year = {2011}
}
@misc{Pan2010,
abstract = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research. {\textcopyright} 2006 IEEE.},
author = {Pan, Sinno Jialin and Yang, Qiang},
booktitle = {IEEE Transactions on Knowledge and Data Engineering},
doi = {10.1109/TKDE.2009.191},
issn = {10414347},
keywords = {Transfer learning,data mining.,machine learning,survey},
title = {{A survey on transfer learning}},
year = {2010}
}
@inproceedings{Lee2013,
abstract = {Recently unsupervised learning algorithms have been successfully used to represent data in many of machine recognition tasks. In particular, sparse feature learning algorithms have shown that they can not only discover meaningful structures from raw data but also outperform many hand-engineered features. In this paper, we apply the sparse feature learning approach to acoustic scene classification. We use a sparse restricted Boltzmann machine to capture manyfold local acoustic structures from audio data and represent the data in a high-dimensional sparse feature space given the learned structures. For scene classification, we summarize the local features by pooling over audio scene data. While the feature pooling is typically performed over uniformly divided segments, we suggest a new pooling method, which first detects audio events and then performs pooling only over detected events, considering the irregular occurrence of audio events in acoustic scene data. We evaluate the learned features on the IEEE AASP Challenge development set, comparing them with a baseline model using mel-frequency cepstral coefficients (MFCCs). The results show that learned features outperform MFCCs, event-based pooling achieves higher accuracy than uniform pooling and, furthermore, a combination of the two methods performs even better than either one used alone. {\textcopyright} 2013 IEEE.},
author = {Lee, Kyogu and Hyung, Ziwon and Nam, Juhan},
booktitle = {IEEE Workshop on Applications of Signal Processing to Audio and Acoustics},
doi = {10.1109/WASPAA.2013.6701893},
isbn = {9781479909728},
keywords = {acoustic scene classification,environmental sound,event detection,feature learning,max-pooling,restricted Boltzmann machine,sparse feature representation},
title = {{Acoustic scene classification using sparse feature learning and event-based pooling}},
year = {2013}
}
@article{Van2013,
author = {{Van Nort}, Doug and Oliveros, Pauline and Braasch, Jonas},
journal = {Journal of New Music Research},
number = {4},
pages = {303--324},
title = {{Electro/acoustic improvisation and deeply listening machines}},
volume = {42},
year = {2013}
}
@misc{SuperDataScienceTeam2018,
author = {{SuperDataScience Team}},
title = {{Recurrent Neural Networks (RNN) - The Vanishing Gradient Problem}},
url = {https://www.superdatascience.com/blogs/recurrent-neural-networks-rnn-the-vanishing-gradient-problem/},
year = {2018}
}
@misc{Singhal2012,
author = {Singhal, Amit},
title = {{Introducing the Knowledge Graph: things, not strings}},
year = {2012}
}
@article{Ruder2017,
author = {Ruder, Sebastian},
title = {{Transfer Learning - Machine Learning's Next Frontier}},
year = {2017}
}
@article{Potamitis2014,
abstract = {The primary purpose for pursuing this research is to present a modular approach that enables reliable automatic bird species identification on the basis of their sound emissions in the field. A practical and complete computer-based framework is proposed to detect and time-stamp particular bird species in continuous real field recordings. Acoustic detection of avian sounds can be used for the automatized monitoring of multiple bird taxa and querying in long-term recordings for species of interest for researchers, conservation practitioners, and decision makers, such as environmental indicator taxa and threatened species. This work describes two novel procedures and offers an open modular framework that detects and time-stamps online calls and songs of target bird species and is fast enough to report results in reasonable time for non-processed field recordings of many thousands files and is generic enough to accommodate any species. The framework is evaluated on two large corpora of real field data, targeting the calls and songs of American Robin Turdus migratorius, a Northamerican oscine passerine (true songbird) and the Common Kingfisher Alcedo atthis, a non-passerine species with a wide distribution throughout Eurasia and North Africa. With the aim of promoting the widespread use of digital autonomous recording units (ARUs) and species recognition technologies the processing code and a large corpus of audio recordings is provided in order to enable other researchers to perform and assess comparative experiments. {\textcopyright} 2014 Elsevier Ltd. All rights reserved.},
author = {Potamitis, Ilyas and Ntalampiras, Stavros and Jahn, Olaf and Riede, Klaus},
doi = {10.1016/j.apacoust.2014.01.001},
issn = {0003682X},
journal = {Applied Acoustics},
keywords = {Bird recognition,Birdsong detection,Computational ecology},
title = {{Automatic bird sound detection in long real-field recordings: Applications and tools}},
year = {2014}
}
@book{WHO2013,
abstract = {The report presents the first global systematic review of scientific data on the prevalence of two forms of violence against women: violence by an intimate partner (intimate partner violence) and sexual violence by someone other than a partner (non-partner sexual violence). It shows, for the first time, global and regional estimates of the prevalence of these two forms of violence, using data from around the world. Previous reporting on violence against women has not differentiated between partner and non-partner violence.},
author = {{WHO. Department of Reproductive Health Research. London School of Hygiene and Tropical Medicine. South African Medical Research Council.}},
booktitle = {WHO},
title = {{WHO | Global and regional estimates of violence against women}},
year = {2013}
}
@article{Krug2002,
abstract = {In 1996, the World Health Assembly declared violence a major public health issue. To follow up on this resolution, on Oct 3 this year, WHO released the first World Report on Violence and Health. The report analyses different types of violence including chil abuse and neglect, youth violence, intimate partner violence, sexual violence, elder abuse, self-directed violence, and collective violence. For all these types of violence, the report explores the magnitude of the health and social effects, the risk and protective factors, and the types of prevention efforts that have been initiated. The launch of the report will be followed by a 1-year Global Campaign on Violence Prevention, focusing on implementation of the recommendations. This article summarises some of the main points of the world report.},
author = {Krug, Etienne G. and Mercy, James A. and Dahlberg, Linda L. and Zwi, Anthony B.},
doi = {10.1016/S0140-6736(02)11133-0},
issn = {01406736},
journal = {Lancet},
title = {{The world report on violence and health}},
year = {2002}
}
@inproceedings{Prajapati2010,
abstract = {Support Vector Machines, a new generation learning system based on recent advances in statistical learning theory deliver state-of-the-art performance in real-world applications such as text categorization, hand-written character recognition, image classification, bio-sequence analysis etc for the classification and regression. This paper emphasizes the classification task with Support Vector Machine. It has several kernel functions including linear, polynomial and radial basis for performing classification. Our comparison between polynomial and radial basis kernel functions for selected feature conclude that radial basis function is preferable than polynomial for large datasets. {\textcopyright} 2010 IEEE.},
author = {Prajapati, Gend Lal and Patle, Arti},
booktitle = {Proceedings - 3rd International Conference on Emerging Trends in Engineering and Technology, ICETET 2010},
doi = {10.1109/ICETET.2010.134},
isbn = {9780769542461},
keywords = {Feature,Kernel,RBF,Support vector},
title = {{On performing classification using SVM with radial basis and polynomial kernel functions}},
year = {2010}
}
@inproceedings{Temko2007,
abstract = {In this paper, we present the results of the Acoustic Event Detection (AED) and Classification (AEC) evaluations carried out in February 2006 by the three participant partners from the CHIL project. The primary evaluation task was AED of the testing portions of the isolated sound databases and seminar recordings produced in CHIL. Additionally, a secondary AEC evaluation task was designed using only the isolated sound databases. The set of meetingroom acoustic event classes and the metrics were agreed by the three partners and ELDA was in charge of the scoring task. In this paper, the various systems for the tasks of AED and AEC and their results are presented. {\textcopyright} Springer-Verlag Berlin Heidelberg 2007.},
author = {Temko, Andrey and Malkin, Robert and Zieger, Christian and Macho, Dusan and Nadeu, Climent and Omologo, Maurizio},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-540-69568-4_29},
isbn = {9783540695677},
issn = {03029743},
title = {{CLEAR evaluation of acoustic event detection and classification systems}},
year = {2007}
}
@misc{Karpathy2016,
abstract = {Convolutional Neural Networks are very similar to ordinary Neural Networks from the previous chapter: they are made up of neurons that have learnable weights and biases. Each neuron receives some inputs, performs a dot product and optionally follows it with a non-linearity. The whole network still expresses a single differentiable score function: from the raw image pixels on one end to class scores at the other. And they still have a loss function (e.g. SVM/Softmax) on the last (fully-connected) layer and all the tips/tricks we developed for learning regular Neural Networks still apply. So what does change? ConvNet architectures make the explicit assumption that the inputs are images, which allows us to encode certain properties into the architecture. These then make the forward function more efficient to implement and vastly reduce the amount of parameters in the network.},
author = {Karpathy, Andrej},
booktitle = {Stanford University},
title = {{CS231n Convolutional Neural Networks for Visual Recognition}},
year = {2016}
}
@inproceedings{Bautista-Duran2017,
abstract = {Detecting violence is an important task due to the amount of people who suffer its effects daily. There is a tendency to focus the problem either in real situations or in non real ones, but both of them are useful on its own right. Until this day there has not been clear effort to try to relate both environments. In this work we try to detect violent situations on two different acoustic databases through the use of crossed information from one of them into the other. The system has been divided into three stages: feature extraction, feature selection based on genetic algorithms and classification to take a binary decision. Results focus on comparing performance loss when a database is evaluated with features selected on itself, or selection based in the other database. In general, complex classifiers tend to suffer higher losses, whereas simple classifiers, such as linear and quadratic detectors, offers less than a 10{\%} loss in most situations.},
author = {Bautista-Duran, Marta and Garc{\'{i}}a-G{\'{o}}mez, Joaqu{\'{i}}n and Gil-Pita, Roberto and S{\'{a}}nchez-Hevia, H{\'{e}}ctor and Mohino-Herranz, Inma and Rosa-Zurera, Manuel},
booktitle = {ICPRAM 2017 - Proceedings of the 6th International Conference on Pattern Recognition Applications and Methods},
doi = {10.5220/0006195004560462},
isbn = {9789897582226},
keywords = {Audio processing,Feature selection,Fictional environment,Real environment,Violence detection},
title = {{Acoustic detection of violence in real and fictional environments}},
year = {2017}
}
@article{Cover1967,
abstract = {The nearest neighbor decision rule assigns to an unclassified sample point the classification of the nearest of a set of previously classified points. This rule is independent of the underlying joint distribution on the sample points and their classifications, and hence the probability of error R of such a rule must be at least as great as the Bayes probability of error R*-the minimum probability of error over all decision rules taking underlying probability structure into account. However, in a large sample analysis, we will show in the M-category case that R* ≤ R ≤ R*(2 - MR*/(M-1)), where these bounds are the tightest possible, for all suitably smooth underlying distributions. Thus for any number of categories, the probability of error of the nearest neighbor rule is bounded above by twice the Bayes probability of error. In this sense, it may be said that half the classification information in an infinite sample set is contained in the nearest neighbor. {\textcopyright} 1967, IEEE. All rights reserved.},
author = {Cover, T. M. and Hart, P. E.},
doi = {10.1109/TIT.1967.1053964},
issn = {15579654},
journal = {IEEE Transactions on Information Theory},
title = {{Nearest Neighbor Pattern Classification}},
year = {1967}
}
@article{Stowell2015,
abstract = {For intelligent systems to make best use of the audio modality, it is important that they can recognize not just speech and music, which have been researched as specific tasks, but also general sounds in everyday environments. To stimulate research in this field we conducted a public research challenge: the IEEE Audio and Acoustic Signal Processing Technical Committee challenge on Detection and Classification of Acoustic Scenes and Events (DCASE). In this paper, we report on the state of the art in automatically classifying audio scenes, and automatically detecting and classifying audio events. We survey prior work as well as the state of the art represented by the submissions to the challenge from various research groups. We also provide detail on the organization of the challenge, so that our experience as challenge hosts may be useful to those organizing challenges in similar domains. We created new audio datasets and baseline systems for the challenge; these, as well as some submitted systems, are publicly available under open licenses, to serve as benchmarks for further research in general-purpose machine listening.},
author = {Stowell, Dan and Giannoulis, Dimitrios and Benetos, Emmanouil and Lagrange, Mathieu and Plumbley, Mark D.},
doi = {10.1109/TMM.2015.2428998},
issn = {15209210},
journal = {IEEE Transactions on Multimedia},
keywords = {Audio databases,event detection,machine intelligence,pattern recognition},
title = {{Detection and Classification of Acoustic Scenes and Events}},
year = {2015}
}
@article{Kruger2018,
abstract = {As computers are becoming more and more a part of our everyday life, the vision of Mark Weiser about ubiquitous computing becomes true. One of the core tasks of such devices is to assist the users in achieving their goals. To do this, the assistive system has to have knowledge about the current situation as well as the user's goal. Such knowledge allows the assistive system to provide strategies to support the users in achieving their goals beginning from the current situation. A GPS navigation device is a simple, yet well known instance of such an assistive system. It recommends a route based on the current location and the manually specified goal. Obviously, effective assistance can only be provided if accurate knowledge about the user's situation and his goal is available. This requires to reason about the actions of the user and to cope with uncertainties that are inherent to human behaviour. The problem becomes even harder, as in real world settings, users cannot be observed directly but through sensors that introduce noise and ambiguity as additional sources of uncertainty. Several applications in the literature showed that probabilistic methods can be used to infer the required information from sensor data. However, massive amounts of training data are needed in order to train classifiers to achieve good recognition rates. This is expensive and prevents trained models from being reused. Recently, researchers employed models of human behaviour in order to reduce the need for training data. These models are generalisable -- they allow the specification of human behaviour without the need for training samples. To this end, these models can be reused in different settings. While these models allow the synthesis of probabilistic models, only few attempts have been made to assess their capabilities with respect to low level sensors such as accelerometers. In fact, different researchers stated that inferring high level knowledge about the user from low level sensor data is an open research topic. To address the above problems, objective of this thesis is to answer the question "how to achieve efficient sensor-based reconstruction of causal structures of human behaviour in order to provide assistance?". To achieve that, in the first step the meaning of this question is analysed and requirements for an inference system are derived. A review of the literature is then conducted and a meta analysis is performed to assess the capabilities of the different approaches and the complexity of their evaluation setting. The results of this analysis show that none of the approaches from the literature satisfies all requirements. To answer the research question, the concept of CCBM is introduced. CCBM allows the specification of human behaviour by means of preconditions and effects and employs Bayesian filtering techniques to reconstruct action sequences from noisy and ambiguous sensor data. Furthermore, a novel approximative inference algorithm -- the Marginal Filter -- is introduced. The Marginal Filter is specifically tailored for categorical state spaces, which are generated by CCBM. To investigate the capabilities with respect to recognition performance and reusability, different experiments are then conducted. Each experiment addresses different aspects of the research question. A detailed analysis of the results of these experiments shows that CCBM is able to achieve good recognition rates. Moreover, the MF is shown to outperform the standard method for approximative Bayesian inference -- the Particle Filter. Furthermore, it is shown that CCBM satisfies the requirements.},
author = {Kr{\"{u}}ger, Frank},
journal = {ResearchGate},
title = {{Activity, Context, and Plan Recognition with Computational Causal Behaviour Models}},
year = {2018}
}
@article{Foggia2015,
abstract = {In this paper we propose a novel method for the detection of audio events for surveillance applications. The method is based on the bag of words approach, adapted to deal with the specific issues of audio surveillance: the need to recognize both short and long sounds, the presence of a significant noise level and of superimposed background sounds of intensity comparable to the audio events to be detected. In order to test the proposed method in complex, realistic scenarios, we have built a large, publicly available dataset of audio events. The dataset has allowed us to evaluate the robustness of our method with respect to varying levels of the Signal-to-Noise Ratio; the experimentation has confirmed its applicability under real world conditions, and has shown a significant performance improvement with respect to other methods from the literature.},
author = {Foggia, Pasquale and Petkov, Nicolai and Saggese, Alessia and Strisciuglio, Nicola and Vento, Mario},
doi = {10.1016/j.patrec.2015.06.026},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Audio surveillance,Bag of words,Event detection},
title = {{Reliable detection of audio events in highly noisy environments}},
year = {2015}
}
@misc{Browniee2018,
author = {Browniee, Jason},
title = {{A Gentle Introduction to k-fold Cross-Validation}},
url = {https://machinelearningmastery.com/k-fold-cross-validation/},
year = {2018}
}
@article{Liang2019,
abstract = {Over the years, activity sensing and recognition has been shown to play a key enabling role in a wide range of applications, from sustainability and human-computer interaction to health care. While many recognition tasks have traditionally employed inertial sensors, acoustic-based methods offer the benefit of capturing rich contextual information, which can be useful when discriminating complex activities. Given the emergence of deep learning techniques and leveraging new, large-scaled multi-media datasets, this paper revisits the opportunity of training audio-based classifiers without the onerous and time-consuming task of annotating audio data. We propose a framework for audio-based activity recognition that makes use of millions of embedding features from public online video sound clips. Based on the combination of oversampling and deep learning approaches, our framework does not require further feature processing or outliers filtering as in prior work. We evaluated our approach in the context of Activities of Daily Living (ADL) by recognizing 15 everyday activities with 14 participants in their own homes, achieving 64.2{\%} and 83.6{\%} averaged within-subject accuracy in terms of top-1 and top-3 classification respectively. Individual class performance was also examined in the paper to further study the co-occurrence characteristics of the activities and the robustness of the framework.},
archivePrefix = {arXiv},
arxivId = {1810.08691},
author = {Liang, Dawei and Thomaz, Edison},
doi = {10.1145/3314404},
eprint = {1810.08691},
issn = {2474-9567},
journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
title = {{Audio-Based Activities of Daily Living (ADL) Recognition with Large-Scale Acoustic Embeddings from Online Videos}},
year = {2019}
}
@article{Aucouturier2007,
abstract = {The "bag-of-frames" approach (BOF) to audio pattern recognition represents signals as the long-term statistical distribution of their local spectral features. This approach has proved nearly optimal for simulating the auditory perception of natural and human environments (or soundscapes), and is also the most predominent paradigm to extract high-level descriptions from music signals. However, recent studies show that, contrary to its application to soundscape signals, BOF only provides limited performance when applied to polyphonic music signals. This paper proposes to explicitly examine the difference between urban soundscapes and polyphonic music with respect to their modeling with the BOF approach. First, the application of the same measure of acoustic similarity on both soundscape and music data sets confirms that the BOF approach can model soundscapes to near-perfect precision, and exhibits none of the limitations observed in the music data set. Second, the modification of this measure by two custom homogeneity transforms reveals critical differences in the temporal and statistical structure of the typical frame distribution of each type of signal. Such differences may explain the uneven performance of BOF algorithms on soundscapes and music signals, and suggest that their human perception rely on cognitive processes of a different nature.},
author = {Aucouturier, Jean-Julien and Defreville, Boris and Pachet, Fran{\c{c}}ois},
doi = {10.1121/1.2750160},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
title = {{The bag-of-frames approach to audio pattern recognition: A sufficient model for urban soundscapes but not for polyphonic music}},
year = {2007}
}
@article{Bahoura2009,
abstract = {In this paper, we present the pattern recognition methods proposed to classify respiratory sounds into normal and wheeze classes. We evaluate and compare the feature extraction techniques based on Fourier transform, linear predictive coding, wavelet transform and Mel-frequency cepstral coefficients (MFCC) in combination with the classification methods based on vector quantization, Gaussian mixture models (GMM) and artificial neural networks, using receiver operating characteristic curves. We propose the use of an optimized threshold to discriminate the wheezing class from the normal one. Also, post-processing filter is employed to considerably improve the classification accuracy. Experimental results show that our approach based on MFCC coefficients combined to GMM is well adapted to classify respiratory sounds in normal and wheeze classes. McNemar's test demonstrated significant difference between results obtained by the presented classifiers (p {\textless} 0.05). {\textcopyright} 2009 Elsevier Ltd. All rights reserved.},
author = {Bahoura, Mohammed},
doi = {10.1016/j.compbiomed.2009.06.011},
issn = {00104825},
journal = {Computers in Biology and Medicine},
keywords = {Gaussian mixture models,Linear predictive coding,McNemar's test,Mel-frequency cepstral coefficients,Multi-layer perceptron,Receiver operating characteristic,Respiratory sounds,Statistical significance,Vector quantization,Wavelet transform},
title = {{Pattern recognition methods applied to respiratory sounds classification into normal and wheeze classes}},
year = {2009}
}
@inproceedings{Giannakopoulos2006,
abstract = {This work studies the problem of violence detection in audio data, which can be used for automated content rating. We employ some popular frame-level audio features both from the time and frequency domain. Afterwards, several statistics of the calculated feature sequences are fed as input to a Support Vector Machine classifier, which decides about the segment content with respect to violence. The presented experimental results verify the validity of the approach and exhibit a better performance than the other known approaches. {\textcopyright} Springer-Verlag Berlin Heidelberg 2006.},
author = {Giannakopoulos, Theodoros and Kosmopoulos, Dimitrios and Aristidou, Andreas and Theodoridis, Sergios},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/11752912_55},
isbn = {354034117X},
issn = {03029743},
title = {{Violence content classification using audio features}},
year = {2006}
}
@inproceedings{Simonyan2015,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
booktitle = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
eprint = {1409.1556},
title = {{Very deep convolutional networks for large-scale image recognition}},
year = {2015}
}
@misc{Mishra2018,
author = {Mishra, Aditya},
title = {{Metrics to Evaluate your Machine Learning Algorithm}},
url = {https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234},
year = {2018}
}
@article{Kumar2017,
author = {Kumar, Anurag and Raj, Bhiksha},
title = {{Deep CNN Framework for Audio Event Recognition using Weakly Labeled Web Data}},
year = {2017}
}
@misc{Drakos2018,
author = {Drakos, Georgios},
title = {{Support Vector Machine vs Logistic Regression}},
year = {2018}
}
@article{Fu2011,
abstract = {Music information retrieval (MIR) is an emerging research area that receives growing attention from both the research community and music industry. It addresses the problem of querying and retrieving certain types of music from large music data set. Classification is a fundamental problem in MIR. Many tasks in MIR can be naturally cast in a classification setting, such as genre classification, mood classification, artist recognition, instrument recognition, etc. Music annotation, a new research area in MIR that has attracted much attention in recent years, is also a classification problem in the general sense. Due to the importance of music classification in MIR research, rapid development of new methods, and lack of review papers on recent progress of the field, we provide a comprehensive review on audio-based classification in this paper and systematically summarize the state-of-the-art techniques for music classification. Specifically, we have stressed the difference in the features and the types of classifiers used for different classification tasks. This survey emphasizes on recent development of the techniques and discusses several open issues for future research. {\textcopyright} 2010 IEEE.},
author = {Fu, Zhouyu and Lu, Guojun and Ting, Kai Ming and Zhang, Dengsheng},
doi = {10.1109/TMM.2010.2098858},
issn = {15209210},
journal = {IEEE Transactions on Multimedia},
keywords = {Acoustic signal processing,classification algorithms,feature extraction,music information retrieval},
title = {{A survey of audio-based music classification and annotation}},
year = {2011}
}
@inproceedings{Wang2016,
abstract = {Multimedia event detection (MED) is the task of detecting given events (e.g. birthday party, making a sandwich) in a large collection of video clips. While visual features and automatic speech recognition typically provide the best features for this task, nonspeech audio can also contribute useful information, such as crowds cheering, engine noises, or animal sounds. MED is typically formulated as a two-stage process: the first stage generates clip-level feature representations, often by aggregating frame-level features; the second stage performs binary or multi-class classification to decide whether a given event occurs in a video clip. Both stages are usually performed «statically», i.e. using only local temporal information, or bag-of-words models. In this paper, we introduce longer-range temporal information with deep recurrent neural networks (RNNs) for both stages. We classify each audio frame among a set of semantic units called «noisemes» the sequence of frame-level confidence distributions is used as a variable-length clip-level representation. Such confidence vector sequences are then fed into long short-term memory (LSTM) networks for clip-level classification. We observe improvements in both frame-level and clip-level performance compared to SVM and feed-forward neural network baselines.},
author = {Wang, Yun and Neves, Leonardo and Metze, Florian},
booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
doi = {10.1109/ICASSP.2016.7472176},
isbn = {9781479999880},
issn = {15206149},
keywords = {Multimedia event detection (MED),long short-term memory (LSTM),noisemes,recurrent neural networks (RNNs)},
title = {{Audio-based multimedia event detection using deep recurrent neural networks}},
year = {2016}
}
@article{Dubois2006,
abstract = {The present research on cognitive categories mediates between individual experiences of soundscapes and collective representations shared in language and elaborated as knowledge. This approach focuses on meanings attributed to soundscapes in an attempt to bridge the gap between individual perceptual categories and sociological representations. First, results of several free categorisation experiments are presented, namely the categorical structures elicited using soundscape recordings and the underlying principles of organisation derived from the analysis of verbal comments. People categorised sound samples on the basis of semantic features that integrate perceptual ones. Specifically, soundscapes reflecting human activity were perceived as more pleasant than soundscapes where mechanical sounds were predominant. Second, the linguistic exploration of free-format verbal description of soundscapes indicated that the meanings attributed to sounds act as a determinant for sound quality evaluations. Soundscape evaluations are therefore qualitative first as they are semiotic in nature as grounded in cultural values given to different types of activities. Physical descriptions of sound properties have to be reconsidered as cues pointing to diverse cognitive objects to be identified first rather as the only adequate, exhaustive and objective description of the sound itself. Finally, methodological and theoretical consequences of these findings are drawn, highlighting the need to address not only noise annoyance but rather sound quality of urban environments. To do so, cognitive evaluations must be conducted in the first place to identify relevant city users' categories of soundscapes and then to use physical measurement to characterize corresponding acoustic events. {\textcopyright} S. Hlrzel Verlag EAA.},
author = {Dubois, Dani{\`{e}}le and Guastavino, Catherine and Raimbault, Manon},
issn = {16101928},
journal = {Acta Acustica united with Acustica},
title = {{A cognitive approach to urban soundscapes: Using verbal data to access everyday life auditory categories}},
year = {2006}
}
@inproceedings{Jabbar2015,
abstract = {Machine learning is an important task for learning artificial neural networks, and we find in the learning one of the common problems of learning the Artificial Neural Network (ANN) is over-fitting and under-fitting to outlier points. In this paper we performed various methods in avoiding over-fitting and under-fitting; that is penalty and early stopping methods. A comparative study has been presented for the aforementioned methods to evaluate their performance within a range of specific parameters such as; speed of training, over-fitting and under-fitting avoidance, difficulty, capacity, time of training, and their accuracy. Besides these parameters we have included comparison between over-fitting and under-fitting. We found the early stopping method as being better as compared to the penalty method, as it can avoid over- fitting and under-fitting with respect to validation time. Besides we find that Under-fitting neural networks perform poorly on both training and test sets, but Over-fitting networks may do very well on training sets though terribly on test sets.},
author = {Jabbar, Haider Khalaf and Khan, Rafiqul Zaman},
doi = {10.3850/978-981-09-5247-1_017},
title = {{Methods to Avoid Over-Fitting and Under-Fitting in Supervised Machine Learning (Comparative Study)}},
year = {2015}
}
@inproceedings{Demarty2013,
abstract = {This paper provides a description of the MediaEval 2013 Affect Task Violent Scenes Detection. This task, which is proposed for the third year to the research community, derives directly from a Technicolor use case which aims at easing a user's selection process from a movie database. This task will therefore apply to movie content. We provide some insight into the Technicolor use case, before giving details on the task itself, which has seen some changes in 2013. Dataset, annotations, and evaluation criteria as well as the required and optional runs are described.},
author = {Demarty, Claire H{\'{e}}l{\`{e}}ne and Penet, C{\'{e}}dric and Schedl, Markus and Ionescu, Bogdan and Quang, Vu Lam and Jiang, Yu Gang},
booktitle = {CEUR Workshop Proceedings},
issn = {16130073},
title = {{The MediaEval 2013 affect task: Violent Scenes Detection}},
year = {2013}
}
@misc{MLGlossary2017,
author = {{ML Glossary}},
title = {{Loss Functions}},
url = {https://ml-cheatsheet.readthedocs.io/en/latest/loss{\_}functions.html},
year = {2017}
}
@inproceedings{Cramer2019,
abstract = {A considerable challenge in applying deep learning to audio classification is the scarcity of labeled data. An increasingly popular solution is to learn deep audio embeddings from large audio collections and use them to train shallow classifiers using small labeled datasets. Look, Listen, and Learn (L3-Net) is an embedding trained through self-supervised learning of audio-visual correspondence in videos as opposed to other embeddings requiring labeled data. This framework has the potential to produce powerful out-of-the-box embeddings for downstream audio classification tasks, but has a number of unexplained design choices that may impact the embeddings' behavior. In this paper we investigate how L3-Net design choices impact the performance of downstream audio classifiers trained with these embeddings. We show that audio-informed choices of input representation are important, and that using sufficient data for training the embedding is key. Surprisingly, we find that matching the content for training the embedding to the downstream task is not beneficial. Finally, we show that our best variant of the L3-Net embedding outperforms both the VGGish and SoundNet embeddings, while having fewer parameters and being trained on less data. Our implementation of the L3-Net embedding model as well as pre-trained models are made freely available online.},
author = {Cramer, Jason and Wu, Ho Hsiang and Salamon, Justin and Bello, Juan Pablo},
booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
doi = {10.1109/ICASSP.2019.8682475},
isbn = {9781479981311},
issn = {15206149},
keywords = {Audio classification,deep audio embeddings,deep learning,machine listening,transfer learning},
title = {{Look, Listen, and Learn More: Design Choices for Deep Audio Embeddings}},
year = {2019}
}
@article{Demarty2015,
abstract = {Content-based analysis to find where violence appears in multimedia content has several applications, from parental control and children protection to surveillance. This paper presents the design and annotation of the Violent Scene Detection dataset, a corpus targeting the detection of physical violence in Hollywood movies. We discuss definitions of physical violence and provide a simple and objective definition which was used to annotate a set of 18 movies, thus resulting in the largest freely-available dataset for such a task. We discuss borderline cases and compare with annotations based on a subjective definition which requires multiple annotators. We provide a detailed analysis of the corpus, in particular regarding the relationship between violence and a set of key audio and visual concepts which were also annotated. The VSD dataset results from two years of benchmarking in the framework of the MediaEval initiative. We provide results from the 2011 and 2012 benchmarks as a validation of the dataset and as a state-of-the-art baseline. The VSD dataset is freely available at the address:http://www.technicolor.com/en/innovation/research-innovation/scientific-data-sharing/violent-scenes-dataset.},
author = {Demarty, Claire H{\'{e}}l{\`{e}}ne and Penet, C{\'{e}}dric and Soleymani, Mohammad and Gravier, Guillaume},
doi = {10.1007/s11042-014-1984-4},
issn = {15737721},
journal = {Multimedia Tools and Applications},
keywords = {Content-based analysis,Corpus design,Multimedia evaluation,Physical violence definition,Semantic audio concepts,Semantic video concepts,Violent scene detection},
title = {{VSD, a public dataset for the detection of violent scenes in movies: design, annotation, analysis and evaluation}},
year = {2015}
}
@inproceedings{Hinton2003,
abstract = {We describe a probabilistic approach to the task of placing objects, described by high-dimensional vectors or by pairwise dissimilarities, in a low-dimensional space in a way that preserves neighbor identities. A Gaussian is centered on each object in the high-dimensional space and the densities under this Gaussian (or the given dissimilarities) are used to define a probability distribution over all the potential neighbors of the object. The aim of the embedding is to approximate this distribution as well as possible when the same operation is performed on the low-dimensional "images" of the objects. A natural cost function is a sum of Kullback-Leibler divergences, one per object, which leads to a simple gradient for adjusting the positions of the low-dimensional images. Unlike other dimensionality reduction methods, this probabilistic framework makes it easy to represent each object by a mixture of widely separated low-dimensional images. This allows ambiguous objects, like the document count vector for the word "bank", to have versions close to the images of both "river" and "finance" without forcing the images of outdoor concepts to be located close to those of corporate concepts.},
author = {Hinton, Geoffrey and Roweis, Sam},
booktitle = {Advances in Neural Information Processing Systems},
isbn = {0262025507},
issn = {10495258},
title = {{Stochastic neighbor embedding}},
year = {2003}
}
@inproceedings{Garcia-Gomez2016,
abstract = {Violence continues being an important problem in the society. Thousands of people suffer its effects every day and statistics show this number has maintained or almost increased recently. In the modern environment of smart cities there is a necessity to develop a system capable of detecting if a violent situation is taking place or not. In this paper we present an automatic acoustic violence detection system for smart cities, integrating both signal processing and pattern recognition techniques. The proposed software has been implemented in three steps: feature extraction in time and frequency domain, genetic algorithm implementation in order to select the best features, and classification to take a binary decision. Results derived from the experiments show that MFCCs are the best features for violence detection, and others like pitch or short time energy have also a good performance. In other words, features that can distinguish between voiced and unvoiced frames seem to be a good election for violence detection in real environments.},
author = {Garc{\'{i}}a-G{\'{o}}mez, Joaqu{\'{i}}n and Bautista-Dur{\'{a}}n, Marta and Gil-Pita, Roberto and Mohino-Herranz, Inma and Rosa-Zurera, Manuel},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-48799-1_52},
isbn = {9783319487984},
issn = {16113349},
keywords = {Audio features,Audio processing,Feature selection,Violence detection},
title = {{Violence detection in real environments for smart cities}},
year = {2016}
}
@article{Amatriain2004,
author = {Amatriain, Xavier},
title = {{An Object-Oriented Metamodel for Digital Signal Processing with a focus on Audio and Music}},
year = {2004}
}
@misc{ImageNet2014,
author = {ImageNet},
title = {{Results for ILSVRC2014}},
year = {2014}
}
@inproceedings{Nam2012,
abstract = {We present a data-processing pipeline based on sparse feature learning and describe its applications to music annotation and retrieval. Content-based music annotation and retrieval systems process audio starting with features. While commonly used features, such as MFCC, are handcrafted to extract characteristics of the audio in a succinct way, there is increasing interest in learning features automatically from data using unsupervised algorithms. We describe a systemic approach applying feature-learning algorithms to music data, in particular, focusing on a high-dimensional sparse-feature representation. Our experiments show that, using only a linear classifier, the newly learned features produce results on the CAL500 dataset comparable to state-of-the-art music annotation and retrieval systems. {\textcopyright} 2012 International Society for Music Information Retrieval.},
author = {Nam, Juhan and Herrera, Jorge and Slaney, Malcolm and Smith, Julius},
booktitle = {Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012},
isbn = {9789727521449},
title = {{Learning sparse feature representations for music annotation and retrieval}},
year = {2012}
}
@article{Ali2018,
abstract = {Violence is autonomous, the contents that one would not let children to see in movies or web videos. This is a challenging problem due to strong content variations among the positive instances. To solve this problem, implementation of deep neural network to classify the violence content in videos is proposed. Currently, deep neural network has shown its efficiency in natural language processing, fraud detection, social media, text classification, image classification. Regardless of the conventional methods applied to overcome this issue, but these techniques seem insufficiently accurate and does not adopt well to certain webs or user needs. Therefore, the purpose of this study is to assess the classification performances on violence video using Deep Neural Network (DNN). Hence, in this paper different architectures of hidden layers and hidden nodes in DNN have been implemented using the try-error method and equation based method, to examine the effect of the number of hidden layers and hidden nodes to the classification performance. From the results, it indicates 53{\%} accuracy rate for try and error approach, meanwhile for equation based approach it indicates 51{\%} accuracy rate.},
author = {Ali, Ashikin and Senan, Norhalina},
doi = {10.1007/978-3-319-72550-5_22},
file = {:Users/Oscar/Downloads/Ali-Senan2018{\_}Chapter{\_}ViolenceVideoClassificationPer.pdf:pdf},
isbn = {9783319725499},
issn = {21945357},
journal = {Advances in Intelligent Systems and Computing},
keywords = {Artificial neural network,Classification,Deep neural network,Violence video},
pages = {225--233},
title = {{Violence video classification performance using deep neural networks}},
volume = {700},
year = {2018}
}
@inproceedings{Le2013,
abstract = {We consider the problem of building high-level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images? To answer this, we train a deep sparse autoencoder on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200×200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies. Starting from these learned features, we trained our network to recognize 22,000 object categories from ImageNet and achieve a leap of 70{\%} relative improvement over the previous state-of-the-art. {\textcopyright} 2013 IEEE.},
archivePrefix = {arXiv},
arxivId = {1112.6209},
author = {Le, Quoc V.},
booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
doi = {10.1109/ICASSP.2013.6639343},
eprint = {1112.6209},
isbn = {9781479903566},
issn = {15206149},
title = {{Building high-level features using large scale unsupervised learning}},
year = {2013}
}
@misc{Nguyen2018,
author = {Nguyen, Michael},
title = {{Illustrated Guide to LSTM's and GRU's: A step by step explanation}},
url = {https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21},
year = {2018}
}
@misc{Scikit-learn,
author = {Scikit-learn},
title = {{Metrics and scoring: quantifying the quality of predictions}},
url = {https://scikit-learn.org/stable/modules/model{\_}evaluation.html}
}
@inproceedings{Piczak2015,
abstract = {This paper evaluates the potential of convolutional neural networks in classifying short audio clips of environmental sounds. A deep model consisting of 2 convolutional layers with max-pooling and 2 fully connected layers is trained on a low level representation of audio data (segmented spectrograms) with deltas. The accuracy of the network is evaluated on 3 public datasets of environmental and urban recordings. The model outperforms baseline implementations relying on mel-frequency cepstral coefficients and achieves results comparable to other state-of-the-art approaches.},
author = {Piczak, Karol J.},
booktitle = {IEEE International Workshop on Machine Learning for Signal Processing, MLSP},
doi = {10.1109/MLSP.2015.7324337},
isbn = {9781467374545},
issn = {21610371},
keywords = {Classification,convolutional neural networks,environmental sound},
title = {{Environmental sound classification with convolutional neural networks}},
year = {2015}
}
@book{EuropeanUnionAgencyforFundamentalRights2014,
abstract = {Violence against women undermines women's core fundamental rights such as dignity, access to justice and gender equality. For example, one in three women has experienced physical and/or sexual violence since the age of 15; one in five women has experienced stalking; every second woman has been confronted with one or more forms of sexual harassment. What emerges is a picture of extensive abuse that affects many women's lives but is systematically under-reported to the authorities. The scale of violence against women is therefore not reflected by official data. This FRA survey is the first of its kind on violence against women across the 28 Member States of the European Union (EU). It is based on interviews with 42,000 women across the EU, who were asked about their experiences of physical, sexual and psychological violence, including incidents of intimate partner violence (‘domestic violence'). The survey also included questions on stalking, sexual harassment, and the role played by new technologies in women's experiences of abuse. In addition, it asked about their experiences of violence in childhood. Based on the detailed findings, FRA suggests courses of action in different areas that are touched by violence against women and go beyond the narrow confines of criminal law, ranging from employment and health to the medium of new technologies.},
author = {{European Union Agency for Fundamental Rights}},
booktitle = {Publications Office of the European Union,},
doi = {10.2811/62230},
isbn = {9789292393427},
keywords = {Violence against women,$\backslash$access to justice,attitudes and awareness,awareness of laws and political initiatives addres,awareness of selected organisations and specialise,carrying something for self-defence,characteristics of perpetrators,characteristics of victims,children's exposure to domestic violence,comprehensive and comparable data,confidence intervals and characteristics of the re,consequences,contact with police or other services,context,criminal law,details about intimate partner violence,details about non-partner violence,dignity,domestic violence,effect of the interview mode when asking sensitive,emotional responses,employment,fear of victimisation and its impact,forms of violence,fundamental rights,gender equality,health,impact of age,injuries,interviews,intimate partner violence,key results for selected respondent groups,methodology,national surveys on violence against women,needs of vitims,new technologies,official data,perceptions on frequency of violence against women,physical violence,prevalence rates,psychological consequences,psychological violence,relationship between violence in childhood and lat,relationship between worry and risk avoidance beha,respondant background variables,sexual harassment,sexual violence,stalking,survey,survey fieldwork outcomes,type of offender,under-reporting,violence in childhood,weighting,women's attitude towards doctors' role in identify,women's awareness of campaigns addressing violence,women's awareness of organisations and specialised,women's knowledge about other women victims of int,women's risk avoidance behaviour,worry about physical or sexual assault},
title = {{Violence against women : An EU-wide survey}},
year = {2014}
}
@misc{Rampurawala2019,
author = {Rampurawala, Mohammed},
title = {{Classification with TensorFlow and Dense Neural Networks}},
year = {2019}
}
@article{Sarkar2018,
author = {Sarkar, Dipanjan},
title = {{A Comprehensive Hands-on Guide to Transfer Learning with Real-World Applications in Deep Learning}},
year = {2018}
}
@misc{Keras,
author = {Keras},
title = {{Convolutional Layers}}
}
@article{Chawla2002,
abstract = {An approach to the construction of classifiers from unbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of "normal" examples with only a small percentage of "abnormal" or "interesting" examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy. {\textcopyright} 2002 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {1106.1813},
author = {Chawla, Nitesh V. and Bowyer, Kevin W. and Hall, Lawrence O. and Kegelmeyer, W. Philip},
doi = {10.1613/jair.953},
eprint = {1106.1813},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
title = {{SMOTE: Synthetic minority over-sampling technique}},
year = {2002}
}
@misc{Browniee2020,
author = {Browniee, Jason},
title = {{SMOTE Oversampling for Imbalanced Classification with Python}},
year = {2020}
}
@article{Blagus2013,
abstract = {Background: Classification using class-imbalanced data is biased in favor of the majority class. The bias is even larger for high-dimensional data, where the number of variables greatly exceeds the number of samples. The problem can be attenuated by undersampling or oversampling, which produce class-balanced data. Generally undersampling is helpful, while random oversampling is not. Synthetic Minority Oversampling TEchnique (SMOTE) is a very popular oversampling method that was proposed to improve random oversampling but its behavior on high-dimensional data has not been thoroughly investigated. In this paper we investigate the properties of SMOTE from a theoretical and empirical point of view, using simulated and real high-dimensional data.Results: While in most cases SMOTE seems beneficial with low-dimensional data, it does not attenuate the bias towards the classification in the majority class for most classifiers when data are high-dimensional, and it is less effective than random undersampling. SMOTE is beneficial for k-NN classifiers for high-dimensional data if the number of variables is reduced performing some type of variable selection; we explain why, otherwise, the k-NN classification is biased towards the minority class. Furthermore, we show that on high-dimensional data SMOTE does not change the class-specific mean values while it decreases the data variability and it introduces correlation between samples. We explain how our findings impact the class-prediction for high-dimensional data.Conclusions: In practice, in the high-dimensional setting only k-NN classifiers based on the Euclidean distance seem to benefit substantially from the use of SMOTE, provided that variable selection is performed before using SMOTE; the benefit is larger if more neighbors are used. SMOTE for k-NN without variable selection should not be used, because it strongly biases the classification towards the minority class. {\textcopyright} 2013 Blagus and Lusa; licensee BioMed Central Ltd.},
author = {Blagus, Rok and Lusa, Lara},
doi = {10.1186/1471-2105-14-106},
issn = {14712105},
journal = {BMC Bioinformatics},
title = {{SMOTE for high-dimensional class-imbalanced data}},
year = {2013}
}
@article{Xie2015,
abstract = {In view of the SVM classification for the imbalanced sand-dust storm data sets, this paper proposes a hybrid self-adaptive sampling method named SRU-AIBSMOTE algorithm. This method can adaptively adjust neighboring selection strategy based on the internal distribution of sample sets. It produces virtual minority class instances through randomized interpolation in the spherical space which consists of minority class instances and their neighbors. The random undersampling is also applied to undersample the majority class instances for removal of redundant data in the sample sets. The comparative experimental results on the real data sets from Yanchi and Tongxin districts in Ningxia of China show that the SRU-AIBSMOTE method can obtain better classification performance than some traditional classification methods.},
author = {Xie, Yonghua and Liu, Yurong and Fu, Qingqiu},
doi = {10.1155/2015/562724},
issn = {1607887X},
journal = {Discrete Dynamics in Nature and Society},
title = {{Imbalanced Data Sets Classification Based on SVM for Sand-Dust Storm Warning}},
year = {2015}
}
