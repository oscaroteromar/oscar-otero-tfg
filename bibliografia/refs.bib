@book{Wang2006,
abstract = {How can we engineer systems capable of “cocktail party” listening? Human listeners are able to perceptually segregate one sound source from an acoustic mixture, such as a single voice from a mixture of other voices and music at a busy cocktail party. How can we engineer “machine listening” systems that achieve this perceptual feat? Albert Bregmans book Auditory Scene Analysis, published in 1990, drew an analogy between the perception of auditory scenes and visual scenes, and described a coherent framework for understanding the perceptual organization of sound. His account has stimulated much interest in computational studies of hearing. Such studies are motivated in part by the demand for practical sound separation systems, which have many applications including noiserobust automatic speech recognition, hearing prostheses, and automatic music transcription. This emerging field has become known as computational auditory scene analysis (CASA). Computational Auditory Scene Analysis: Principles, Algorithms, and Applications provides a comprehensive and coherent account of the state of the art in CASA, in terms of the underlying principles, the algorithms and system architectures that are employed, and the potential applications of this exciting new technology. With a Foreword by Bregman, its chapters are written by leading researchers and cover a wide range of topics including: Estimation of multiple fundamental frequenciesFeaturebased and modelbased approaches to CASASound separation based on spatial locationProcessing for reverberant environmentsSegregation of speech and musical signalsAutomatic speech recognition in noisy environmentsNeural and perceptual modeling of auditory organizationThe text is written at a level that will be accessible to graduate students and researchers from related science and engineering disciplines. The extensive bibliography accompanying each chapter will also make this book a valuable reference source. A web site accompanying the text, http://www.casabook.org, features software tools and sound demonstrations.},
author = {Wang, Deliang and Brown, Guy J.},
booktitle = {Computational Auditory Scene Analysis: Principles, Algorithms, and Applications},
doi = {10.1109/9780470043387},
isbn = {0470043385},
title = {{Computational auditory scene analysis: Principles, algorithms, and applications}},
year = {2006}
}
@inproceedings{Temko2007,
abstract = {In this paper, we present the results of the Acoustic Event Detection (AED) and Classification (AEC) evaluations carried out in February 2006 by the three participant partners from the CHIL project. The primary evaluation task was AED of the testing portions of the isolated sound databases and seminar recordings produced in CHIL. Additionally, a secondary AEC evaluation task was designed using only the isolated sound databases. The set of meetingroom acoustic event classes and the metrics were agreed by the three partners and ELDA was in charge of the scoring task. In this paper, the various systems for the tasks of AED and AEC and their results are presented. {\textcopyright} Springer-Verlag Berlin Heidelberg 2007.},
author = {Temko, Andrey and Malkin, Robert and Zieger, Christian and Macho, Dusan and Nadeu, Climent and Omologo, Maurizio},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-540-69568-4_29},
isbn = {9783540695677},
issn = {03029743},
title = {{CLEAR evaluation of acoustic event detection and classification systems}},
year = {2007}
}
@incollection{Temko2009,
abstract = {The human activity that takes place in meeting-rooms or class-rooms is reflected in a rich variety of acoustic events, either produced by the human body or by objects handled by humans, so the determination of both the identity of sounds and their position in time may help to detect and describe that human activity. Additionally, detection of sounds other than speech may be useful to enhance the robustness of speech technologies like automatic speech recognition. Automatic detection and classification of acoustic events is the objective of this thesis work. It aims at processing the acoustic signals collected by distant microphones in meeting-room or classroom environments to convert them into symbolic descriptions corresponding to a listener's perception of the different sound events that are present in the signals and their sources. First of all, the task of acoustic event classification is faced using Support Vector Machine (SVM) classifiers, which are motivated by the scarcity of training data. A confusion-matrix-based variable-feature-set clustering scheme is developed for the multiclass recognition problem, and tested on the gathered database. With it, a higher classification rate than the GMM-based technique is obtained, arriving to a large relative average error reduction with respect to the best result from the conventional binary tree scheme. Moreover, several ways to extend SVMs to sequence processing are compared, in an attempt to avoid the drawback of SVMs when dealing with audio data, i.e. their restriction to work with fixed-length vectors, observing that the dynamic time warping kernels work well for sounds that show a temporal structure. Furthermore, concepts and tools from the fuzzy theory are used to investigate, first, the importance of and degree of interaction among features, and second, ways to fuse the outputs of several classification systems. The developed AEC systems are tested also by participating in several international evaluations from 2004 to 2006, and the results are reported. The second main contribution of this thesis work is the development of systems for detection of acoustic events. The detection problem is more complex since it includes both classification and determination of the time intervals where the sound takes place. Two system versions are developed and tested on the datasets of the two CLEAR international evaluation campaigns in 2006 and 2007. Two kinds of databases are used: two databases of isolated acoustic events, and a database of interactive seminars containing a significant number of acoustic events of interest. Our developed systems, which consist of SVM-based classification within a sliding window plus post-processing, were the only submissions not using HMMs, and each of them obtained competitive results in the corresponding evaluation. Speech activity detection was also pursued in this thesis since, in fact, it is a – especially important – particular case of acoustic event detection. An enhanced SVM training approach for the speech activity detection task is developed, mainly to cope with the problem of dataset reduction. The resulting SVM-based system is tested with several NIST Rich Transcription (RT) evaluation datasets, and it shows better scores than our GMM-based system, which ranked among the best systems in the RT06 evaluation. Finally, it is worth mentioning a few side outcomes from this thesis work. As it has been carried out in the framework of the CHIL EU project, the author has been responsible for the organization of the above mentioned international evaluations in acoustic event classification and detection, taking a leading role in the specification of acoustic event classes, databases, and evaluation protocols, and, especially, in the proposal and implementation of the various metrics that have been used. Moreover, the detection systems have been implemented in the UPC's smart-room and work in real time for purposes of testing and demonstration.},
author = {Temko, Andrey and Nadeu, Climent and Macho, Du{\v{s}}an and Malkin, Robert and Zieger, Christian and Omologo, Maurizio},
booktitle = {Computers in the Human Interaction Loop},
chapter = {Part II, 7},
doi = {10.1007/978-1-84882-054-8_7},
title = {{Acoustic Event Detection and Classification}},
year = {2009}
}
@article{Bahoura2009,
abstract = {In this paper, we present the pattern recognition methods proposed to classify respiratory sounds into normal and wheeze classes. We evaluate and compare the feature extraction techniques based on Fourier transform, linear predictive coding, wavelet transform and Mel-frequency cepstral coefficients (MFCC) in combination with the classification methods based on vector quantization, Gaussian mixture models (GMM) and artificial neural networks, using receiver operating characteristic curves. We propose the use of an optimized threshold to discriminate the wheezing class from the normal one. Also, post-processing filter is employed to considerably improve the classification accuracy. Experimental results show that our approach based on MFCC coefficients combined to GMM is well adapted to classify respiratory sounds in normal and wheeze classes. McNemar's test demonstrated significant difference between results obtained by the presented classifiers (p {\textless} 0.05). {\textcopyright} 2009 Elsevier Ltd. All rights reserved.},
author = {Bahoura, Mohammed},
doi = {10.1016/j.compbiomed.2009.06.011},
issn = {00104825},
journal = {Computers in Biology and Medicine},
keywords = {Gaussian mixture models,Linear predictive coding,McNemar's test,Mel-frequency cepstral coefficients,Multi-layer perceptron,Receiver operating characteristic,Respiratory sounds,Statistical significance,Vector quantization,Wavelet transform},
title = {{Pattern recognition methods applied to respiratory sounds classification into normal and wheeze classes}},
year = {2009}
}
@inproceedings{Mesaros2010,
abstract = {This paper presents a system for acoustic event detection in recordings from real life environments. The events are modeled using a network of hidden Markov models; their size and topology is chosen based on a study of isolated events recognition. We also studied the effect of ambient background noise on event classification performance. On real life recordings, we tested recognition of isolated sound events and event detection. For event detection, the system performs recognition and temporal positioning of a sequence of events. An accuracy of 24{\%} was obtained in classifying isolated sound events into 61 classes. This corresponds to the accuracy of classifying between 61 events when mixed with ambient background noise at 0dB signal-to-noise ratio. In event detection, the system is capable of recognizing almost one third of the events, and the temporal positioning of the events is not correct for 84{\%} of the time. {\textcopyright} EURASIP, 2010.},
author = {Mesaros, Annamaria and Heittola, Toni and Eronen, Antti and Virtanen, Tuomas},
booktitle = {European Signal Processing Conference},
issn = {22195491},
title = {{Acoustic event detection in real life recordings}},
year = {2010}
}
@article{Bahoura2010,
abstract = {Our understanding of etiology of obesity and overweight is incomplete due to lack of objective and accurate methods for monitoring of ingestive behavior (MIB) in the free-living population. Our research has shown that frequency of swallowing may serve as a predictor for detecting food intake, differentiating liquids and solids, and estimating ingested mass. This paper proposes and compares two methods of acoustical swallowing detection from sounds contaminated by motion artifacts, speech, and external noise. Methods based on mel-scale Fourier spectrum, wavelet packets, and support vector machines are studied considering the effects of epoch size, level of decomposition, and lagging on classification accuracy. The methodology was tested on a large dataset (64.5 h with a total of 9966 swallows) collected from 20 human subjects with various degrees of adiposity. Average weighted epoch-recognition accuracy for intravisit individual models was 96.8{\%}, which resulted in 84.7{\%} average weighted accuracy in detection of swallowing events. These results suggest high efficiency of the proposed methodology in separation of swallowing sounds from artifacts that originate from respiration, intrinsic speech, head movements, food ingestion, and ambient noise. The recognition accuracy was not related to body mass index, suggesting that the methodology is suitable for obese individuals. {\textcopyright} 2006 IEEE.},
author = {Sazonov, Edward S. and Makeyev, Oleksandr and Schuckers, Stephanie and Lopez-Meyer, Paulo and Melanson, Edward L. and Neuman, Michael R.},
doi = {10.1109/TBME.2009.2033037},
issn = {00189294},
journal = {IEEE Transactions on Biomedical Engineering},
keywords = {Biomedical signal processing,Obesity,Pattern recognition,Swallowing,Wearable devices},
title = {{Automatic detection of swallowing events by acoustical means for applications of monitoring of ingestive behavior}},
year = {2010}
}
@article{Aucouturier2007,
abstract = {The "bag-of-frames" approach (BOF) to audio pattern recognition represents signals as the long-term statistical distribution of their local spectral features. This approach has proved nearly optimal for simulating the auditory perception of natural and human environments (or soundscapes), and is also the most predominent paradigm to extract high-level descriptions from music signals. However, recent studies show that, contrary to its application to soundscape signals, BOF only provides limited performance when applied to polyphonic music signals. This paper proposes to explicitly examine the difference between urban soundscapes and polyphonic music with respect to their modeling with the BOF approach. First, the application of the same measure of acoustic similarity on both soundscape and music data sets confirms that the BOF approach can model soundscapes to near-perfect precision, and exhibits none of the limitations observed in the music data set. Second, the modification of this measure by two custom homogeneity transforms reveals critical differences in the temporal and statistical structure of the typical frame distribution of each type of signal. Such differences may explain the uneven performance of BOF algorithms on soundscapes and music signals, and suggest that their human perception rely on cognitive processes of a different nature.},
author = {Aucouturier, Jean-Julien and Defreville, Boris and Pachet, Fran{\c{c}}ois},
doi = {10.1121/1.2750160},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
title = {{The bag-of-frames approach to audio pattern recognition: A sufficient model for urban soundscapes but not for polyphonic music}},
year = {2007}
}
@article{Dubois2006,
abstract = {The present research on cognitive categories mediates between individual experiences of soundscapes and collective representations shared in language and elaborated as knowledge. This approach focuses on meanings attributed to soundscapes in an attempt to bridge the gap between individual perceptual categories and sociological representations. First, results of several free categorisation experiments are presented, namely the categorical structures elicited using soundscape recordings and the underlying principles of organisation derived from the analysis of verbal comments. People categorised sound samples on the basis of semantic features that integrate perceptual ones. Specifically, soundscapes reflecting human activity were perceived as more pleasant than soundscapes where mechanical sounds were predominant. Second, the linguistic exploration of free-format verbal description of soundscapes indicated that the meanings attributed to sounds act as a determinant for sound quality evaluations. Soundscape evaluations are therefore qualitative first as they are semiotic in nature as grounded in cultural values given to different types of activities. Physical descriptions of sound properties have to be reconsidered as cues pointing to diverse cognitive objects to be identified first rather as the only adequate, exhaustive and objective description of the sound itself. Finally, methodological and theoretical consequences of these findings are drawn, highlighting the need to address not only noise annoyance but rather sound quality of urban environments. To do so, cognitive evaluations must be conducted in the first place to identify relevant city users' categories of soundscapes and then to use physical measurement to characterize corresponding acoustic events. {\textcopyright} S. Hlrzel Verlag EAA.},
author = {Dubois, Dani{\`{e}}le and Guastavino, Catherine and Raimbault, Manon},
issn = {16101928},
journal = {Acta Acustica united with Acustica},
title = {{A cognitive approach to urban soundscapes: Using verbal data to access everyday life auditory categories}},
year = {2006}
}
@inproceedings{Henaff2011,
abstract = {In this work we present a system to automatically learn features from audio in an unsupervised manner. Our method first learns an overcomplete dictionary which can be used to sparsely decompose log-scaled spectrograms. It then trains an efficient encoder which quickly maps new inputs to approximations of their sparse representations using the learned dictionary. This avoids expensive iterative procedures usually required to infer sparse codes. We then use these sparse codes as inputs for a linear Support Vector Machine (SVM). Our system achieves 83.4{\%} accuracy in predicting genres on the GTZAN dataset, which is competitive with current state-of-the-art approaches. Furthermore, the use of a simple linear classifier combined with a fast feature extraction system allows our approach to scale well to large datasets. {\textcopyright} 2011 International Society for Music Information Retrieval.},
author = {Henaff, Mikael and Jarrett, Kevin and Kavukcuoglu, Koray and Lecun, Yann},
booktitle = {Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011},
isbn = {9780615548654},
title = {{Unsupervised learning of sparse features for scalable audio classification}},
year = {2011}
}
@article{Liang2019,
abstract = {Over the years, activity sensing and recognition has been shown to play a key enabling role in a wide range of applications, from sustainability and human-computer interaction to health care. While many recognition tasks have traditionally employed inertial sensors, acoustic-based methods offer the benefit of capturing rich contextual information, which can be useful when discriminating complex activities. Given the emergence of deep learning techniques and leveraging new, large-scaled multi-media datasets, this paper revisits the opportunity of training audio-based classifiers without the onerous and time-consuming task of annotating audio data. We propose a framework for audio-based activity recognition that makes use of millions of embedding features from public online video sound clips. Based on the combination of oversampling and deep learning approaches, our framework does not require further feature processing or outliers filtering as in prior work. We evaluated our approach in the context of Activities of Daily Living (ADL) by recognizing 15 everyday activities with 14 participants in their own homes, achieving 64.2{\%} and 83.6{\%} averaged within-subject accuracy in terms of top-1 and top-3 classification respectively. Individual class performance was also examined in the paper to further study the co-occurrence characteristics of the activities and the robustness of the framework.},
archivePrefix = {arXiv},
arxivId = {1810.08691},
author = {Liang, Dawei and Thomaz, Edison},
doi = {10.1145/3314404},
eprint = {1810.08691},
issn = {2474-9567},
journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
title = {{Audio-Based Activities of Daily Living (ADL) Recognition with Large-Scale Acoustic Embeddings from Online Videos}},
year = {2019}
}
@article{Barchiesi2015,
abstract = {In this article, we present an account of the state of the art in acoustic scene classification (ASC), the task of classifying environments from the sounds they produce. Starting from a historical review of previous research in this area, we define a general framework for ASC and present different implementations of its components. We then describe a range of different algorithms submitted for a data challenge that was held to provide a general and fair benchmark for ASC techniques. The data set recorded for this purpose is presented along with the performance metrics that are used to evaluate the algorithms and statistical significance tests to compare the submitted methods.},
author = {Barchiesi, Daniele and Giannoulis, D. Dimitrios and Stowell, Dan and Plumbley, Mark D.},
doi = {10.1109/MSP.2014.2326181},
issn = {10535888},
journal = {IEEE Signal Processing Magazine},
title = {{Acoustic Scene Classification: Classifying environments from the sounds they produce}},
year = {2015}
}
@inproceedings{Lee2013,
abstract = {Recently unsupervised learning algorithms have been successfully used to represent data in many of machine recognition tasks. In particular, sparse feature learning algorithms have shown that they can not only discover meaningful structures from raw data but also outperform many hand-engineered features. In this paper, we apply the sparse feature learning approach to acoustic scene classification. We use a sparse restricted Boltzmann machine to capture manyfold local acoustic structures from audio data and represent the data in a high-dimensional sparse feature space given the learned structures. For scene classification, we summarize the local features by pooling over audio scene data. While the feature pooling is typically performed over uniformly divided segments, we suggest a new pooling method, which first detects audio events and then performs pooling only over detected events, considering the irregular occurrence of audio events in acoustic scene data. We evaluate the learned features on the IEEE AASP Challenge development set, comparing them with a baseline model using mel-frequency cepstral coefficients (MFCCs). The results show that learned features outperform MFCCs, event-based pooling achieves higher accuracy than uniform pooling and, furthermore, a combination of the two methods performs even better than either one used alone. {\textcopyright} 2013 IEEE.},
author = {Lee, Kyogu and Hyung, Ziwon and Nam, Juhan},
booktitle = {IEEE Workshop on Applications of Signal Processing to Audio and Acoustics},
doi = {10.1109/WASPAA.2013.6701893},
isbn = {9781479909728},
keywords = {acoustic scene classification,environmental sound,event detection,feature learning,max-pooling,restricted Boltzmann machine,sparse feature representation},
title = {{Acoustic scene classification using sparse feature learning and event-based pooling}},
year = {2013}
}
@article{Stowell2015,
abstract = {For intelligent systems to make best use of the audio modality, it is important that they can recognize not just speech and music, which have been researched as specific tasks, but also general sounds in everyday environments. To stimulate research in this field we conducted a public research challenge: the IEEE Audio and Acoustic Signal Processing Technical Committee challenge on Detection and Classification of Acoustic Scenes and Events (DCASE). In this paper, we report on the state of the art in automatically classifying audio scenes, and automatically detecting and classifying audio events. We survey prior work as well as the state of the art represented by the submissions to the challenge from various research groups. We also provide detail on the organization of the challenge, so that our experience as challenge hosts may be useful to those organizing challenges in similar domains. We created new audio datasets and baseline systems for the challenge; these, as well as some submitted systems, are publicly available under open licenses, to serve as benchmarks for further research in general-purpose machine listening.},
author = {Stowell, Dan and Giannoulis, Dimitrios and Benetos, Emmanouil and Lagrange, Mathieu and Plumbley, Mark D.},
doi = {10.1109/TMM.2015.2428998},
issn = {15209210},
journal = {IEEE Transactions on Multimedia},
keywords = {Audio databases,event detection,machine intelligence,pattern recognition},
title = {{Detection and Classification of Acoustic Scenes and Events}},
year = {2015}
}
@article{Potamitis2014,
abstract = {The primary purpose for pursuing this research is to present a modular approach that enables reliable automatic bird species identification on the basis of their sound emissions in the field. A practical and complete computer-based framework is proposed to detect and time-stamp particular bird species in continuous real field recordings. Acoustic detection of avian sounds can be used for the automatized monitoring of multiple bird taxa and querying in long-term recordings for species of interest for researchers, conservation practitioners, and decision makers, such as environmental indicator taxa and threatened species. This work describes two novel procedures and offers an open modular framework that detects and time-stamps online calls and songs of target bird species and is fast enough to report results in reasonable time for non-processed field recordings of many thousands files and is generic enough to accommodate any species. The framework is evaluated on two large corpora of real field data, targeting the calls and songs of American Robin Turdus migratorius, a Northamerican oscine passerine (true songbird) and the Common Kingfisher Alcedo atthis, a non-passerine species with a wide distribution throughout Eurasia and North Africa. With the aim of promoting the widespread use of digital autonomous recording units (ARUs) and species recognition technologies the processing code and a large corpus of audio recordings is provided in order to enable other researchers to perform and assess comparative experiments. {\textcopyright} 2014 Elsevier Ltd. All rights reserved.},
author = {Potamitis, Ilyas and Ntalampiras, Stavros and Jahn, Olaf and Riede, Klaus},
doi = {10.1016/j.apacoust.2014.01.001},
issn = {0003682X},
journal = {Applied Acoustics},
keywords = {Bird recognition,Birdsong detection,Computational ecology},
title = {{Automatic bird sound detection in long real-field recordings: Applications and tools}},
year = {2014}
}
@inproceedings{Wang2016,
abstract = {Multimedia event detection (MED) is the task of detecting given events (e.g. birthday party, making a sandwich) in a large collection of video clips. While visual features and automatic speech recognition typically provide the best features for this task, nonspeech audio can also contribute useful information, such as crowds cheering, engine noises, or animal sounds. MED is typically formulated as a two-stage process: the first stage generates clip-level feature representations, often by aggregating frame-level features; the second stage performs binary or multi-class classification to decide whether a given event occurs in a video clip. Both stages are usually performed «statically», i.e. using only local temporal information, or bag-of-words models. In this paper, we introduce longer-range temporal information with deep recurrent neural networks (RNNs) for both stages. We classify each audio frame among a set of semantic units called «noisemes» the sequence of frame-level confidence distributions is used as a variable-length clip-level representation. Such confidence vector sequences are then fed into long short-term memory (LSTM) networks for clip-level classification. We observe improvements in both frame-level and clip-level performance compared to SVM and feed-forward neural network baselines.},
author = {Wang, Yun and Neves, Leonardo and Metze, Florian},
booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
doi = {10.1109/ICASSP.2016.7472176},
isbn = {9781479999880},
issn = {15206149},
keywords = {Multimedia event detection (MED),long short-term memory (LSTM),noisemes,recurrent neural networks (RNNs)},
title = {{Audio-based multimedia event detection using deep recurrent neural networks}},
year = {2016}
}
@inproceedings{Eronen2006,
abstract = {The aim of this paper is to investigate the feasibility of an audio-based context recognition system. Here, context recognition refers to the automatic classification of the context or an environment around a device. A system is developed and compared to the accuracy of human listeners in the same task. Particular emphasis is placed on the computational complexity of the methods, since the application is of particular interest in resource-constrained portable devices. Simplistic low-dimensional feature vectors are evaluated against more standard spectral features. Using discriminative training, competitive recognition accuracies are achieved with very low-order hidden Markov models (1-3 Gaussian components). Slight improvement in recognition accuracy is observed when linear data-driven feature transformations are applied to mel-cepstral features. The recognition rate of the system as a function of the test sequence length appears to converge only after about 30 to 60 s. Some degree of accuracy can be achieved even with less than 1-s test sequence lengths. The average reaction time of the human listeners was 14 s, i.e., somewhat smaller, but of the same order as that of the system. The average recognition accuracy of the system was 58{\%} against 69{\%}, obtained in the listening tests in recognizing between 24 everyday contexts. The accuracies in recognizing six high-level classes were 82{\%} for the system and 88{\%} for the subjects. {\textcopyright} 2006 IEEE.},
author = {Eronen, Antti J. and Peltonen, Vesa T. and Tuomi, Juha T. and Klapuri, Anssi P. and Fagerlund, Seppo and Sorsa, Timo and Lorho, Ga{\"{e}}tan and Huopaniemi, Jyri},
booktitle = {IEEE Transactions on Audio, Speech and Language Processing},
doi = {10.1109/TSA.2005.854103},
issn = {15587916},
keywords = {Audio classification,Context awareness,Feature extraction,Hidden markov models (hmms)},
title = {{Audio-based context recognition}},
year = {2006}
}
@article{Van2013,
author = {{Van Nort}, Doug and Oliveros, Pauline and Braasch, Jonas},
journal = {Journal of New Music Research},
number = {4},
pages = {303--324},
title = {{Electro/acoustic improvisation and deeply listening machines}},
volume = {42},
year = {2013}
}
@inproceedings{Nam2012,
abstract = {We present a data-processing pipeline based on sparse feature learning and describe its applications to music annotation and retrieval. Content-based music annotation and retrieval systems process audio starting with features. While commonly used features, such as MFCC, are handcrafted to extract characteristics of the audio in a succinct way, there is increasing interest in learning features automatically from data using unsupervised algorithms. We describe a systemic approach applying feature-learning algorithms to music data, in particular, focusing on a high-dimensional sparse-feature representation. Our experiments show that, using only a linear classifier, the newly learned features produce results on the CAL500 dataset comparable to state-of-the-art music annotation and retrieval systems. {\textcopyright} 2012 International Society for Music Information Retrieval.},
author = {Nam, Juhan and Herrera, Jorge and Slaney, Malcolm and Smith, Julius},
booktitle = {Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012},
isbn = {9789727521449},
title = {{Learning sparse feature representations for music annotation and retrieval}},
year = {2012}
}
@inproceedings{Geiger2013,
abstract = {This work describes a system for acoustic scene classification using large-scale audio feature extraction. It is our contribution to the Scene Classification track of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (D-CASE). The system classifies 30 second long recordings of 10 different acoustic scenes. From the highly variable recordings, a large number of spectral, cepstral, energy and voicing-related audio features are extracted. Using a sliding window approach, classification is performed on short windows. SVM are used to classify these short segments, and a majority voting scheme is employed to get a decision for longer recordings. On the official development set of the challenge, an accuracy of 73 {\%} is achieved. SVM are compared with a nearest neighbour classifier and an approach called Latent Perceptual Indexing, whereby SVM achieve the best results. A feature analysis using the t-statistic shows that mainly Mel spectra are the most relevant features. {\textcopyright} 2013 IEEE.},
author = {Geiger, Jurgen T. and Schuller, Bjorn and Rigoll, Gerhard},
booktitle = {IEEE Workshop on Applications of Signal Processing to Audio and Acoustics},
doi = {10.1109/WASPAA.2013.6701857},
isbn = {9781479909728},
keywords = {Computational auditory scene analysis,acoustic scene recognition,feature extraction},
title = {{Large-scale audio feature extraction and SVM for acoustic scene classification}},
year = {2013}
}
@article{Krug2002,
abstract = {In 1996, the World Health Assembly declared violence a major public health issue. To follow up on this resolution, on Oct 3 this year, WHO released the first World Report on Violence and Health. The report analyses different types of violence including chil abuse and neglect, youth violence, intimate partner violence, sexual violence, elder abuse, self-directed violence, and collective violence. For all these types of violence, the report explores the magnitude of the health and social effects, the risk and protective factors, and the types of prevention efforts that have been initiated. The launch of the report will be followed by a 1-year Global Campaign on Violence Prevention, focusing on implementation of the recommendations. This article summarises some of the main points of the world report.},
author = {Krug, Etienne G. and Mercy, James A. and Dahlberg, Linda L. and Zwi, Anthony B.},
doi = {10.1016/S0140-6736(02)11133-0},
issn = {01406736},
journal = {Lancet},
title = {{The world report on violence and health}},
year = {2002}
}
@inproceedings{Giannakopoulos2010,
abstract = {In this paper we present our research towards the detection of violent scenes in movies, employing fusion methodologies, based on learning. Towards this goal, a multi-step approach is followed: initially, automated auditory and visual processing and analysis is performed in order to estimate probabilistic measures regarding particular audio and visual related classes. At a second stage, a meta-classification architecture is adopted, which combines the audio and visual information, in order to classify mid-term video segments as "violent" or "non-violent". The proposed scheme has been evaluated on a real dataset from 10 films. {\textcopyright} Springer-Verlag Berlin Heidelberg 2010.},
author = {Giannakopoulos, Theodoros and Makris, Alexandros and Kosmopoulos, Dimitrios and Perantonis, Stavros and Theodoridis, Sergios},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-12842-4_13},
isbn = {3642128416},
issn = {03029743},
keywords = {Multi-modal video classification,Violence detection},
title = {{Audio-visual fusion for detecting violent scenes in videos}},
year = {2010}
}
@article{Ali2018,
abstract = {Violence is autonomous, the contents that one would not let children to see in movies or web videos. This is a challenging problem due to strong content variations among the positive instances. To solve this problem, implementation of deep neural network to classify the violence content in videos is proposed. Currently, deep neural network has shown its efficiency in natural language processing, fraud detection, social media, text classification, image classification. Regardless of the conventional methods applied to overcome this issue, but these techniques seem insufficiently accurate and does not adopt well to certain webs or user needs. Therefore, the purpose of this study is to assess the classification performances on violence video using Deep Neural Network (DNN). Hence, in this paper different architectures of hidden layers and hidden nodes in DNN have been implemented using the try-error method and equation based method, to examine the effect of the number of hidden layers and hidden nodes to the classification performance. From the results, it indicates 53{\%} accuracy rate for try and error approach, meanwhile for equation based approach it indicates 51{\%} accuracy rate.},
author = {Ali, Ashikin and Senan, Norhalina},
doi = {10.1007/978-3-319-72550-5_22},
file = {:Users/Oscar/Downloads/Ali-Senan2018{\_}Chapter{\_}ViolenceVideoClassificationPer.pdf:pdf},
isbn = {9783319725499},
issn = {21945357},
journal = {Advances in Intelligent Systems and Computing},
keywords = {Artificial neural network,Classification,Deep neural network,Violence video},
pages = {225--233},
title = {{Violence video classification performance using deep neural networks}},
volume = {700},
year = {2018}
}
@inproceedings{Chua2014,
abstract = {Modern elevators are equipped with closed-circuit television (CCTV) cameras to record videos for post-incident investigation rather than providing proactive event monitoring. While there are some attempts at automated video surveillance, events such as urinating, vandalism, and crimes that involved vulnerable targets may not exhibit significant visual cues. On contrary, such events are more discerning from audio cues. In this work, we propose a hierarchical audio-visual surveillance framework for elevators. Audio analytic module acts as the front line detector to monitor for such events. This means audio cue is the main determining source to infer the event occurrence. The secondary inference process involves queries to visual analytic module to build-up the evidences leading to event detection. We validate the performance of our system at a residential trial site and the initial results are promising. {\textcopyright} 2014 Springer International Publishing.},
author = {Chua, Teck Wee and Leman, Karianto and Gao, Feng},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-04117-9_5},
isbn = {9783319041162},
issn = {03029743},
title = {{Hierarchical audio-visual surveillance for passenger elevators}},
year = {2014}
}
@inproceedings{Demarty2013,
abstract = {This paper provides a description of the MediaEval 2013 Affect Task Violent Scenes Detection. This task, which is proposed for the third year to the research community, derives directly from a Technicolor use case which aims at easing a user's selection process from a movie database. This task will therefore apply to movie content. We provide some insight into the Technicolor use case, before giving details on the task itself, which has seen some changes in 2013. Dataset, annotations, and evaluation criteria as well as the required and optional runs are described.},
author = {Demarty, Claire H{\'{e}}l{\`{e}}ne and Penet, C{\'{e}}dric and Schedl, Markus and Ionescu, Bogdan and Quang, Vu Lam and Jiang, Yu Gang},
booktitle = {CEUR Workshop Proceedings},
issn = {16130073},
title = {{The MediaEval 2013 affect task: Violent Scenes Detection}},
year = {2013}
}
@inproceedings{Giannakopoulos2006,
abstract = {This work studies the problem of violence detection in audio data, which can be used for automated content rating. We employ some popular frame-level audio features both from the time and frequency domain. Afterwards, several statistics of the calculated feature sequences are fed as input to a Support Vector Machine classifier, which decides about the segment content with respect to violence. The presented experimental results verify the validity of the approach and exhibit a better performance than the other known approaches. {\textcopyright} Springer-Verlag Berlin Heidelberg 2006.},
author = {Giannakopoulos, Theodoros and Kosmopoulos, Dimitrios and Aristidou, Andreas and Theodoridis, Sergios},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/11752912_55},
isbn = {354034117X},
issn = {03029743},
title = {{Violence content classification using audio features}},
year = {2006}
}
@inproceedings{Bautista-Duran2017,
abstract = {Detecting violence is an important task due to the amount of people who suffer its effects daily. There is a tendency to focus the problem either in real situations or in non real ones, but both of them are useful on its own right. Until this day there has not been clear effort to try to relate both environments. In this work we try to detect violent situations on two different acoustic databases through the use of crossed information from one of them into the other. The system has been divided into three stages: feature extraction, feature selection based on genetic algorithms and classification to take a binary decision. Results focus on comparing performance loss when a database is evaluated with features selected on itself, or selection based in the other database. In general, complex classifiers tend to suffer higher losses, whereas simple classifiers, such as linear and quadratic detectors, offers less than a 10{\%} loss in most situations.},
author = {Bautista-Duran, Marta and Garc{\'{i}}a-G{\'{o}}mez, Joaqu{\'{i}}n and Gil-Pita, Roberto and S{\'{a}}nchez-Hevia, H{\'{e}}ctor and Mohino-Herranz, Inma and Rosa-Zurera, Manuel},
booktitle = {ICPRAM 2017 - Proceedings of the 6th International Conference on Pattern Recognition Applications and Methods},
doi = {10.5220/0006195004560462},
isbn = {9789897582226},
keywords = {Audio processing,Feature selection,Fictional environment,Real environment,Violence detection},
title = {{Acoustic detection of violence in real and fictional environments}},
year = {2017}
}
@inproceedings{Garcia-Gomez2016,
abstract = {Violence continues being an important problem in the society. Thousands of people suffer its effects every day and statistics show this number has maintained or almost increased recently. In the modern environment of smart cities there is a necessity to develop a system capable of detecting if a violent situation is taking place or not. In this paper we present an automatic acoustic violence detection system for smart cities, integrating both signal processing and pattern recognition techniques. The proposed software has been implemented in three steps: feature extraction in time and frequency domain, genetic algorithm implementation in order to select the best features, and classification to take a binary decision. Results derived from the experiments show that MFCCs are the best features for violence detection, and others like pitch or short time energy have also a good performance. In other words, features that can distinguish between voiced and unvoiced frames seem to be a good election for violence detection in real environments.},
author = {Garc{\'{i}}a-G{\'{o}}mez, Joaqu{\'{i}}n and Bautista-Dur{\'{a}}n, Marta and Gil-Pita, Roberto and Mohino-Herranz, Inma and Rosa-Zurera, Manuel},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-48799-1_52},
isbn = {9783319487984},
issn = {16113349},
keywords = {Audio features,Audio processing,Feature selection,Violence detection},
title = {{Violence detection in real environments for smart cities}},
year = {2016}
}
