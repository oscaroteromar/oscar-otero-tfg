% !TeX spellcheck = en_GB
% State-of-the-art text

\section{Acoustic Scene Classification and Acoustic Event Detection and Classification}

	Acoustic scene classification, also known as ASC, refers to the association of an audio sequence to a certain semantic label that describes the environment in which it took place \cite{Barchiesi2015}. With this idea in mind, the classification of acoustic sceneries have been attacked with two different kinds of concepts: soundscape cognition, i.e. understanding how the human being perceives the sounds in a subjective way from the physical environment that surrounds them  \cite{Dubois2006}, and working on new computational methods that may help and allow to perform this task in an automatic way by using machine learning and processing signal techniques, which is also called, computational auditory scene analysis (CASA) \cite{Wang2006}. In many applications this notion \todo{\textit{notion} here refers to ASC. Clear?} can be found based on allowing devices to achieve benefits and information from the situation it is placed in \cite{Eronen2006}, also for medical utilizations \cite{Bahoura2009}, as a tool for musical recognition \cite{Van2013} or for a complement to computer vision.
	
	While all the advances in the ASC field took place, another related area has evolved during last years. Some computational work has been deployed for the tasks of acoustic event detection and classification, also known as AED/C. It can be described as the processing or treatment of sound signals in order to convert them into significant descriptions that match a listener's sensing of the events and sources that compose the acoustic environment \cite{Temko2009}. The detection part consists on identifying the events in a temporal stream of audio and assign them a label. The result is usually accompanied by the time interval in which the occurrence is set. However, the classification is a task that acts directly on the event that has been already isolated and has the purpose of designating a label or class to the sound \cite{Temko2007}. There exist plenty of applications in which these techniques have been used for, as in the medical field \cite{Bahoura2010}, in biological topics such as bird noise detection \cite{Potamitis2014}, and for multimedia information retrieval from video sources in social media \cite{Wang2016}.

\subsection{Features and methods}
	
	In the literature, a bunch of works have been published related to ASC field. These can be sorted into two different currents in regard to how the problem is addressed. One of them considers the scene as a single instance with the purpose of representing it through a long-term statistical distribution that models a set of low-level features \cite{Stowell2015}. There exist different ways of characterizing an acoustic event or scene for this type of method. In previous works, some of the common habits usually utilized for speech recognition had the main role in the extraction of features, such as the fundamental frequency, or F0, F0 envelope and the probability of voicing. Apart from these, also spectral features, as Mel-Spectrum bins, zero crossing rate (ZCR) and spectral flux (SF), and energy features, such as the energy in bands or the logarithmic-energy \cite{Geiger2013} had an important job on this task. However, the best results have been achieved with what is called  \acrfull{mfcc} \todo{Review acronyms list} which is defined as a cepstral feature\todo{Explain more MFCC?}, \doubt{which will be explained further on}. This kind of characteristics extracted from the audio can be called low-level descriptors and they are usually combined with algorithms and methods to address the classification task. In this "bag-of-frames" approach, in which the scene is considered as a single object, a typical technique was to model the samples features into global statistical characteristics from the local descriptors by using Gaussian Mixture Models (GMM) \cite{Aucouturier2007}.
	
	There is another path to dig for acoustic scene classification, which consists on including a representation of data previous to the classification which is based on transforming the scene by using a set of high level features normally obtained with a vocabulary or dictionary formed by acoustic atoms. These are usually a depiction of events or streams within the scene and do not need to be known a priori \cite{Stowell2015}. Apart from the typical well-known audio features, the ones named above as low-level descriptors, there exists other acoustic characteristics which may seem to be hidden in the data but can be found by using unsupervised-learning methods. This is the way to act when dealing with the acoustic atoms mentioned above. 
	One of the approaches that can be found in the literature about this idea is based on the use of a previously learned overcomplete dictionary that is utilized to sparsely decomposed the spectrogram of audio. This dictionary will be used by an encoder which has the labour of mapping new input data to real similar version of their own sparse representation in a fast and efficient way. Finally, the obtained codes will feed a Support Vector Machine Classifier, also known as SVM, used for the task of music genre prediction \cite{Henaff2011}. \todo{Include results?}
	
	Another job done in the sparse-feature representation framework presents a way of mixing high feature learning techniques with a pooling method for the objective of music information retrieval and annotation. After some preprocessing of the audio signals data, three feature-learning algorithms are trained finding that sparse restricted Boltzmann machine (sparse-RBM) gets better results than K-means and Sparse Coding. Once the features are obtained, an extra step takes place before performing the classification task, the one called pooling and aggregation. The goal of this procedure is to achieve a feature representation for a long sequence as a song is. Since when joining short-term features that belong to small segments inside the song may result in a loss of their local meaning, a max-pooling operation is computed over each subsegment in order just to consider the maximum value for each feature dimension. After that, these are aggregated by computing the average. The max-pooling contribution resides on reducing the smoothing effect when averaging the values \cite{Nam2012}. This approach is feasible because of the homogeneity in music data. However, this technique could be a bit risky when dealing with acoustic scenes. For this case, a modified version of this method has been proposed. Taking into account that the presence of events is less frequent, instead of considering the whole long sequence to apply the max-pooling for, it will just be used in those segments that had been already detected as significant events by establishing a threshold value and setting an onset and offset that allow to know the start and end time \cite{Lee2013}.
	\todo{Include picture of the pipeline?}
	
	When the target is the acoustic event detection and classification, the working method used is really similar to the one used for ASC. Then, it is not surprising that most of the works found in the literature address this task with the use of MFCC as features and with techniques such as HMM or GMM. For the purpose of finding the desired events, the whole detection process can be split in two parts. Firstly, a classification of already isolated events should be executed in order to build a vocabulary of acoustic actions. In this case, the data used belong to short-term sequences that must strongly show the semantic meaning of the corresponding event. This is important because there may be more acoustic representations in the same short segment than the one that is desired to detect, but this must stand out among the others.  Then, for the detection part, the input data will be composed by long tracks so time allocation of the events will be implemented. So, after obtaining the different short segments from breaking the long sequence up, they will be classify taking into account the results from the first step \cite{Mesaros2010}. \todo{Go deeper? HMM, GMM, MFCC}


\section{Violent Event Detection}

	All the multimedia information available can be applied to many fields and in different connotations. One of the slopes that has appeared in the acoustic scenes and events sphere is the one applied to violence. For this case, an essential point before addressing any problem is to decide what kind of definition the word violence is going to adopt since it is a really subjective concept. An objective perspective has been given by the World Health Organization as "The  intentional  use  of physical  force  or  power,  threatened  or  actual,  against oneself, another person, or against a group or community, that either results in or has a high likelihood of resulting in injury,  death,  psychological  harm,  maldevelopment  or deprivation" \cite{Krug2002}. There exists other definitions found in different works as "physical violence or accident resulting in human injury or pain" \cite{Demarty2013} and "any situation or action that may cause physical or mental harm to one or more persons" \cite{Giannakopoulos2006}.
	
	Recent studies have treated this problem in different ways due to all types of conditions that this may take place in. During the last years, the possibility of creating and providing audiovisual content has grown widely which has led to an enormous amount of multimedia data. Within this content, the variety of topics is uncountable and some of them may be considered unappropriated for certain parts of the audience. This is the reason why there have just been done works related to the field of video content analysis and detection of violence. In some cases, audio and image features have been combined to address these problem \cite{Giannakopoulos2010}. However, it has been found that sound information could be really useful and a more efficient way of working compared to image, since it is easier to process and the cost is lower. Related works have utilized audio features in the time-domain and in the frequency-domain, similar to the ones explained for ASC, then combined with a normal SVM classifier \cite{Giannakopoulos2006}. Other researches have tried more complicated models with the intention of improving the classification task. It is the case of using DNN, i.e., Deep Neural Networks, fed with both image and audio data, which performs the task more efficiently \cite{Ali2018}. Violence detection has been used for other applications such as video surveillance. For example, one of the scenarios for this purpose consists on preventing violent acts inside elevators \cite{Chua2014}. For this case, the considered dangerous situations are composed of anti-social actions that are likely to happen in this kind of places, concretely, urinating, vandalism and attacks on vulnerable victims, such as women, children or elderly. The framework proposed is based on audio-visual data, but the master classifier will be driven by audio, due to the possible subtleness of the scenes that are desired to detect. So, first the audio incident detector will trigger the process when a non-silent event takes place. Then, the image processing will begin in order to extract information related to who is involved in the action and how aggressive is it. Another utilization of the surveillance approach is its use for the evolution of smart cities \cite{Garcia-Gomez2016}. For this goal, since the system will be implemented in real-life environments, one of the advantages about working with data coming from sounds is the respect for privacy, that, otherwise, using video recordings it would be violated.
	
	The difference in these two applications, apart from the task they are addressing, resides on the data they are working with. For violent content analysis, the data usually comes from fictional audio sources as movies or video-games. However, for real-environment systems, the data is extracted straight from actual day-to-day life situations. In this second case, some disadvantages can be appreciated. For example, the signals are not preprocessed, which means the original properties of the sound are not modified so the processing part before classification becomes tougher. Also, the presence of background noise is more common and loudness of some events, as speech, may vary with time \cite{Bautista-Duran2017}. \todo{Makes sense?}

\subsection{Gender-based violence}

	\doubt{Throughout history, women have been an object of abuse and suffering in many different situations even though in those that were considered as their familiar surroundings. They have been bashed, sexually harmed and psychologically maltreated by those who were supposed to be one of their closest intimates \cite{UnitedNations1989}.} In the same way, in the recent times, late studies have shown that 35\% of women from all over the world have been victims of physical or sexual damage \cite{WHO2013}, and 43\% of women from Europe have declared going through some psychological or mental violence at least once in their lives \cite{EuropeanUnionAgencyforFundamentalRights2014}. In this context, it is necessary to define the concept of gender-based violence, which can be described as the multitude of harmful behaviours that are focused on women and girls just because of their sex, such as female children and wife abuse, sexual assault, dowry-related murder and marital rape, among others. Particularly, violence against women involves any act of verbal or physical force, extortion or lethal denial which has a woman or girl as a target and provokes the physical or psychological hurt, humiliation or irrational privation of liberty and contributes to continue women subordination \cite{Heise1999}. Within this definition, it can be considered that most of the times that these violent situations take place, they are originated due to persons that are supposed to be part of the victims' closest circle of trust, i.e., their husbands or boyfriends. This is called \acrfull{ipv} intimate partner violence (IPV) \todo{Glossary?} and it is recognized as a public health problem affecting women across their life span and may resulting in different undesirable unhealthy outcomes, such as depression, chronic pain and even dead \cite{Beyer2015}.
	
\subsection{Our point of view} \todo{Explicaci√≥n EMPATIA?}

	As a contribution to the EMPATIA-TC project developed by \acrlong{uc3m}, the main goal in this work is to make progress in detecting gender-based violence situations, specifically applied to day-to-day scenes, in which \acrshort{ipv} is likely to be present. One of the parts from the proposed system is composed by wearable devices that the victim can carry to collect diverse types of information and process them to obtain conclusions and increase the efficiency. Among these accessories, we can find a pendant that pays attention to the user's voice and the surrounding audio to analyse what is happening at a certain moment. For our purpose, the interesting part resides on achieving auditory data so as to detect violent incidents that are formed by sounds already known for characterizing these episodes considered dangerous by the victim.
	
	The definition that is assigned to violence is really important in order to define which audio events should be taken into account. However, considering the subjectiveness of this concept, categorizing violence for every type of user is an extremely difficult task. For this reason, the final idea to answer this question is to make the victim able to decide which kind of hearing events the system must be aware of. In the complete project, this can be carried out by a phone user interface which displays a list of sound events and she has the labour of picking up those that are violent according to her criteria. Since the development of this tool is out of the scope of this work, we have decided to implement a simpler mechanism which will be explained \doubt{further on}.
	
\section{Databases} \todo{Talk about different DB: TUT, DCASE...?}

	A fundamental objective was to find a database that allows for building a system with these characteristics, so a rich variety of acoustic events is needed with a essential big representation of violent sounds.
	
	\todo{Insert table}
