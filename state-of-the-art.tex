% !TeX spellcheck = en_GB
% State-of-the-art text

	Since the main topic of this work, acoustic violent scene detection, has not been researched as such in the literature before, we decided to start our search by addressing close two topics that have been largely studied for the last years related to acoustics scenes and events also including some aspects on violence detection mainly aimed at the detection of violent scenes in movies and video-surveillance.
	
\section{\acrlong{asc} and \acrlong{aedc}}

	 \acrfull{asc} refers to the association of an audio sequence to a certain semantic label that describes the environment in which it took place \cite{Barchiesi2015}. With this idea in mind, the classification of acoustic scenarios has been tackled with two different kinds of concepts: soundscape cognition, i.e. understanding how the human being perceives the sounds subjectively from the physical environment that surrounds them  \cite{Dubois2006}, and  \acrfull{casa}, that is working on new computational methods that may help automatize this task through machine learning and processing signal techniques \cite{Wang2006}. This notion can have many applications, such as content recognition --by allowing devices to obtain benefits and information from its situation \cite{Eronen2006}--, for medical usage \cite{Bahoura2009}, as a tool to aid  musical recognition \cite{Van2013} or as a complement to \acrfull{cv}.
	
	Simultaneously to these advances in the \acrshort{asc}, another related area has evolved during the last years: %. Some computational work has been deployed for the tasks of 
	\acrfull{aedc}. It can be described as the processing or treatment of sound signals in order to convert them into significant descriptions that match a listener's sensing of the events and sources composing the acoustic environment \cite{Temko2009}. The detection part consists in identifying the events in a temporal stream of audio and labelling them. The result is usually accompanied by the time interval in which the occurrence is set. However, classification is a task that acts directly on the event that has been already isolated and has the purpose of designating a label or class to the sound \cite{Temko2007}. These techniques also have plenty of applications, e.g., in the medical field \cite{Bahoura2010}, in biological topics such as bird noise detection \cite{Potamitis2014}, and for multimedia information retrieval from video sources in social media \cite{Wang2016}, etc.

	% Version 2 of features and methods
	\input{sota-v2.tex}


\subsection{Data augmentation}
\label{subsection:data-augmentation}

	\acrlong{dnn} models, as the ones just mentioned above, have proved to achieve very good results for audio classification tasks by extracting information from audio-spectral type of features. Sometimes, the need of a huge quantity of data becomes a main problem due to the lack of enough correctly labeled samples, which result in an impossible actual employment of these deep and complex methods \cite{Salamon2017}. In order to obtain to solve this problem, some methods have been developed inside the field of data augmentation. This consists in a strategy which has been commonly adopted so as to increase the amount of data in the training process of the models and avoid some problems as overfitting\footnote{Overfitting is a term referring to the disability of the model to generalize a pattern from the training data which means that the \acrshort{nn} achieves very different results for train and test sets \cite{Jabbar2015}.} so the model can be more robust \cite{Ko2015}.
	
	The way to obtain more training samples can vary depending on the task that is been developed. In the case of audio data, these can subjected to some low-level deformations in order to generate observations that maintain the semantic of the original ones but are actually different for the model. In \cite{Salamon2017a}, they perform a couple of transformations such as time stretching of the signal, i.e. by changing the speed of the sample; pitch shifting, by raising or lowering the pitch; adding background noise and compressing the dynamic range of the audio sample. Also, other types of augmentation can be found in the literature that work in a more synthetic manner. For example, in \cite{Wei2018}, it is provided a technique based on sample mixed data augmentation for a domestic audio tagging application. Among the different proposed methods, one of them is the \textit{SamplePairing}, which generates a synthetic new observation by computing the average between two input vectors.
	
	In our case, we opted for a technique also based on synthetic samples creation which is called \acrfull{smote}. This selects a random sample from the less populated classes within the training data, pick one of its closest neighbours and generates a new observation placed between them in the feature space. It is explained in much more detailed in \ref{subsection:smote}.
	
\subsection{Transfer learning}
\label{subsection:transfer-learning}

	Another option to work with deep models, as the ones mentioned in \ref{section:features-and-methods-for-asc-aedc}, consists in pre-training them with available huge collections of data so as to use them in a pre-processing stage and then apply them to models considerably much less complicated to address other type of problems. This is possible due to a kind of techniques commonly known as transfer learning and domain adaptation.
	
	The typical consideration in plenty of machine learning tasks consists on extracting the training and testing subsets of data from the same feature space and same distribution. When one of these initial assumptions change, it is necessary to rebuild the whole model from the initial point, including new training data, which means a lot of computational cost and loss of efficiency \cite{Pan2010}. Its working manner can be explained as an analogy of how humans transforms their ability on a certain task to obtain knowledge for other purpose. An example could be how musicians apply their previous experience to get to know faster how to play another instrument.
	
	The first work in which this topic has been treated widely was in 1995 in the workshop \textit{Learning to Learn} \cite{Sarkar2018} and since then many approaches have arisen and renamed the same idea as knowledge consolidation or inductive transfer. However, it was 10 ten years later, in 2005, when the first idea of the ability of a system to identify and apply previously learned skills to completely new problems appeared from the hand of the \acrfull{baa} 05-29 of \acrfull{darpa}â€™s \acrfull{ipto} \cite{Pan2010}. This can be expressed as a relation between a \textit{source} task, where the abilities are learned, and \textit{target} task, the novel problem that needs to be resolved. As a difference with other similar methods, in this concept of transfer learning the roles of these two are not equal since the weight of the target is much heavier. In figure \ref{fig:mesh4}, the difference between a common machine learning approach and the use of transfer learning is included.
	
	% Difference between machine learning and transfer learning
	\begin{figure}[h]
		\centering
		\captionsetup{justification=centering}
		\includegraphics[width=0.8\linewidth]{ml_tl}
		\caption{Difference between traditional machine learning (a) process and feature learning (b) \cite{Pan2010}}
		\label{fig:mesh4}
	\end{figure}
	
	The same idea can be understood from a mathematical point of view as the analysis of the relationship between the two different spaces from types of targets \cite{Pan2010}. 
	
	Considering a \textit{domain} $D$ that is composed by a feature space denoted by $X$ and a marginal probability named $P(X)$, where $X = \{x_1,..., x_n\} \in \chi$. The whole domain can be expressed as $D = \{\chi, P(X)\}$ where $x_{i}$ is a certain vector inside the feature space.
	
	In the same way, a \textit{task} can be defined as $T$ formed by a label space $\gamma$ and an objective predictive function $\eta$. The task formulation is $T = \{\gamma, \eta\}\}$. This predictive function cannot be observed, however the intention is to learn it from the training data, that is composed by pairs of the form $\{x_i, y_i\}$, $x_i \in X$ and $y_i \in \gamma$.
	
	The predictive function $\eta$ can be used to predict a corresponding label of a new sample $x$. From a probabilistic perspective, this new label can be expressed as $P(y|x)$. So, the task $T$ can be defined as $T = \{y, P(Y|X)\}$, in which $Y = \{y_1,..., y_n\} \in \gamma$. For each vector $x_i$, the function $\eta$ finds a prediction $y_i$.
	
	Once these parameters have been defined, considering the source domain $D_S$, task of source domain $T_s$, the target domain as $D_T$ and its respective task as $T_T$, the transfer learning has the purpose of obtain the condition distribution in the target domain $P(Y_T|X_T)$ with the information extracted from $D_S$ and $T_S$ where $D_S \neq D_T$ or $T_S \neq T_T$ \cite{Ruder2017}.
	
	We have took advantage of this technique in order to obtain features for our final model. As it is explained further on in \ref{subsection:vggish}, the network \acrshort{vgg}ish has been pretrained with a huge dataset with similar data as the one we take. Thanks to this method of transferring learning from models, we can extract our features without need of training the complex \acrshort{vgg}ish, which eases the whole process in a considerable way.
	
\section{\acrlong{ved}}
\label{section:ved}

	All the multimedia information available can be applied to many fields and with different connotations. One of the variants that has appeared in the acoustic scenes and events field is the one applied to violence detection. For this case, an essential point before addressing any problem is to decide what kind of definition the word 'violence' is going to adopt since it is a subjective concept. An objective perspective has been given by the World Health Organization as "The  intentional  use  of physical  force  or  power,  threatened  or  actual,  against oneself, another person, or against a group or community, that either results in or has a high likelihood of resulting in injury,  death,  psychological  harm,  maldevelopment  or deprivation" \cite{Krug2002}. There are other definitions found in different works as "physical violence or accident resulting in human injury or pain" \cite{Demarty2013} or "any situation or action that may cause physical or mental harm to one or more persons" \cite{Giannakopoulos2006}.
	
	Recent studies have treated this problem in different ways due to all types of conditions that it may take place in. During the last years, the possibility of creating and providing audiovisual content has grown widely, which has led to an enormous variety of topics in which, some of them, could be considered inappropriate for certain  audiences. This is the reason why there have just been done works related to the field of video content analysis and detection of violence. 
	
	In some cases, audio and image features have been combined to address these problem \cite{Giannakopoulos2010}. However, it has been found that sound information could be really useful and a more efficient way of working compared to image, since it is easier to process and the cost is lower. Related works have utilized audio features in the time-domain and in the frequency-domain, similar to the ones explained for \acrshort{asc}, then combined with a normal SVM classifier \cite{Giannakopoulos2006}. Other researches have tried more complicated models with the intention of improving the classification task. It is the case of using \acrshort{dnn}, fed with both image and audio data, which performs the task more efficiently \cite{Ali2018}. Violence detection has also been used for other applications such as video surveillance. For example, one of the scenarios for this purpose consists on preventing violent acts inside elevators \cite{Chua2014}. For this case, the considered dangerous situations are composed of anti-social actions that are likely to happen in this kind of places, concretely, urinating, vandalism and attacks on vulnerable victims, such as women, children or elderly. The framework proposed is based on audio-visual data, but the master classifier is driven by audio, due to the possible subtleness of the scenes that are desired to detect. So, first the audio incident detector triggers the process when a non-silent event takes place. Then, the image processing begins in order to extract information related to who is involved in the action and how aggressive is it. Another utilization of the surveillance approach is its use for the evolution of smart cities \cite{Garcia-Gomez2016}. For this goal, since the system will be implemented in real-life environments, one of the advantages about working with data coming from sounds is the respect for privacy, that, otherwise, using video recordings would be violated.
	
	The difference in these two applications, apart from the task they are addressing, resides on the data they are working with. For violent content analysis, the data usually comes from fictional audio sources as movies or video-games. However, for real-environment systems, the data is extracted straight from actual day-to-day life situations. In this second case, some disadvantages can be appreciated. For example, the signals are not pre-processed, which means the original properties of the sound are not modified so the processing part before classification becomes tougher. Also, the presence of background noise is more common and loudness of some events, as speech, may vary with time \cite{Bautista-Duran2017}.

\subsection{Gender-based violence}
	
	 In the same way, in the recent times, late studies have shown that 35\% of women from all over the world have been victims of physical or sexual damage \cite{WHO2013}, and 43\% of women from Europe have declared going through some psychological or mental violence at least once in their lives \cite{EuropeanUnionAgencyforFundamentalRights2014}. In this context, it is necessary to define the concept of gender-based violence, which can be described as the set of harmful behaviours that are focused on women and girls just because of their sex, such as female children and wife abuse, sexual assault, dowry-related murder and marital rape, among others. 
	 
	 Particularly, violence against women involves any act of verbal or physical force, extortion or lethal denial which has a woman or girl as a target and provokes the physical or psychological hurt, humiliation or irrational privation of liberty and contributes to continue women subordination \cite{Heise1999}. Within this definition, it can be considered that most of the times that these violent situations take place, they are originated due to persons that are supposed to be part of the victims' closest circle of trust, i.e., their husbands or boyfriends. This is called \acrfull{ipv} and it is recognized as a public health problem affecting women across their life span resulting in different undesirable unhealthy outcomes, such as depression, chronic pain and even dead \cite{Beyer2015}.
	
\subsection{Our point of view}
\label{subsection:our-point-of-view}

	As a contribution to the EMPATIA\footnote{\textit{protEcciÃ³nn integral de las vÃ­ctimas de violencia de gÃ©nero Mediante comPutaciÃ³nn AfecTIva multimodAl} funded by Department  of  Research  and Innovation of Madrid Regional Authority, in the EMPATIA-CM research project(reference  Y2018/TCS-5046)} project developed by \acrlong{uc3m}, the main goal in this work is to make progress in detecting gender-based violence situations, specifically applied to day-to-day scenes, in which \acrshort{ipv} is likely to be present. One of the parts from the proposed system within the whole project is composed by wearable devices that the victim can carry to collect diverse types of information and process them to obtain conclusions and increase the efficiency. Among these accessories, we can find a pendant that senses the user's voice and the surrounding audio to analyse what is happening at a given moment. For our purpose, the interesting part resides on achieving auditory data so as to detect violent incidents that consists of sounds already known for characterizing these episodes considered dangerous by the victim.
	
	The definition that is assigned to violence is really important in order to define which audio events should be taken into account. However, considering the subjectiveness of this concept, categorizing violence for every type of user is an extremely difficult task. For this reason, the final idea to answer this question is to make the victim able to decide which kind of hearing events the system must be aware of. In the complete project, this can be carried out by a phone user interface which displays a list of sound events and she has the labour of picking up those that are violent according to her criteria. Since the development of this tool is out of the scope of this work, we have decided to implement a simpler mechanism which will be explained in subsection \ref{subsection:violent-classes}.
		

\section{Databases}

	A main objective was to find a database that allows to build a system with the desired characteristics. This should satisfy the need of a rich variety of acoustic events with an essential big representation of violent sounds. In the table \ref{table:1} is represented a relation of the different databases that have been considered for the realization of this work.
	
	% Databases table
	\input{databases.tex}
	
	The last three options are the ones that better suit the problem of this project. \textit{VSD Benchmark} was the first option we chose. Within the two ways of working given, the movies and the YouTube videos, the former was the easiest to use since the annotation specified exactly what kind of violent events were present in the scene and the onset and offset time stamps within the whole film. However, this data is copyright restricted and it was necessary to pay for the contents. The latter was accessible but just indicated the presence of violence, without determining the type of event. Another choice was the \textit{Freesound dataset} because it contains all types of videos so we could extract those classes that are more interesting for us. However, it is still under an annotation process and it is not ready to download yet. As a final conclusion, we decided to go for \textit{AudioSet}, which will be explained further on in \ref{section:audioset}.
	
	
