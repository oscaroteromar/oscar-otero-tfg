% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.0 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{none/global//global/global}
    \entry{Barchiesi2015}{article}{}
      \name{author}{4}{}{%
        {{hash=3be2b3b5a2d914953345adb47cb30493}{%
           family={Barchiesi},
           familyi={B\bibinitperiod},
           given={Daniele},
           giveni={D\bibinitperiod}}}%
        {{hash=3a04b84eae59edb51966677067a36215}{%
           family={Giannoulis},
           familyi={G\bibinitperiod},
           given={D.\bibnamedelimi Dimitrios},
           giveni={D\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
        {{hash=720621ffa11137749699f824497e8e0b}{%
           family={Stowell},
           familyi={S\bibinitperiod},
           given={Dan},
           giveni={D\bibinitperiod}}}%
        {{hash=af302f6b1171db4b3b2979f068a9bafe}{%
           family={Plumbley},
           familyi={P\bibinitperiod},
           given={Mark\bibnamedelima D.},
           giveni={M\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
      }
      \strng{namehash}{2558a8d1e36f9596aa286adf5968f03e}
      \strng{fullhash}{5995851aaebc41e48a03ba98ad7c85d5}
      \strng{bibnamehash}{5995851aaebc41e48a03ba98ad7c85d5}
      \strng{authorbibnamehash}{5995851aaebc41e48a03ba98ad7c85d5}
      \strng{authornamehash}{2558a8d1e36f9596aa286adf5968f03e}
      \strng{authorfullhash}{5995851aaebc41e48a03ba98ad7c85d5}
      \field{sortinit}{6}
      \field{sortinithash}{57e57fb8451e7fcfa45d1e069f6d3136}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In this article, we present an account of the state of the art in acoustic scene classification (ASC), the task of classifying environments from the sounds they produce. Starting from a historical review of previous research in this area, we define a general framework for ASC and present different implementations of its components. We then describe a range of different algorithms submitted for a data challenge that was held to provide a general and fair benchmark for ASC techniques. The data set recorded for this purpose is presented along with the performance metrics that are used to evaluate the algorithms and statistical significance tests to compare the submitted methods.}
      \field{issn}{10535888}
      \field{journaltitle}{IEEE Signal Processing Magazine}
      \field{title}{{Acoustic Scene Classification: Classifying environments from the sounds they produce}}
      \field{year}{2015}
      \verb{doi}
      \verb 10.1109/MSP.2014.2326181
      \endverb
    \endentry
    \entry{Dubois2006}{article}{}
      \name{author}{3}{}{%
        {{hash=c6bb81c76a4bfed547a40a0581c1c4c0}{%
           family={Dubois},
           familyi={D\bibinitperiod},
           given={Dani{è}le},
           giveni={D\bibinitperiod}}}%
        {{hash=8e3e2ef8e452aeaed33d27e7a68652c7}{%
           family={Guastavino},
           familyi={G\bibinitperiod},
           given={Catherine},
           giveni={C\bibinitperiod}}}%
        {{hash=2802ee21ee2cbd30f5b0024641241ab0}{%
           family={Raimbault},
           familyi={R\bibinitperiod},
           given={Manon},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{dfc1a8821db0b25a34c11df9fe8d4259}
      \strng{fullhash}{dfc1a8821db0b25a34c11df9fe8d4259}
      \strng{bibnamehash}{dfc1a8821db0b25a34c11df9fe8d4259}
      \strng{authorbibnamehash}{dfc1a8821db0b25a34c11df9fe8d4259}
      \strng{authornamehash}{dfc1a8821db0b25a34c11df9fe8d4259}
      \strng{authorfullhash}{dfc1a8821db0b25a34c11df9fe8d4259}
      \field{sortinit}{7}
      \field{sortinithash}{c818dd9105a2852444fc9f5e145c294e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The present research on cognitive categories mediates between individual experiences of soundscapes and collective representations shared in language and elaborated as knowledge. This approach focuses on meanings attributed to soundscapes in an attempt to bridge the gap between individual perceptual categories and sociological representations. First, results of several free categorisation experiments are presented, namely the categorical structures elicited using soundscape recordings and the underlying principles of organisation derived from the analysis of verbal comments. People categorised sound samples on the basis of semantic features that integrate perceptual ones. Specifically, soundscapes reflecting human activity were perceived as more pleasant than soundscapes where mechanical sounds were predominant. Second, the linguistic exploration of free-format verbal description of soundscapes indicated that the meanings attributed to sounds act as a determinant for sound quality evaluations. Soundscape evaluations are therefore qualitative first as they are semiotic in nature as grounded in cultural values given to different types of activities. Physical descriptions of sound properties have to be reconsidered as cues pointing to diverse cognitive objects to be identified first rather as the only adequate, exhaustive and objective description of the sound itself. Finally, methodological and theoretical consequences of these findings are drawn, highlighting the need to address not only noise annoyance but rather sound quality of urban environments. To do so, cognitive evaluations must be conducted in the first place to identify relevant city users' categories of soundscapes and then to use physical measurement to characterize corresponding acoustic events. {©} S. Hlrzel Verlag EAA.}
      \field{issn}{16101928}
      \field{journaltitle}{Acta Acustica united with Acustica}
      \field{title}{{A cognitive approach to urban soundscapes: Using verbal data to access everyday life auditory categories}}
      \field{year}{2006}
    \endentry
    \entry{Wang2006}{book}{}
      \name{author}{2}{}{%
        {{hash=be523e3be2cd7ae84ae25812787a6138}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Deliang},
           giveni={D\bibinitperiod}}}%
        {{hash=8bbee333c5da8578412d448adc4f0ae0}{%
           family={Brown},
           familyi={B\bibinitperiod},
           given={Guy\bibnamedelima J.},
           giveni={G\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \strng{namehash}{48ea56c2f06c1db5b4a3b66ca5f6789c}
      \strng{fullhash}{48ea56c2f06c1db5b4a3b66ca5f6789c}
      \strng{bibnamehash}{48ea56c2f06c1db5b4a3b66ca5f6789c}
      \strng{authorbibnamehash}{48ea56c2f06c1db5b4a3b66ca5f6789c}
      \strng{authornamehash}{48ea56c2f06c1db5b4a3b66ca5f6789c}
      \strng{authorfullhash}{48ea56c2f06c1db5b4a3b66ca5f6789c}
      \field{sortinit}{8}
      \field{sortinithash}{07edf88d4ea82509b9c4b4d13f41c452}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{How can we engineer systems capable of “cocktail party” listening? Human listeners are able to perceptually segregate one sound source from an acoustic mixture, such as a single voice from a mixture of other voices and music at a busy cocktail party. How can we engineer “machine listening” systems that achieve this perceptual feat? Albert Bregmans book Auditory Scene Analysis, published in 1990, drew an analogy between the perception of auditory scenes and visual scenes, and described a coherent framework for understanding the perceptual organization of sound. His account has stimulated much interest in computational studies of hearing. Such studies are motivated in part by the demand for practical sound separation systems, which have many applications including noiserobust automatic speech recognition, hearing prostheses, and automatic music transcription. This emerging field has become known as computational auditory scene analysis (CASA). Computational Auditory Scene Analysis: Principles, Algorithms, and Applications provides a comprehensive and coherent account of the state of the art in CASA, in terms of the underlying principles, the algorithms and system architectures that are employed, and the potential applications of this exciting new technology. With a Foreword by Bregman, its chapters are written by leading researchers and cover a wide range of topics including: Estimation of multiple fundamental frequenciesFeaturebased and modelbased approaches to CASASound separation based on spatial locationProcessing for reverberant environmentsSegregation of speech and musical signalsAutomatic speech recognition in noisy environmentsNeural and perceptual modeling of auditory organizationThe text is written at a level that will be accessible to graduate students and researchers from related science and engineering disciplines. The extensive bibliography accompanying each chapter will also make this book a valuable reference source. A web site accompanying the text, http://www.casabook.org, features software tools and sound demonstrations.}
      \field{booktitle}{Computational Auditory Scene Analysis: Principles, Algorithms, and Applications}
      \field{isbn}{0470043385}
      \field{title}{{Computational auditory scene analysis: Principles, algorithms, and applications}}
      \field{year}{2006}
      \verb{doi}
      \verb 10.1109/9780470043387
      \endverb
    \endentry
    \entry{Eronen2006}{inproceedings}{}
      \name{author}{8}{}{%
        {{hash=533106d3ea25182a0f6e2f7738efe3c6}{%
           family={Eronen},
           familyi={E\bibinitperiod},
           given={Antti\bibnamedelima J.},
           giveni={A\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=dbdcc22a85b5f5191e2ea1e162554af6}{%
           family={Peltonen},
           familyi={P\bibinitperiod},
           given={Vesa\bibnamedelima T.},
           giveni={V\bibinitperiod\bibinitdelim T\bibinitperiod}}}%
        {{hash=52fe2c7a6c11e71e042bc891f7c08a9b}{%
           family={Tuomi},
           familyi={T\bibinitperiod},
           given={Juha\bibnamedelima T.},
           giveni={J\bibinitperiod\bibinitdelim T\bibinitperiod}}}%
        {{hash=213a97837e6ac9a9511e71defe9911f5}{%
           family={Klapuri},
           familyi={K\bibinitperiod},
           given={Anssi\bibnamedelima P.},
           giveni={A\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
        {{hash=2c8981a89c145717dfbac21982764b12}{%
           family={Fagerlund},
           familyi={F\bibinitperiod},
           given={Seppo},
           giveni={S\bibinitperiod}}}%
        {{hash=d4d157452a59c24c1e5a2f757f2f1309}{%
           family={Sorsa},
           familyi={S\bibinitperiod},
           given={Timo},
           giveni={T\bibinitperiod}}}%
        {{hash=a68b467743f5a612ac83c14a0377bce4}{%
           family={Lorho},
           familyi={L\bibinitperiod},
           given={Ga{ë}tan},
           giveni={G\bibinitperiod}}}%
        {{hash=29d74f37601ecf5f04d69141f517fdfa}{%
           family={Huopaniemi},
           familyi={H\bibinitperiod},
           given={Jyri},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{23d7c54e2aef3b2e605d3b5cfee4a5b9}
      \strng{fullhash}{8077df8e18908c3440c9777fe49cac07}
      \strng{bibnamehash}{1ee8905d80cfdb8f82349ed5532990aa}
      \strng{authorbibnamehash}{1ee8905d80cfdb8f82349ed5532990aa}
      \strng{authornamehash}{23d7c54e2aef3b2e605d3b5cfee4a5b9}
      \strng{authorfullhash}{8077df8e18908c3440c9777fe49cac07}
      \field{sortinit}{9}
      \field{sortinithash}{1dd72ab054147731c9d824b49aba0534}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The aim of this paper is to investigate the feasibility of an audio-based context recognition system. Here, context recognition refers to the automatic classification of the context or an environment around a device. A system is developed and compared to the accuracy of human listeners in the same task. Particular emphasis is placed on the computational complexity of the methods, since the application is of particular interest in resource-constrained portable devices. Simplistic low-dimensional feature vectors are evaluated against more standard spectral features. Using discriminative training, competitive recognition accuracies are achieved with very low-order hidden Markov models (1-3 Gaussian components). Slight improvement in recognition accuracy is observed when linear data-driven feature transformations are applied to mel-cepstral features. The recognition rate of the system as a function of the test sequence length appears to converge only after about 30 to 60 s. Some degree of accuracy can be achieved even with less than 1-s test sequence lengths. The average reaction time of the human listeners was 14 s, i.e., somewhat smaller, but of the same order as that of the system. The average recognition accuracy of the system was 58{\%} against 69{\%}, obtained in the listening tests in recognizing between 24 everyday contexts. The accuracies in recognizing six high-level classes were 82{\%} for the system and 88{\%} for the subjects. {©} 2006 IEEE.}
      \field{booktitle}{IEEE Transactions on Audio, Speech and Language Processing}
      \field{issn}{15587916}
      \field{title}{{Audio-based context recognition}}
      \field{year}{2006}
      \verb{doi}
      \verb 10.1109/TSA.2005.854103
      \endverb
      \keyw{Audio classification,Context awareness,Feature extraction,Hidden markov models (hmms)}
    \endentry
    \entry{Bahoura2009}{article}{}
      \name{author}{1}{}{%
        {{hash=54889d121e06c21637190270580ab7b9}{%
           family={Bahoura},
           familyi={B\bibinitperiod},
           given={Mohammed},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{54889d121e06c21637190270580ab7b9}
      \strng{fullhash}{54889d121e06c21637190270580ab7b9}
      \strng{bibnamehash}{54889d121e06c21637190270580ab7b9}
      \strng{authorbibnamehash}{54889d121e06c21637190270580ab7b9}
      \strng{authornamehash}{54889d121e06c21637190270580ab7b9}
      \strng{authorfullhash}{54889d121e06c21637190270580ab7b9}
      \field{sortinit}{1}
      \field{sortinithash}{2174f786c6195e7fe2ee1c229b416e29}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In this paper, we present the pattern recognition methods proposed to classify respiratory sounds into normal and wheeze classes. We evaluate and compare the feature extraction techniques based on Fourier transform, linear predictive coding, wavelet transform and Mel-frequency cepstral coefficients (MFCC) in combination with the classification methods based on vector quantization, Gaussian mixture models (GMM) and artificial neural networks, using receiver operating characteristic curves. We propose the use of an optimized threshold to discriminate the wheezing class from the normal one. Also, post-processing filter is employed to considerably improve the classification accuracy. Experimental results show that our approach based on MFCC coefficients combined to GMM is well adapted to classify respiratory sounds in normal and wheeze classes. McNemar's test demonstrated significant difference between results obtained by the presented classifiers (p {<} 0.05). {©} 2009 Elsevier Ltd. All rights reserved.}
      \field{issn}{00104825}
      \field{journaltitle}{Computers in Biology and Medicine}
      \field{title}{{Pattern recognition methods applied to respiratory sounds classification into normal and wheeze classes}}
      \field{year}{2009}
      \verb{doi}
      \verb 10.1016/j.compbiomed.2009.06.011
      \endverb
      \keyw{Gaussian mixture models,Linear predictive coding,McNemar's test,Mel-frequency cepstral coefficients,Multi-layer perceptron,Receiver operating characteristic,Respiratory sounds,Statistical significance,Vector quantization,Wavelet transform}
    \endentry
    \entry{Van2013}{article}{}
      \name{author}{3}{}{%
        {{hash=c5e803ec713afc2fc42a3ed9c8386c6d}{%
           family={{Van Nort}},
           familyi={V\bibinitperiod},
           given={Doug},
           giveni={D\bibinitperiod}}}%
        {{hash=0c9b20b555747761afc7db51e3ba6fb3}{%
           family={Oliveros},
           familyi={O\bibinitperiod},
           given={Pauline},
           giveni={P\bibinitperiod}}}%
        {{hash=a248d26902533c772e4ac357b3a0552d}{%
           family={Braasch},
           familyi={B\bibinitperiod},
           given={Jonas},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{3985dcfc90fef93802d45d7e6fbc8297}
      \strng{fullhash}{3985dcfc90fef93802d45d7e6fbc8297}
      \strng{bibnamehash}{3985dcfc90fef93802d45d7e6fbc8297}
      \strng{authorbibnamehash}{3985dcfc90fef93802d45d7e6fbc8297}
      \strng{authornamehash}{3985dcfc90fef93802d45d7e6fbc8297}
      \strng{authorfullhash}{3985dcfc90fef93802d45d7e6fbc8297}
      \field{sortinit}{1}
      \field{sortinithash}{2174f786c6195e7fe2ee1c229b416e29}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Journal of New Music Research}
      \field{number}{4}
      \field{title}{{Electro/acoustic improvisation and deeply listening machines}}
      \field{volume}{42}
      \field{year}{2013}
      \field{pages}{303\bibrangedash 324}
      \range{pages}{22}
    \endentry
    \entry{Temko2009}{incollection}{}
      \name{author}{6}{}{%
        {{hash=ec2746008998a92a73838e0c027feb11}{%
           family={Temko},
           familyi={T\bibinitperiod},
           given={Andrey},
           giveni={A\bibinitperiod}}}%
        {{hash=b50a6f34018e38466e009093c97f6305}{%
           family={Nadeu},
           familyi={N\bibinitperiod},
           given={Climent},
           giveni={C\bibinitperiod}}}%
        {{hash=8fc9be3c2b38bd1ce33fdc15fbd698c6}{%
           family={Macho},
           familyi={M\bibinitperiod},
           given={Du{š}an},
           giveni={D\bibinitperiod}}}%
        {{hash=b240d4c2c38b76890194edc503159f6c}{%
           family={Malkin},
           familyi={M\bibinitperiod},
           given={Robert},
           giveni={R\bibinitperiod}}}%
        {{hash=4b5ecf7a08c4744efa7ab88df81b3fb6}{%
           family={Zieger},
           familyi={Z\bibinitperiod},
           given={Christian},
           giveni={C\bibinitperiod}}}%
        {{hash=4a6f491cc56bccd25bc706b7425a93a3}{%
           family={Omologo},
           familyi={O\bibinitperiod},
           given={Maurizio},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{51f032f7845b62202f6963ef7fe3eec0}
      \strng{fullhash}{0f37c002b63f9b3c7d16889851da659f}
      \strng{bibnamehash}{aa4d25906bf690a96eae1cb147750dc0}
      \strng{authorbibnamehash}{aa4d25906bf690a96eae1cb147750dc0}
      \strng{authornamehash}{51f032f7845b62202f6963ef7fe3eec0}
      \strng{authorfullhash}{0f37c002b63f9b3c7d16889851da659f}
      \field{sortinit}{1}
      \field{sortinithash}{2174f786c6195e7fe2ee1c229b416e29}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The human activity that takes place in meeting-rooms or class-rooms is reflected in a rich variety of acoustic events, either produced by the human body or by objects handled by humans, so the determination of both the identity of sounds and their position in time may help to detect and describe that human activity. Additionally, detection of sounds other than speech may be useful to enhance the robustness of speech technologies like automatic speech recognition. Automatic detection and classification of acoustic events is the objective of this thesis work. It aims at processing the acoustic signals collected by distant microphones in meeting-room or classroom environments to convert them into symbolic descriptions corresponding to a listener's perception of the different sound events that are present in the signals and their sources. First of all, the task of acoustic event classification is faced using Support Vector Machine (SVM) classifiers, which are motivated by the scarcity of training data. A confusion-matrix-based variable-feature-set clustering scheme is developed for the multiclass recognition problem, and tested on the gathered database. With it, a higher classification rate than the GMM-based technique is obtained, arriving to a large relative average error reduction with respect to the best result from the conventional binary tree scheme. Moreover, several ways to extend SVMs to sequence processing are compared, in an attempt to avoid the drawback of SVMs when dealing with audio data, i.e. their restriction to work with fixed-length vectors, observing that the dynamic time warping kernels work well for sounds that show a temporal structure. Furthermore, concepts and tools from the fuzzy theory are used to investigate, first, the importance of and degree of interaction among features, and second, ways to fuse the outputs of several classification systems. The developed AEC systems are tested also by participating in several international evaluations from 2004 to 2006, and the results are reported. The second main contribution of this thesis work is the development of systems for detection of acoustic events. The detection problem is more complex since it includes both classification and determination of the time intervals where the sound takes place. Two system versions are developed and tested on the datasets of the two CLEAR international evaluation campaigns in 2006 and 2007. Two kinds of databases are used: two databases of isolated acoustic events, and a database of interactive seminars containing a significant number of acoustic events of interest. Our developed systems, which consist of SVM-based classification within a sliding window plus post-processing, were the only submissions not using HMMs, and each of them obtained competitive results in the corresponding evaluation. Speech activity detection was also pursued in this thesis since, in fact, it is a – especially important – particular case of acoustic event detection. An enhanced SVM training approach for the speech activity detection task is developed, mainly to cope with the problem of dataset reduction. The resulting SVM-based system is tested with several NIST Rich Transcription (RT) evaluation datasets, and it shows better scores than our GMM-based system, which ranked among the best systems in the RT06 evaluation. Finally, it is worth mentioning a few side outcomes from this thesis work. As it has been carried out in the framework of the CHIL EU project, the author has been responsible for the organization of the above mentioned international evaluations in acoustic event classification and detection, taking a leading role in the specification of acoustic event classes, databases, and evaluation protocols, and, especially, in the proposal and implementation of the various metrics that have been used. Moreover, the detection systems have been implemented in the UPC's smart-room and work in real time for purposes of testing and demonstration.}
      \field{booktitle}{Computers in the Human Interaction Loop}
      \field{chapter}{Part II, 7}
      \field{title}{{Acoustic Event Detection and Classification}}
      \field{year}{2009}
      \verb{doi}
      \verb 10.1007/978-1-84882-054-8_7
      \endverb
    \endentry
    \entry{Temko2007}{inproceedings}{}
      \name{author}{6}{}{%
        {{hash=ec2746008998a92a73838e0c027feb11}{%
           family={Temko},
           familyi={T\bibinitperiod},
           given={Andrey},
           giveni={A\bibinitperiod}}}%
        {{hash=b240d4c2c38b76890194edc503159f6c}{%
           family={Malkin},
           familyi={M\bibinitperiod},
           given={Robert},
           giveni={R\bibinitperiod}}}%
        {{hash=4b5ecf7a08c4744efa7ab88df81b3fb6}{%
           family={Zieger},
           familyi={Z\bibinitperiod},
           given={Christian},
           giveni={C\bibinitperiod}}}%
        {{hash=e81fc8913883dd81eea23295d8744ba8}{%
           family={Macho},
           familyi={M\bibinitperiod},
           given={Dusan},
           giveni={D\bibinitperiod}}}%
        {{hash=b50a6f34018e38466e009093c97f6305}{%
           family={Nadeu},
           familyi={N\bibinitperiod},
           given={Climent},
           giveni={C\bibinitperiod}}}%
        {{hash=4a6f491cc56bccd25bc706b7425a93a3}{%
           family={Omologo},
           familyi={O\bibinitperiod},
           given={Maurizio},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{96fd8bed1a82e6a324d4f3e27758a2eb}
      \strng{fullhash}{274c3e255818716c9806ad7c03bffd96}
      \strng{bibnamehash}{aa4d25906bf690a96eae1cb147750dc0}
      \strng{authorbibnamehash}{aa4d25906bf690a96eae1cb147750dc0}
      \strng{authornamehash}{96fd8bed1a82e6a324d4f3e27758a2eb}
      \strng{authorfullhash}{274c3e255818716c9806ad7c03bffd96}
      \field{sortinit}{1}
      \field{sortinithash}{2174f786c6195e7fe2ee1c229b416e29}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In this paper, we present the results of the Acoustic Event Detection (AED) and Classification (AEC) evaluations carried out in February 2006 by the three participant partners from the CHIL project. The primary evaluation task was AED of the testing portions of the isolated sound databases and seminar recordings produced in CHIL. Additionally, a secondary AEC evaluation task was designed using only the isolated sound databases. The set of meetingroom acoustic event classes and the metrics were agreed by the three partners and ELDA was in charge of the scoring task. In this paper, the various systems for the tasks of AED and AEC and their results are presented. {©} Springer-Verlag Berlin Heidelberg 2007.}
      \field{booktitle}{Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
      \field{isbn}{9783540695677}
      \field{issn}{03029743}
      \field{title}{{CLEAR evaluation of acoustic event detection and classification systems}}
      \field{year}{2007}
      \verb{doi}
      \verb 10.1007/978-3-540-69568-4_29
      \endverb
    \endentry
    \entry{Bahoura2010}{article}{}
      \name{author}{6}{}{%
        {{hash=1ca1f173c17f674eae71aeaba56ff184}{%
           family={Sazonov},
           familyi={S\bibinitperiod},
           given={Edward\bibnamedelima S.},
           giveni={E\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=ba04c5aec65d987b1b82a22828e5dedb}{%
           family={Makeyev},
           familyi={M\bibinitperiod},
           given={Oleksandr},
           giveni={O\bibinitperiod}}}%
        {{hash=ea34f2cd4cbfeee5220c6fadf1fba5d8}{%
           family={Schuckers},
           familyi={S\bibinitperiod},
           given={Stephanie},
           giveni={S\bibinitperiod}}}%
        {{hash=d6f3519aae4349d425a628e7ee448a1b}{%
           family={Lopez-Meyer},
           familyi={L\bibinithyphendelim M\bibinitperiod},
           given={Paulo},
           giveni={P\bibinitperiod}}}%
        {{hash=47f18564e35ba306fe51da46b347d791}{%
           family={Melanson},
           familyi={M\bibinitperiod},
           given={Edward\bibnamedelima L.},
           giveni={E\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
        {{hash=7d310a8afb49b73d46480a965c10924c}{%
           family={Neuman},
           familyi={N\bibinitperiod},
           given={Michael\bibnamedelima R.},
           giveni={M\bibinitperiod\bibinitdelim R\bibinitperiod}}}%
      }
      \strng{namehash}{ed4ec4973fc2de157ec053f9d1820f1b}
      \strng{fullhash}{e2d31c8ccc8fcdfe09686a8b5daaaf60}
      \strng{bibnamehash}{98d6f257d3e6d2eaa440d324a9df078a}
      \strng{authorbibnamehash}{98d6f257d3e6d2eaa440d324a9df078a}
      \strng{authornamehash}{ed4ec4973fc2de157ec053f9d1820f1b}
      \strng{authorfullhash}{e2d31c8ccc8fcdfe09686a8b5daaaf60}
      \field{sortinit}{1}
      \field{sortinithash}{2174f786c6195e7fe2ee1c229b416e29}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Our understanding of etiology of obesity and overweight is incomplete due to lack of objective and accurate methods for monitoring of ingestive behavior (MIB) in the free-living population. Our research has shown that frequency of swallowing may serve as a predictor for detecting food intake, differentiating liquids and solids, and estimating ingested mass. This paper proposes and compares two methods of acoustical swallowing detection from sounds contaminated by motion artifacts, speech, and external noise. Methods based on mel-scale Fourier spectrum, wavelet packets, and support vector machines are studied considering the effects of epoch size, level of decomposition, and lagging on classification accuracy. The methodology was tested on a large dataset (64.5 h with a total of 9966 swallows) collected from 20 human subjects with various degrees of adiposity. Average weighted epoch-recognition accuracy for intravisit individual models was 96.8{\%}, which resulted in 84.7{\%} average weighted accuracy in detection of swallowing events. These results suggest high efficiency of the proposed methodology in separation of swallowing sounds from artifacts that originate from respiration, intrinsic speech, head movements, food ingestion, and ambient noise. The recognition accuracy was not related to body mass index, suggesting that the methodology is suitable for obese individuals. {©} 2006 IEEE.}
      \field{issn}{00189294}
      \field{journaltitle}{IEEE Transactions on Biomedical Engineering}
      \field{title}{{Automatic detection of swallowing events by acoustical means for applications of monitoring of ingestive behavior}}
      \field{year}{2010}
      \verb{doi}
      \verb 10.1109/TBME.2009.2033037
      \endverb
      \keyw{Biomedical signal processing,Obesity,Pattern recognition,Swallowing,Wearable devices}
    \endentry
    \entry{Potamitis2014}{article}{}
      \name{author}{4}{}{%
        {{hash=a1f710b9966a3c4f0f2383a115ac8088}{%
           family={Potamitis},
           familyi={P\bibinitperiod},
           given={Ilyas},
           giveni={I\bibinitperiod}}}%
        {{hash=5fd3f345bc90c512d9b6490685a6bf2e}{%
           family={Ntalampiras},
           familyi={N\bibinitperiod},
           given={Stavros},
           giveni={S\bibinitperiod}}}%
        {{hash=8f7e4ef39a51c53e1f587f4658ef0f46}{%
           family={Jahn},
           familyi={J\bibinitperiod},
           given={Olaf},
           giveni={O\bibinitperiod}}}%
        {{hash=159c800f26e01232b37ee3891cda8cbd}{%
           family={Riede},
           familyi={R\bibinitperiod},
           given={Klaus},
           giveni={K\bibinitperiod}}}%
      }
      \strng{namehash}{e0b00f294c5c8ad9d3370a615ef61bc3}
      \strng{fullhash}{1949bf7e85492cc937f4f37334609685}
      \strng{bibnamehash}{1949bf7e85492cc937f4f37334609685}
      \strng{authorbibnamehash}{1949bf7e85492cc937f4f37334609685}
      \strng{authornamehash}{e0b00f294c5c8ad9d3370a615ef61bc3}
      \strng{authorfullhash}{1949bf7e85492cc937f4f37334609685}
      \field{sortinit}{1}
      \field{sortinithash}{2174f786c6195e7fe2ee1c229b416e29}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The primary purpose for pursuing this research is to present a modular approach that enables reliable automatic bird species identification on the basis of their sound emissions in the field. A practical and complete computer-based framework is proposed to detect and time-stamp particular bird species in continuous real field recordings. Acoustic detection of avian sounds can be used for the automatized monitoring of multiple bird taxa and querying in long-term recordings for species of interest for researchers, conservation practitioners, and decision makers, such as environmental indicator taxa and threatened species. This work describes two novel procedures and offers an open modular framework that detects and time-stamps online calls and songs of target bird species and is fast enough to report results in reasonable time for non-processed field recordings of many thousands files and is generic enough to accommodate any species. The framework is evaluated on two large corpora of real field data, targeting the calls and songs of American Robin Turdus migratorius, a Northamerican oscine passerine (true songbird) and the Common Kingfisher Alcedo atthis, a non-passerine species with a wide distribution throughout Eurasia and North Africa. With the aim of promoting the widespread use of digital autonomous recording units (ARUs) and species recognition technologies the processing code and a large corpus of audio recordings is provided in order to enable other researchers to perform and assess comparative experiments. {©} 2014 Elsevier Ltd. All rights reserved.}
      \field{issn}{0003682X}
      \field{journaltitle}{Applied Acoustics}
      \field{title}{{Automatic bird sound detection in long real-field recordings: Applications and tools}}
      \field{year}{2014}
      \verb{doi}
      \verb 10.1016/j.apacoust.2014.01.001
      \endverb
      \keyw{Bird recognition,Birdsong detection,Computational ecology}
    \endentry
    \entry{Wang2016}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=8e448470980462cb6c82de10f864754c}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Yun},
           giveni={Y\bibinitperiod}}}%
        {{hash=1a813ab78d5a234506dd6dd48fddd130}{%
           family={Neves},
           familyi={N\bibinitperiod},
           given={Leonardo},
           giveni={L\bibinitperiod}}}%
        {{hash=64e42b033385cc6471532215b5b12d27}{%
           family={Metze},
           familyi={M\bibinitperiod},
           given={Florian},
           giveni={F\bibinitperiod}}}%
      }
      \strng{namehash}{bbc67b128a9eccff5d4a89235bd056bb}
      \strng{fullhash}{bbc67b128a9eccff5d4a89235bd056bb}
      \strng{bibnamehash}{bbc67b128a9eccff5d4a89235bd056bb}
      \strng{authorbibnamehash}{bbc67b128a9eccff5d4a89235bd056bb}
      \strng{authornamehash}{bbc67b128a9eccff5d4a89235bd056bb}
      \strng{authorfullhash}{bbc67b128a9eccff5d4a89235bd056bb}
      \field{sortinit}{1}
      \field{sortinithash}{2174f786c6195e7fe2ee1c229b416e29}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Multimedia event detection (MED) is the task of detecting given events (e.g. birthday party, making a sandwich) in a large collection of video clips. While visual features and automatic speech recognition typically provide the best features for this task, nonspeech audio can also contribute useful information, such as crowds cheering, engine noises, or animal sounds. MED is typically formulated as a two-stage process: the first stage generates clip-level feature representations, often by aggregating frame-level features; the second stage performs binary or multi-class classification to decide whether a given event occurs in a video clip. Both stages are usually performed «statically», i.e. using only local temporal information, or bag-of-words models. In this paper, we introduce longer-range temporal information with deep recurrent neural networks (RNNs) for both stages. We classify each audio frame among a set of semantic units called «noisemes» the sequence of frame-level confidence distributions is used as a variable-length clip-level representation. Such confidence vector sequences are then fed into long short-term memory (LSTM) networks for clip-level classification. We observe improvements in both frame-level and clip-level performance compared to SVM and feed-forward neural network baselines.}
      \field{booktitle}{ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings}
      \field{isbn}{9781479999880}
      \field{issn}{15206149}
      \field{title}{{Audio-based multimedia event detection using deep recurrent neural networks}}
      \field{year}{2016}
      \verb{doi}
      \verb 10.1109/ICASSP.2016.7472176
      \endverb
      \keyw{Multimedia event detection (MED),long short-term memory (LSTM),noisemes,recurrent neural networks (RNNs)}
    \endentry
    \entry{Giannakopoulos2014}{book}{}
      \name{author}{2}{}{%
        {{hash=e032ec645401a1838ee933a73d41794f}{%
           family={Giannakopoulos},
           familyi={G\bibinitperiod},
           given={Theodoros},
           giveni={T\bibinitperiod}}}%
        {{hash=dea050aeb2788734bc9987d69823bff5}{%
           family={Pikrakis},
           familyi={P\bibinitperiod},
           given={Aggelos},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{dc772b43a6a4bc2243986fa01ac7de5d}
      \strng{fullhash}{dc772b43a6a4bc2243986fa01ac7de5d}
      \strng{bibnamehash}{dc772b43a6a4bc2243986fa01ac7de5d}
      \strng{authorbibnamehash}{dc772b43a6a4bc2243986fa01ac7de5d}
      \strng{authornamehash}{dc772b43a6a4bc2243986fa01ac7de5d}
      \strng{authorfullhash}{dc772b43a6a4bc2243986fa01ac7de5d}
      \field{sortinit}{1}
      \field{sortinithash}{2174f786c6195e7fe2ee1c229b416e29}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Introduction to Audio Analysis serves as a standalone introduction to audio analysis, providing theoretical background to many state-of-the-art techniques. It covers the essential theory necessary to develop audio engineering applications, but also uses programming techniques, notably MATLAB{®}, to take a more applied approach to the topic. Basic theory and reproducible experiments are combined to demonstrate theoretical concepts from a practical point of view and provide a solid foundation in the field of audio analysis. Audio feature extraction, audio classification, audio segmentation, and music information retrieval are all addressed in detail, along with material on basic audio processing and frequency domain representations and filtering. Throughout the text, reproducible MATLAB{®} examples are accompanied by theoretical descriptions, illustrating how concepts and equations can be applied to the development of audio analysis systems and components. A blend of reproducible MATLAB{®} code and essential theory provides enable the reader to delve into the world of audio signals and develop real-world audio applications in various domains. {©} 2014 Elsevier Ltd. All rights reserved.}
      \field{booktitle}{Introduction to Audio Analysis: A MATLAB Approach}
      \field{isbn}{9780080993881}
      \field{title}{{Introduction to Audio Analysis: A MATLAB Approach}}
      \field{year}{2014}
      \verb{doi}
      \verb 10.1016/C2012-0-03524-7
      \endverb
    \endentry
    \entry{Amatriain2004}{article}{}
      \name{author}{1}{}{%
        {{hash=73d7725e7b8c19a7a179442f898f8a48}{%
           family={Amatriain},
           familyi={A\bibinitperiod},
           given={Xavier},
           giveni={X\bibinitperiod}}}%
      }
      \strng{namehash}{73d7725e7b8c19a7a179442f898f8a48}
      \strng{fullhash}{73d7725e7b8c19a7a179442f898f8a48}
      \strng{bibnamehash}{73d7725e7b8c19a7a179442f898f8a48}
      \strng{authorbibnamehash}{73d7725e7b8c19a7a179442f898f8a48}
      \strng{authornamehash}{73d7725e7b8c19a7a179442f898f8a48}
      \strng{authorfullhash}{73d7725e7b8c19a7a179442f898f8a48}
      \field{sortinit}{1}
      \field{sortinithash}{2174f786c6195e7fe2ee1c229b416e29}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{{An Object-Oriented Metamodel for Digital Signal Processing with a focus on Audio and Music}}
      \field{year}{2004}
    \endentry
    \entry{Marr1982}{article}{}
      \name{author}{1}{}{%
        {{hash=49d936583765e8a7bd38370987be2642}{%
           family={Marr},
           familyi={M\bibinitperiod},
           given={D.},
           giveni={D\bibinitperiod}}}%
      }
      \strng{namehash}{49d936583765e8a7bd38370987be2642}
      \strng{fullhash}{49d936583765e8a7bd38370987be2642}
      \strng{bibnamehash}{49d936583765e8a7bd38370987be2642}
      \strng{authorbibnamehash}{49d936583765e8a7bd38370987be2642}
      \strng{authornamehash}{49d936583765e8a7bd38370987be2642}
      \strng{authorfullhash}{49d936583765e8a7bd38370987be2642}
      \field{sortinit}{1}
      \field{sortinithash}{2174f786c6195e7fe2ee1c229b416e29}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A text which approaches vision through an information processing framework, with particular attention to the computer processing of visual information. Machine information processing necessitates both a study of computer facilities and of human information processing. An introductory section outlines the philosophical approach using representational theories of the mind. Vision is then studied using theories of representation for images and surfaces, shapes and pattern recognition. Algorithmic approaches and theoretical models are discussed. A final chapter details a 'conversation' about the material undertaken by the author and colleagues. -M.Blakemore}
      \field{isbn}{0716712849}
      \field{issn}{00222496}
      \field{journaltitle}{Vision: a computational investigation into the human representation and processing of visual information.}
      \field{title}{{Vision: a computational investigation into the human representation and processing of visual information.}}
      \field{year}{1982}
      \verb{doi}
      \verb 10.1016/0022-2496(83)90030-5
      \endverb
    \endentry
    \entry{Stowell2015}{article}{}
      \name{author}{5}{}{%
        {{hash=720621ffa11137749699f824497e8e0b}{%
           family={Stowell},
           familyi={S\bibinitperiod},
           given={Dan},
           giveni={D\bibinitperiod}}}%
        {{hash=2d09055e9f0708746d52cacc153c7711}{%
           family={Giannoulis},
           familyi={G\bibinitperiod},
           given={Dimitrios},
           giveni={D\bibinitperiod}}}%
        {{hash=b068724f00d17295882e712edcc6315c}{%
           family={Benetos},
           familyi={B\bibinitperiod},
           given={Emmanouil},
           giveni={E\bibinitperiod}}}%
        {{hash=6c82631b239f38e19ec21d972445583c}{%
           family={Lagrange},
           familyi={L\bibinitperiod},
           given={Mathieu},
           giveni={M\bibinitperiod}}}%
        {{hash=af302f6b1171db4b3b2979f068a9bafe}{%
           family={Plumbley},
           familyi={P\bibinitperiod},
           given={Mark\bibnamedelima D.},
           giveni={M\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
      }
      \strng{namehash}{d32ba681d4d29026b25af8194187e69d}
      \strng{fullhash}{f8451221f4c11de97a3505fc08d1ff32}
      \strng{bibnamehash}{f8451221f4c11de97a3505fc08d1ff32}
      \strng{authorbibnamehash}{f8451221f4c11de97a3505fc08d1ff32}
      \strng{authornamehash}{d32ba681d4d29026b25af8194187e69d}
      \strng{authorfullhash}{f8451221f4c11de97a3505fc08d1ff32}
      \field{sortinit}{2}
      \field{sortinithash}{cbff857e587bcb4635511624d773949e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{For intelligent systems to make best use of the audio modality, it is important that they can recognize not just speech and music, which have been researched as specific tasks, but also general sounds in everyday environments. To stimulate research in this field we conducted a public research challenge: the IEEE Audio and Acoustic Signal Processing Technical Committee challenge on Detection and Classification of Acoustic Scenes and Events (DCASE). In this paper, we report on the state of the art in automatically classifying audio scenes, and automatically detecting and classifying audio events. We survey prior work as well as the state of the art represented by the submissions to the challenge from various research groups. We also provide detail on the organization of the challenge, so that our experience as challenge hosts may be useful to those organizing challenges in similar domains. We created new audio datasets and baseline systems for the challenge; these, as well as some submitted systems, are publicly available under open licenses, to serve as benchmarks for further research in general-purpose machine listening.}
      \field{issn}{15209210}
      \field{journaltitle}{IEEE Transactions on Multimedia}
      \field{title}{{Detection and Classification of Acoustic Scenes and Events}}
      \field{year}{2015}
      \verb{doi}
      \verb 10.1109/TMM.2015.2428998
      \endverb
      \keyw{Audio databases,event detection,machine intelligence,pattern recognition}
    \endentry
    \entry{Geiger2013}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=1cc1331ef4d1ac805c126fb99ac5dcfa}{%
           family={Geiger},
           familyi={G\bibinitperiod},
           given={Jurgen\bibnamedelima T.},
           giveni={J\bibinitperiod\bibinitdelim T\bibinitperiod}}}%
        {{hash=eaef5410663e7c9b49d766559d67be5a}{%
           family={Schuller},
           familyi={S\bibinitperiod},
           given={Bjorn},
           giveni={B\bibinitperiod}}}%
        {{hash=a412590ea8e1a24b3b627f33dce92dc1}{%
           family={Rigoll},
           familyi={R\bibinitperiod},
           given={Gerhard},
           giveni={G\bibinitperiod}}}%
      }
      \strng{namehash}{a6b4e69d116bdb7c5e53307255edaa3e}
      \strng{fullhash}{a6b4e69d116bdb7c5e53307255edaa3e}
      \strng{bibnamehash}{a6b4e69d116bdb7c5e53307255edaa3e}
      \strng{authorbibnamehash}{a6b4e69d116bdb7c5e53307255edaa3e}
      \strng{authornamehash}{a6b4e69d116bdb7c5e53307255edaa3e}
      \strng{authorfullhash}{a6b4e69d116bdb7c5e53307255edaa3e}
      \field{sortinit}{2}
      \field{sortinithash}{cbff857e587bcb4635511624d773949e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This work describes a system for acoustic scene classification using large-scale audio feature extraction. It is our contribution to the Scene Classification track of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (D-CASE). The system classifies 30 second long recordings of 10 different acoustic scenes. From the highly variable recordings, a large number of spectral, cepstral, energy and voicing-related audio features are extracted. Using a sliding window approach, classification is performed on short windows. SVM are used to classify these short segments, and a majority voting scheme is employed to get a decision for longer recordings. On the official development set of the challenge, an accuracy of 73 {\%} is achieved. SVM are compared with a nearest neighbour classifier and an approach called Latent Perceptual Indexing, whereby SVM achieve the best results. A feature analysis using the t-statistic shows that mainly Mel spectra are the most relevant features. {©} 2013 IEEE.}
      \field{booktitle}{IEEE Workshop on Applications of Signal Processing to Audio and Acoustics}
      \field{isbn}{9781479909728}
      \field{title}{{Large-scale audio feature extraction and SVM for acoustic scene classification}}
      \field{year}{2013}
      \verb{doi}
      \verb 10.1109/WASPAA.2013.6701857
      \endverb
      \keyw{Computational auditory scene analysis,acoustic scene recognition,feature extraction}
    \endentry
    \entry{Aucouturier2007}{article}{}
      \name{author}{3}{}{%
        {{hash=f1496f4bb4ffe9c2896e7d6438f1cc52}{%
           family={Aucouturier},
           familyi={A\bibinitperiod},
           given={Jean-Julien},
           giveni={J\bibinithyphendelim J\bibinitperiod}}}%
        {{hash=72a7f006eb86f4fd3606a6f2dbc8a994}{%
           family={Defreville},
           familyi={D\bibinitperiod},
           given={Boris},
           giveni={B\bibinitperiod}}}%
        {{hash=806be538478ce47a6a363ecce2630d34}{%
           family={Pachet},
           familyi={P\bibinitperiod},
           given={Fran{ç}ois},
           giveni={F\bibinitperiod}}}%
      }
      \strng{namehash}{ec134b52e053e7e2e22010f76134fb4a}
      \strng{fullhash}{ec134b52e053e7e2e22010f76134fb4a}
      \strng{bibnamehash}{ec134b52e053e7e2e22010f76134fb4a}
      \strng{authorbibnamehash}{ec134b52e053e7e2e22010f76134fb4a}
      \strng{authornamehash}{ec134b52e053e7e2e22010f76134fb4a}
      \strng{authorfullhash}{ec134b52e053e7e2e22010f76134fb4a}
      \field{sortinit}{2}
      \field{sortinithash}{cbff857e587bcb4635511624d773949e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The "bag-of-frames" approach (BOF) to audio pattern recognition represents signals as the long-term statistical distribution of their local spectral features. This approach has proved nearly optimal for simulating the auditory perception of natural and human environments (or soundscapes), and is also the most predominent paradigm to extract high-level descriptions from music signals. However, recent studies show that, contrary to its application to soundscape signals, BOF only provides limited performance when applied to polyphonic music signals. This paper proposes to explicitly examine the difference between urban soundscapes and polyphonic music with respect to their modeling with the BOF approach. First, the application of the same measure of acoustic similarity on both soundscape and music data sets confirms that the BOF approach can model soundscapes to near-perfect precision, and exhibits none of the limitations observed in the music data set. Second, the modification of this measure by two custom homogeneity transforms reveals critical differences in the temporal and statistical structure of the typical frame distribution of each type of signal. Such differences may explain the uneven performance of BOF algorithms on soundscapes and music signals, and suggest that their human perception rely on cognitive processes of a different nature.}
      \field{issn}{0001-4966}
      \field{journaltitle}{The Journal of the Acoustical Society of America}
      \field{title}{{The bag-of-frames approach to audio pattern recognition: A sufficient model for urban soundscapes but not for polyphonic music}}
      \field{year}{2007}
      \verb{doi}
      \verb 10.1121/1.2750160
      \endverb
    \endentry
    \entry{Henaff2011}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=401562d96db136e78effff95552f584b}{%
           family={Henaff},
           familyi={H\bibinitperiod},
           given={Mikael},
           giveni={M\bibinitperiod}}}%
        {{hash=acd66fce01ebb04c4b384bd3295b7bea}{%
           family={Jarrett},
           familyi={J\bibinitperiod},
           given={Kevin},
           giveni={K\bibinitperiod}}}%
        {{hash=481308b301166b521c74fde6566e97e6}{%
           family={Kavukcuoglu},
           familyi={K\bibinitperiod},
           given={Koray},
           giveni={K\bibinitperiod}}}%
        {{hash=d0ab8cbca75df7aa3e0b6024d60ac5f8}{%
           family={Lecun},
           familyi={L\bibinitperiod},
           given={Yann},
           giveni={Y\bibinitperiod}}}%
      }
      \strng{namehash}{0220a155f993894332b4d93a208e4c93}
      \strng{fullhash}{aa617037b675581fa02105dff81ab452}
      \strng{bibnamehash}{aa617037b675581fa02105dff81ab452}
      \strng{authorbibnamehash}{aa617037b675581fa02105dff81ab452}
      \strng{authornamehash}{0220a155f993894332b4d93a208e4c93}
      \strng{authorfullhash}{aa617037b675581fa02105dff81ab452}
      \field{sortinit}{2}
      \field{sortinithash}{cbff857e587bcb4635511624d773949e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In this work we present a system to automatically learn features from audio in an unsupervised manner. Our method first learns an overcomplete dictionary which can be used to sparsely decompose log-scaled spectrograms. It then trains an efficient encoder which quickly maps new inputs to approximations of their sparse representations using the learned dictionary. This avoids expensive iterative procedures usually required to infer sparse codes. We then use these sparse codes as inputs for a linear Support Vector Machine (SVM). Our system achieves 83.4{\%} accuracy in predicting genres on the GTZAN dataset, which is competitive with current state-of-the-art approaches. Furthermore, the use of a simple linear classifier combined with a fast feature extraction system allows our approach to scale well to large datasets. {©} 2011 International Society for Music Information Retrieval.}
      \field{booktitle}{Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011}
      \field{isbn}{9780615548654}
      \field{title}{{Unsupervised learning of sparse features for scalable audio classification}}
      \field{year}{2011}
    \endentry
    \entry{Nam2012}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=b7c57c9ad12471fb5ec7eb082cd69eeb}{%
           family={Nam},
           familyi={N\bibinitperiod},
           given={Juhan},
           giveni={J\bibinitperiod}}}%
        {{hash=dca13edca3f7969223cd952e1bf8eb35}{%
           family={Herrera},
           familyi={H\bibinitperiod},
           given={Jorge},
           giveni={J\bibinitperiod}}}%
        {{hash=af3eb849c7bb824718da580e0514e62b}{%
           family={Slaney},
           familyi={S\bibinitperiod},
           given={Malcolm},
           giveni={M\bibinitperiod}}}%
        {{hash=ef8d0c138caffb1d3301c65ccfb27407}{%
           family={Smith},
           familyi={S\bibinitperiod},
           given={Julius},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{beb89e147c185dfd4b37e4abeeb1493e}
      \strng{fullhash}{572897c4888707da4b6711ef95a367fe}
      \strng{bibnamehash}{572897c4888707da4b6711ef95a367fe}
      \strng{authorbibnamehash}{572897c4888707da4b6711ef95a367fe}
      \strng{authornamehash}{beb89e147c185dfd4b37e4abeeb1493e}
      \strng{authorfullhash}{572897c4888707da4b6711ef95a367fe}
      \field{sortinit}{2}
      \field{sortinithash}{cbff857e587bcb4635511624d773949e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We present a data-processing pipeline based on sparse feature learning and describe its applications to music annotation and retrieval. Content-based music annotation and retrieval systems process audio starting with features. While commonly used features, such as MFCC, are handcrafted to extract characteristics of the audio in a succinct way, there is increasing interest in learning features automatically from data using unsupervised algorithms. We describe a systemic approach applying feature-learning algorithms to music data, in particular, focusing on a high-dimensional sparse-feature representation. Our experiments show that, using only a linear classifier, the newly learned features produce results on the CAL500 dataset comparable to state-of-the-art music annotation and retrieval systems. {©} 2012 International Society for Music Information Retrieval.}
      \field{booktitle}{Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012}
      \field{isbn}{9789727521449}
      \field{title}{{Learning sparse feature representations for music annotation and retrieval}}
      \field{year}{2012}
    \endentry
    \entry{Lee2013}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=0f47d6f8a9660c5c5bcdb2f43935a120}{%
           family={Lee},
           familyi={L\bibinitperiod},
           given={Kyogu},
           giveni={K\bibinitperiod}}}%
        {{hash=3f1de141b1e9c9235408ae778e97b94d}{%
           family={Hyung},
           familyi={H\bibinitperiod},
           given={Ziwon},
           giveni={Z\bibinitperiod}}}%
        {{hash=b7c57c9ad12471fb5ec7eb082cd69eeb}{%
           family={Nam},
           familyi={N\bibinitperiod},
           given={Juhan},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{30244510521533ff899be917eeb74ed8}
      \strng{fullhash}{30244510521533ff899be917eeb74ed8}
      \strng{bibnamehash}{30244510521533ff899be917eeb74ed8}
      \strng{authorbibnamehash}{30244510521533ff899be917eeb74ed8}
      \strng{authornamehash}{30244510521533ff899be917eeb74ed8}
      \strng{authorfullhash}{30244510521533ff899be917eeb74ed8}
      \field{sortinit}{2}
      \field{sortinithash}{cbff857e587bcb4635511624d773949e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recently unsupervised learning algorithms have been successfully used to represent data in many of machine recognition tasks. In particular, sparse feature learning algorithms have shown that they can not only discover meaningful structures from raw data but also outperform many hand-engineered features. In this paper, we apply the sparse feature learning approach to acoustic scene classification. We use a sparse restricted Boltzmann machine to capture manyfold local acoustic structures from audio data and represent the data in a high-dimensional sparse feature space given the learned structures. For scene classification, we summarize the local features by pooling over audio scene data. While the feature pooling is typically performed over uniformly divided segments, we suggest a new pooling method, which first detects audio events and then performs pooling only over detected events, considering the irregular occurrence of audio events in acoustic scene data. We evaluate the learned features on the IEEE AASP Challenge development set, comparing them with a baseline model using mel-frequency cepstral coefficients (MFCCs). The results show that learned features outperform MFCCs, event-based pooling achieves higher accuracy than uniform pooling and, furthermore, a combination of the two methods performs even better than either one used alone. {©} 2013 IEEE.}
      \field{booktitle}{IEEE Workshop on Applications of Signal Processing to Audio and Acoustics}
      \field{isbn}{9781479909728}
      \field{title}{{Acoustic scene classification using sparse feature learning and event-based pooling}}
      \field{year}{2013}
      \verb{doi}
      \verb 10.1109/WASPAA.2013.6701893
      \endverb
      \keyw{acoustic scene classification,environmental sound,event detection,feature learning,max-pooling,restricted Boltzmann machine,sparse feature representation}
    \endentry
    \entry{Mesaros2010}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=2a152e3c23c87cd9b35d36ff7cc03974}{%
           family={Mesaros},
           familyi={M\bibinitperiod},
           given={Annamaria},
           giveni={A\bibinitperiod}}}%
        {{hash=6c32fc555cb6cb65401de698f7673917}{%
           family={Heittola},
           familyi={H\bibinitperiod},
           given={Toni},
           giveni={T\bibinitperiod}}}%
        {{hash=c1436e1460d47553be172c0ccbf7703e}{%
           family={Eronen},
           familyi={E\bibinitperiod},
           given={Antti},
           giveni={A\bibinitperiod}}}%
        {{hash=57782ed29cdf64f4eca2ebe4cebed0f2}{%
           family={Virtanen},
           familyi={V\bibinitperiod},
           given={Tuomas},
           giveni={T\bibinitperiod}}}%
      }
      \strng{namehash}{6ec81f98008f0b995e46fd50d3209b4d}
      \strng{fullhash}{7f7aa4fda807631643057e918c669622}
      \strng{bibnamehash}{7f7aa4fda807631643057e918c669622}
      \strng{authorbibnamehash}{7f7aa4fda807631643057e918c669622}
      \strng{authornamehash}{6ec81f98008f0b995e46fd50d3209b4d}
      \strng{authorfullhash}{7f7aa4fda807631643057e918c669622}
      \field{sortinit}{2}
      \field{sortinithash}{cbff857e587bcb4635511624d773949e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper presents a system for acoustic event detection in recordings from real life environments. The events are modeled using a network of hidden Markov models; their size and topology is chosen based on a study of isolated events recognition. We also studied the effect of ambient background noise on event classification performance. On real life recordings, we tested recognition of isolated sound events and event detection. For event detection, the system performs recognition and temporal positioning of a sequence of events. An accuracy of 24{\%} was obtained in classifying isolated sound events into 61 classes. This corresponds to the accuracy of classifying between 61 events when mixed with ambient background noise at 0dB signal-to-noise ratio. In event detection, the system is capable of recognizing almost one third of the events, and the temporal positioning of the events is not correct for 84{\%} of the time. {©} EURASIP, 2010.}
      \field{booktitle}{European Signal Processing Conference}
      \field{issn}{22195491}
      \field{title}{{Acoustic event detection in real life recordings}}
      \field{year}{2010}
    \endentry
    \entry{Cramer2019}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=e3e363c2f61a35365aff8307c03861b0}{%
           family={Cramer},
           familyi={C\bibinitperiod},
           given={Jason},
           giveni={J\bibinitperiod}}}%
        {{hash=f6a8390227f296a24935c36a0288ab74}{%
           family={Wu},
           familyi={W\bibinitperiod},
           given={Ho\bibnamedelima Hsiang},
           giveni={H\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
        {{hash=6f29e3fb8b241a304c6a9778479b58d2}{%
           family={Salamon},
           familyi={S\bibinitperiod},
           given={Justin},
           giveni={J\bibinitperiod}}}%
        {{hash=dc1dca849ac4954f3a5b35d55cd40631}{%
           family={Bello},
           familyi={B\bibinitperiod},
           given={Juan\bibnamedelima Pablo},
           giveni={J\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
      }
      \strng{namehash}{8e6daa751f60a7530f9c7824c7e7c37c}
      \strng{fullhash}{cd929bb8080f45eb200f5f8a43873185}
      \strng{bibnamehash}{cd929bb8080f45eb200f5f8a43873185}
      \strng{authorbibnamehash}{cd929bb8080f45eb200f5f8a43873185}
      \strng{authornamehash}{8e6daa751f60a7530f9c7824c7e7c37c}
      \strng{authorfullhash}{cd929bb8080f45eb200f5f8a43873185}
      \field{sortinit}{2}
      \field{sortinithash}{cbff857e587bcb4635511624d773949e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A considerable challenge in applying deep learning to audio classification is the scarcity of labeled data. An increasingly popular solution is to learn deep audio embeddings from large audio collections and use them to train shallow classifiers using small labeled datasets. Look, Listen, and Learn (L3-Net) is an embedding trained through self-supervised learning of audio-visual correspondence in videos as opposed to other embeddings requiring labeled data. This framework has the potential to produce powerful out-of-the-box embeddings for downstream audio classification tasks, but has a number of unexplained design choices that may impact the embeddings' behavior. In this paper we investigate how L3-Net design choices impact the performance of downstream audio classifiers trained with these embeddings. We show that audio-informed choices of input representation are important, and that using sufficient data for training the embedding is key. Surprisingly, we find that matching the content for training the embedding to the downstream task is not beneficial. Finally, we show that our best variant of the L3-Net embedding outperforms both the VGGish and SoundNet embeddings, while having fewer parameters and being trained on less data. Our implementation of the L3-Net embedding model as well as pre-trained models are made freely available online.}
      \field{booktitle}{ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings}
      \field{isbn}{9781479981311}
      \field{issn}{15206149}
      \field{title}{{Look, Listen, and Learn More: Design Choices for Deep Audio Embeddings}}
      \field{year}{2019}
      \verb{doi}
      \verb 10.1109/ICASSP.2019.8682475
      \endverb
      \keyw{Audio classification,deep audio embeddings,deep learning,machine listening,transfer learning}
    \endentry
    \entry{Aytar2016}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=849e55f41f7cf2c528a25a5d904adb90}{%
           family={Aytar},
           familyi={A\bibinitperiod},
           given={Yusuf},
           giveni={Y\bibinitperiod}}}%
        {{hash=ce5550f8b9b930b07de1681387feb3fa}{%
           family={Vondrick},
           familyi={V\bibinitperiod},
           given={Carl},
           giveni={C\bibinitperiod}}}%
        {{hash=2a80dd1cae02059e95bfed2f2715fd0a}{%
           family={Torralba},
           familyi={T\bibinitperiod},
           given={Antonio},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{c5e4f71bd4434b12d6c6968ba9aafd16}
      \strng{fullhash}{c5e4f71bd4434b12d6c6968ba9aafd16}
      \strng{bibnamehash}{c5e4f71bd4434b12d6c6968ba9aafd16}
      \strng{authorbibnamehash}{c5e4f71bd4434b12d6c6968ba9aafd16}
      \strng{authornamehash}{c5e4f71bd4434b12d6c6968ba9aafd16}
      \strng{authorfullhash}{c5e4f71bd4434b12d6c6968ba9aafd16}
      \field{sortinit}{3}
      \field{sortinithash}{a4b52e5432884761f50fb9571273b93e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We learn rich natural sound representations by capitalizing on large amounts of unlabeled sound data collected in the wild. We leverage the natural synchronization between vision and sound to learn an acoustic representation using two-million unlabeled videos. Unlabeled video has the advantage that it can be economically acquired at massive scales, yet contains useful signals about natural sound. We propose a student-teacher training procedure which transfers discriminative visual knowledge from well established visual recognition models into the sound modality using unlabeled video as a bridge. Our sound representation yields significant performance improvements over the state-of-the-art results on standard benchmarks for acoustic scene/object classification. Visualizations suggest some high-level semantics automatically emerge in the sound network, even though it is trained without ground truth labels.}
      \field{booktitle}{Advances in Neural Information Processing Systems}
      \field{eprinttype}{arXiv}
      \field{issn}{10495258}
      \field{title}{{SoundNet: Learning sound representations from unlabeled video}}
      \field{year}{2016}
      \verb{eprint}
      \verb 1610.09001
      \endverb
    \endentry
    \entry{Pan2010}{misc}{}
      \name{author}{2}{}{%
        {{hash=224c02b5d289fefd08d4f9f1a83c5d0c}{%
           family={Pan},
           familyi={P\bibinitperiod},
           given={Sinno\bibnamedelima Jialin},
           giveni={S\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=55242d2a60270145342841e4d4238da0}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Qiang},
           giveni={Q\bibinitperiod}}}%
      }
      \strng{namehash}{770fc1d95710074c9506ab4f7d386573}
      \strng{fullhash}{770fc1d95710074c9506ab4f7d386573}
      \strng{bibnamehash}{770fc1d95710074c9506ab4f7d386573}
      \strng{authorbibnamehash}{770fc1d95710074c9506ab4f7d386573}
      \strng{authornamehash}{770fc1d95710074c9506ab4f7d386573}
      \strng{authorfullhash}{770fc1d95710074c9506ab4f7d386573}
      \field{sortinit}{3}
      \field{sortinithash}{a4b52e5432884761f50fb9571273b93e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research. {©} 2006 IEEE.}
      \field{booktitle}{IEEE Transactions on Knowledge and Data Engineering}
      \field{issn}{10414347}
      \field{title}{{A survey on transfer learning}}
      \field{year}{2010}
      \verb{doi}
      \verb 10.1109/TKDE.2009.191
      \endverb
      \keyw{Transfer learning,data mining.,machine learning,survey}
    \endentry
    \entry{Sarkar2018}{article}{}
      \name{author}{1}{}{%
        {{hash=19620e2d7ebdec18c9c7f5c60d155d33}{%
           family={Sarkar},
           familyi={S\bibinitperiod},
           given={Dipanjan},
           giveni={D\bibinitperiod}}}%
      }
      \strng{namehash}{19620e2d7ebdec18c9c7f5c60d155d33}
      \strng{fullhash}{19620e2d7ebdec18c9c7f5c60d155d33}
      \strng{bibnamehash}{19620e2d7ebdec18c9c7f5c60d155d33}
      \strng{authorbibnamehash}{19620e2d7ebdec18c9c7f5c60d155d33}
      \strng{authornamehash}{19620e2d7ebdec18c9c7f5c60d155d33}
      \strng{authorfullhash}{19620e2d7ebdec18c9c7f5c60d155d33}
      \field{sortinit}{3}
      \field{sortinithash}{a4b52e5432884761f50fb9571273b93e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{{A Comprehensive Hands-on Guide to Transfer Learning with Real-World Applications in Deep Learning}}
      \field{year}{2018}
    \endentry
    \entry{Ruder2017}{article}{}
      \name{author}{1}{}{%
        {{hash=b468248a20d75c52ee742f4592c2569f}{%
           family={Ruder},
           familyi={R\bibinitperiod},
           given={Sebastian},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{b468248a20d75c52ee742f4592c2569f}
      \strng{fullhash}{b468248a20d75c52ee742f4592c2569f}
      \strng{bibnamehash}{b468248a20d75c52ee742f4592c2569f}
      \strng{authorbibnamehash}{b468248a20d75c52ee742f4592c2569f}
      \strng{authornamehash}{b468248a20d75c52ee742f4592c2569f}
      \strng{authorfullhash}{b468248a20d75c52ee742f4592c2569f}
      \field{sortinit}{3}
      \field{sortinithash}{a4b52e5432884761f50fb9571273b93e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{{Transfer Learning - Machine Learning's Next Frontier}}
      \field{year}{2017}
    \endentry
    \entry{Krug2002}{article}{}
      \name{author}{4}{}{%
        {{hash=dcc1f4f6bbf9c27ac8b8964338cd4aba}{%
           family={Krug},
           familyi={K\bibinitperiod},
           given={Etienne\bibnamedelima G.},
           giveni={E\bibinitperiod\bibinitdelim G\bibinitperiod}}}%
        {{hash=59a01715c1f86ae60ed0000a7059a527}{%
           family={Mercy},
           familyi={M\bibinitperiod},
           given={James\bibnamedelima A.},
           giveni={J\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=7d63410bf6f85a688062502f576085e0}{%
           family={Dahlberg},
           familyi={D\bibinitperiod},
           given={Linda\bibnamedelima L.},
           giveni={L\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
        {{hash=b824b807e5b59fe9ad6f318337275878}{%
           family={Zwi},
           familyi={Z\bibinitperiod},
           given={Anthony\bibnamedelima B.},
           giveni={A\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
      }
      \strng{namehash}{4cadcc9f7d5ea7c2aa817d964abbdc7d}
      \strng{fullhash}{2a47dfa74cc406a5e1559dd7405d52aa}
      \strng{bibnamehash}{2a47dfa74cc406a5e1559dd7405d52aa}
      \strng{authorbibnamehash}{2a47dfa74cc406a5e1559dd7405d52aa}
      \strng{authornamehash}{4cadcc9f7d5ea7c2aa817d964abbdc7d}
      \strng{authorfullhash}{2a47dfa74cc406a5e1559dd7405d52aa}
      \field{sortinit}{3}
      \field{sortinithash}{a4b52e5432884761f50fb9571273b93e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In 1996, the World Health Assembly declared violence a major public health issue. To follow up on this resolution, on Oct 3 this year, WHO released the first World Report on Violence and Health. The report analyses different types of violence including chil abuse and neglect, youth violence, intimate partner violence, sexual violence, elder abuse, self-directed violence, and collective violence. For all these types of violence, the report explores the magnitude of the health and social effects, the risk and protective factors, and the types of prevention efforts that have been initiated. The launch of the report will be followed by a 1-year Global Campaign on Violence Prevention, focusing on implementation of the recommendations. This article summarises some of the main points of the world report.}
      \field{issn}{01406736}
      \field{journaltitle}{Lancet}
      \field{title}{{The world report on violence and health}}
      \field{year}{2002}
      \verb{doi}
      \verb 10.1016/S0140-6736(02)11133-0
      \endverb
    \endentry
    \entry{Demarty2013}{inproceedings}{}
      \name{author}{6}{}{%
        {{hash=1cfcfd6b95053490ce13f39a2f9fc831}{%
           family={Demarty},
           familyi={D\bibinitperiod},
           given={Claire\bibnamedelima H{é}l{è}ne},
           giveni={C\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
        {{hash=369e4a23a04ce26754dacdeb5204a807}{%
           family={Penet},
           familyi={P\bibinitperiod},
           given={C{é}dric},
           giveni={C\bibinitperiod}}}%
        {{hash=28b51640509c48a3b9b4bbbb0b26c578}{%
           family={Schedl},
           familyi={S\bibinitperiod},
           given={Markus},
           giveni={M\bibinitperiod}}}%
        {{hash=e431854cbe69ee8d6f8d60d4e33e26a0}{%
           family={Ionescu},
           familyi={I\bibinitperiod},
           given={Bogdan},
           giveni={B\bibinitperiod}}}%
        {{hash=e4a85ed94c01fe19740378a7ed805e27}{%
           family={Quang},
           familyi={Q\bibinitperiod},
           given={Vu\bibnamedelima Lam},
           giveni={V\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
        {{hash=058f9bb989e2b6aa619f923aa82450df}{%
           family={Jiang},
           familyi={J\bibinitperiod},
           given={Yu\bibnamedelima Gang},
           giveni={Y\bibinitperiod\bibinitdelim G\bibinitperiod}}}%
      }
      \strng{namehash}{7c9c2b1b97453f9570bbba23c1804ee0}
      \strng{fullhash}{98ccbf1d5a5ccc90c6e7297ee34b538a}
      \strng{bibnamehash}{c31f8473bce8ffe1b2892ee3bb019605}
      \strng{authorbibnamehash}{c31f8473bce8ffe1b2892ee3bb019605}
      \strng{authornamehash}{7c9c2b1b97453f9570bbba23c1804ee0}
      \strng{authorfullhash}{98ccbf1d5a5ccc90c6e7297ee34b538a}
      \field{sortinit}{3}
      \field{sortinithash}{a4b52e5432884761f50fb9571273b93e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper provides a description of the MediaEval 2013 Affect Task Violent Scenes Detection. This task, which is proposed for the third year to the research community, derives directly from a Technicolor use case which aims at easing a user's selection process from a movie database. This task will therefore apply to movie content. We provide some insight into the Technicolor use case, before giving details on the task itself, which has seen some changes in 2013. Dataset, annotations, and evaluation criteria as well as the required and optional runs are described.}
      \field{booktitle}{CEUR Workshop Proceedings}
      \field{issn}{16130073}
      \field{title}{{The MediaEval 2013 affect task: Violent Scenes Detection}}
      \field{year}{2013}
    \endentry
    \entry{Giannakopoulos2006}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=e032ec645401a1838ee933a73d41794f}{%
           family={Giannakopoulos},
           familyi={G\bibinitperiod},
           given={Theodoros},
           giveni={T\bibinitperiod}}}%
        {{hash=4910c9f2e5d6feca84c56fc6bca10fda}{%
           family={Kosmopoulos},
           familyi={K\bibinitperiod},
           given={Dimitrios},
           giveni={D\bibinitperiod}}}%
        {{hash=3b50a230f9dd108433d683b1417c549f}{%
           family={Aristidou},
           familyi={A\bibinitperiod},
           given={Andreas},
           giveni={A\bibinitperiod}}}%
        {{hash=fd3acabdbe6cc1c8fdbab64dd76ae135}{%
           family={Theodoridis},
           familyi={T\bibinitperiod},
           given={Sergios},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{91fdaae41bab2442477f6059454f1c6b}
      \strng{fullhash}{535eb30dba8849193930b58e15868b81}
      \strng{bibnamehash}{535eb30dba8849193930b58e15868b81}
      \strng{authorbibnamehash}{535eb30dba8849193930b58e15868b81}
      \strng{authornamehash}{91fdaae41bab2442477f6059454f1c6b}
      \strng{authorfullhash}{535eb30dba8849193930b58e15868b81}
      \field{sortinit}{3}
      \field{sortinithash}{a4b52e5432884761f50fb9571273b93e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This work studies the problem of violence detection in audio data, which can be used for automated content rating. We employ some popular frame-level audio features both from the time and frequency domain. Afterwards, several statistics of the calculated feature sequences are fed as input to a Support Vector Machine classifier, which decides about the segment content with respect to violence. The presented experimental results verify the validity of the approach and exhibit a better performance than the other known approaches. {©} Springer-Verlag Berlin Heidelberg 2006.}
      \field{booktitle}{Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
      \field{isbn}{354034117X}
      \field{issn}{03029743}
      \field{title}{{Violence content classification using audio features}}
      \field{year}{2006}
      \verb{doi}
      \verb 10.1007/11752912_55
      \endverb
    \endentry
    \entry{Giannakopoulos2010}{inproceedings}{}
      \name{author}{5}{}{%
        {{hash=e032ec645401a1838ee933a73d41794f}{%
           family={Giannakopoulos},
           familyi={G\bibinitperiod},
           given={Theodoros},
           giveni={T\bibinitperiod}}}%
        {{hash=c45ba1037f24a9ad6866153a209f54bb}{%
           family={Makris},
           familyi={M\bibinitperiod},
           given={Alexandros},
           giveni={A\bibinitperiod}}}%
        {{hash=4910c9f2e5d6feca84c56fc6bca10fda}{%
           family={Kosmopoulos},
           familyi={K\bibinitperiod},
           given={Dimitrios},
           giveni={D\bibinitperiod}}}%
        {{hash=5e81142e26ff9012396dded1c3a1dfcc}{%
           family={Perantonis},
           familyi={P\bibinitperiod},
           given={Stavros},
           giveni={S\bibinitperiod}}}%
        {{hash=fd3acabdbe6cc1c8fdbab64dd76ae135}{%
           family={Theodoridis},
           familyi={T\bibinitperiod},
           given={Sergios},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{85a934c0c1802132e4e20bd06066ea50}
      \strng{fullhash}{b9fceb18c1424cbfdf1828c71ea663fe}
      \strng{bibnamehash}{b9fceb18c1424cbfdf1828c71ea663fe}
      \strng{authorbibnamehash}{b9fceb18c1424cbfdf1828c71ea663fe}
      \strng{authornamehash}{85a934c0c1802132e4e20bd06066ea50}
      \strng{authorfullhash}{b9fceb18c1424cbfdf1828c71ea663fe}
      \field{sortinit}{4}
      \field{sortinithash}{11cdaee3b18e01d77f3f428b13c1fc76}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In this paper we present our research towards the detection of violent scenes in movies, employing fusion methodologies, based on learning. Towards this goal, a multi-step approach is followed: initially, automated auditory and visual processing and analysis is performed in order to estimate probabilistic measures regarding particular audio and visual related classes. At a second stage, a meta-classification architecture is adopted, which combines the audio and visual information, in order to classify mid-term video segments as "violent" or "non-violent". The proposed scheme has been evaluated on a real dataset from 10 films. {©} Springer-Verlag Berlin Heidelberg 2010.}
      \field{booktitle}{Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
      \field{isbn}{3642128416}
      \field{issn}{03029743}
      \field{title}{{Audio-visual fusion for detecting violent scenes in videos}}
      \field{year}{2010}
      \verb{doi}
      \verb 10.1007/978-3-642-12842-4_13
      \endverb
      \keyw{Multi-modal video classification,Violence detection}
    \endentry
    \entry{Ali2018}{article}{}
      \name{author}{2}{}{%
        {{hash=fdcb4772c49be5c6e264e67eae9e56aa}{%
           family={Ali},
           familyi={A\bibinitperiod},
           given={Ashikin},
           giveni={A\bibinitperiod}}}%
        {{hash=eff078098dc38e55217993b529280211}{%
           family={Senan},
           familyi={S\bibinitperiod},
           given={Norhalina},
           giveni={N\bibinitperiod}}}%
      }
      \strng{namehash}{aff433f1660ce99ead1696e8eeb59c31}
      \strng{fullhash}{aff433f1660ce99ead1696e8eeb59c31}
      \strng{bibnamehash}{aff433f1660ce99ead1696e8eeb59c31}
      \strng{authorbibnamehash}{aff433f1660ce99ead1696e8eeb59c31}
      \strng{authornamehash}{aff433f1660ce99ead1696e8eeb59c31}
      \strng{authorfullhash}{aff433f1660ce99ead1696e8eeb59c31}
      \field{sortinit}{4}
      \field{sortinithash}{11cdaee3b18e01d77f3f428b13c1fc76}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Violence is autonomous, the contents that one would not let children to see in movies or web videos. This is a challenging problem due to strong content variations among the positive instances. To solve this problem, implementation of deep neural network to classify the violence content in videos is proposed. Currently, deep neural network has shown its efficiency in natural language processing, fraud detection, social media, text classification, image classification. Regardless of the conventional methods applied to overcome this issue, but these techniques seem insufficiently accurate and does not adopt well to certain webs or user needs. Therefore, the purpose of this study is to assess the classification performances on violence video using Deep Neural Network (DNN). Hence, in this paper different architectures of hidden layers and hidden nodes in DNN have been implemented using the try-error method and equation based method, to examine the effect of the number of hidden layers and hidden nodes to the classification performance. From the results, it indicates 53{\%} accuracy rate for try and error approach, meanwhile for equation based approach it indicates 51{\%} accuracy rate.}
      \field{isbn}{9783319725499}
      \field{issn}{21945357}
      \field{journaltitle}{Advances in Intelligent Systems and Computing}
      \field{title}{{Violence video classification performance using deep neural networks}}
      \field{volume}{700}
      \field{year}{2018}
      \field{pages}{225\bibrangedash 233}
      \range{pages}{9}
      \verb{doi}
      \verb 10.1007/978-3-319-72550-5_22
      \endverb
      \verb{file}
      \verb :Users/Oscar/Downloads/Ali-Senan2018{\_}Chapter{\_}ViolenceVideoClassificationPer.pdf:pdf
      \endverb
      \keyw{Artificial neural network,Classification,Deep neural network,Violence video}
    \endentry
    \entry{Chua2014}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=d436e6485a2785e7cf90c5b0d669698d}{%
           family={Chua},
           familyi={C\bibinitperiod},
           given={Teck\bibnamedelima Wee},
           giveni={T\bibinitperiod\bibinitdelim W\bibinitperiod}}}%
        {{hash=882bb0b25a103fa5e9823dfb2502d89a}{%
           family={Leman},
           familyi={L\bibinitperiod},
           given={Karianto},
           giveni={K\bibinitperiod}}}%
        {{hash=c8c5bb9f8ba6ff0df0521ea75cae9bda}{%
           family={Gao},
           familyi={G\bibinitperiod},
           given={Feng},
           giveni={F\bibinitperiod}}}%
      }
      \strng{namehash}{2987f3d62b41136d596910e540aeaa10}
      \strng{fullhash}{2987f3d62b41136d596910e540aeaa10}
      \strng{bibnamehash}{2987f3d62b41136d596910e540aeaa10}
      \strng{authorbibnamehash}{2987f3d62b41136d596910e540aeaa10}
      \strng{authornamehash}{2987f3d62b41136d596910e540aeaa10}
      \strng{authorfullhash}{2987f3d62b41136d596910e540aeaa10}
      \field{sortinit}{4}
      \field{sortinithash}{11cdaee3b18e01d77f3f428b13c1fc76}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Modern elevators are equipped with closed-circuit television (CCTV) cameras to record videos for post-incident investigation rather than providing proactive event monitoring. While there are some attempts at automated video surveillance, events such as urinating, vandalism, and crimes that involved vulnerable targets may not exhibit significant visual cues. On contrary, such events are more discerning from audio cues. In this work, we propose a hierarchical audio-visual surveillance framework for elevators. Audio analytic module acts as the front line detector to monitor for such events. This means audio cue is the main determining source to infer the event occurrence. The secondary inference process involves queries to visual analytic module to build-up the evidences leading to event detection. We validate the performance of our system at a residential trial site and the initial results are promising. {©} 2014 Springer International Publishing.}
      \field{booktitle}{Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
      \field{isbn}{9783319041162}
      \field{issn}{03029743}
      \field{title}{{Hierarchical audio-visual surveillance for passenger elevators}}
      \field{year}{2014}
      \verb{doi}
      \verb 10.1007/978-3-319-04117-9_5
      \endverb
    \endentry
    \entry{Garcia-Gomez2016}{inproceedings}{}
      \name{author}{5}{}{%
        {{hash=7f0280add579ec87f2409c8ec89fbc6e}{%
           family={Garc{í}a-G{ó}mez},
           familyi={G\bibinithyphendelim G\bibinitperiod},
           given={Joaqu{í}n},
           giveni={J\bibinitperiod}}}%
        {{hash=115b1f7caed7019c4ddce1e07b47efbf}{%
           family={Bautista-Dur{á}n},
           familyi={B\bibinithyphendelim D\bibinitperiod},
           given={Marta},
           giveni={M\bibinitperiod}}}%
        {{hash=00390bd046602da7f8801f4764a8400f}{%
           family={Gil-Pita},
           familyi={G\bibinithyphendelim P\bibinitperiod},
           given={Roberto},
           giveni={R\bibinitperiod}}}%
        {{hash=2e1237bf39c89be621943b6888e7032b}{%
           family={Mohino-Herranz},
           familyi={M\bibinithyphendelim H\bibinitperiod},
           given={Inma},
           giveni={I\bibinitperiod}}}%
        {{hash=9262296bcd40ac19d2654f0421d2bed6}{%
           family={Rosa-Zurera},
           familyi={R\bibinithyphendelim Z\bibinitperiod},
           given={Manuel},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{1eb1fb8882bb3358ee563b9b23918ab0}
      \strng{fullhash}{9302244f8a6a6e012cbc74b32700b068}
      \strng{bibnamehash}{9302244f8a6a6e012cbc74b32700b068}
      \strng{authorbibnamehash}{9302244f8a6a6e012cbc74b32700b068}
      \strng{authornamehash}{1eb1fb8882bb3358ee563b9b23918ab0}
      \strng{authorfullhash}{9302244f8a6a6e012cbc74b32700b068}
      \field{sortinit}{4}
      \field{sortinithash}{11cdaee3b18e01d77f3f428b13c1fc76}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Violence continues being an important problem in the society. Thousands of people suffer its effects every day and statistics show this number has maintained or almost increased recently. In the modern environment of smart cities there is a necessity to develop a system capable of detecting if a violent situation is taking place or not. In this paper we present an automatic acoustic violence detection system for smart cities, integrating both signal processing and pattern recognition techniques. The proposed software has been implemented in three steps: feature extraction in time and frequency domain, genetic algorithm implementation in order to select the best features, and classification to take a binary decision. Results derived from the experiments show that MFCCs are the best features for violence detection, and others like pitch or short time energy have also a good performance. In other words, features that can distinguish between voiced and unvoiced frames seem to be a good election for violence detection in real environments.}
      \field{booktitle}{Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}
      \field{isbn}{9783319487984}
      \field{issn}{16113349}
      \field{title}{{Violence detection in real environments for smart cities}}
      \field{year}{2016}
      \verb{doi}
      \verb 10.1007/978-3-319-48799-1_52
      \endverb
      \keyw{Audio features,Audio processing,Feature selection,Violence detection}
    \endentry
    \entry{Bautista-Duran2017}{inproceedings}{}
      \name{author}{6}{}{%
        {{hash=4c4e6a04694f3a10c5216618152fb73b}{%
           family={Bautista-Duran},
           familyi={B\bibinithyphendelim D\bibinitperiod},
           given={Marta},
           giveni={M\bibinitperiod}}}%
        {{hash=7f0280add579ec87f2409c8ec89fbc6e}{%
           family={Garc{í}a-G{ó}mez},
           familyi={G\bibinithyphendelim G\bibinitperiod},
           given={Joaqu{í}n},
           giveni={J\bibinitperiod}}}%
        {{hash=00390bd046602da7f8801f4764a8400f}{%
           family={Gil-Pita},
           familyi={G\bibinithyphendelim P\bibinitperiod},
           given={Roberto},
           giveni={R\bibinitperiod}}}%
        {{hash=f7cb65fa761287cc1f7383f6090ba245}{%
           family={S{á}nchez-Hevia},
           familyi={S\bibinithyphendelim H\bibinitperiod},
           given={H{é}ctor},
           giveni={H\bibinitperiod}}}%
        {{hash=2e1237bf39c89be621943b6888e7032b}{%
           family={Mohino-Herranz},
           familyi={M\bibinithyphendelim H\bibinitperiod},
           given={Inma},
           giveni={I\bibinitperiod}}}%
        {{hash=9262296bcd40ac19d2654f0421d2bed6}{%
           family={Rosa-Zurera},
           familyi={R\bibinithyphendelim Z\bibinitperiod},
           given={Manuel},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{a08c26f45ac7720fc96440b4ea7ebebb}
      \strng{fullhash}{ffe4ce4907b163158e179e33d2499150}
      \strng{bibnamehash}{7a52fc90ff014c90d4b2e38788725d89}
      \strng{authorbibnamehash}{7a52fc90ff014c90d4b2e38788725d89}
      \strng{authornamehash}{a08c26f45ac7720fc96440b4ea7ebebb}
      \strng{authorfullhash}{ffe4ce4907b163158e179e33d2499150}
      \field{sortinit}{4}
      \field{sortinithash}{11cdaee3b18e01d77f3f428b13c1fc76}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Detecting violence is an important task due to the amount of people who suffer its effects daily. There is a tendency to focus the problem either in real situations or in non real ones, but both of them are useful on its own right. Until this day there has not been clear effort to try to relate both environments. In this work we try to detect violent situations on two different acoustic databases through the use of crossed information from one of them into the other. The system has been divided into three stages: feature extraction, feature selection based on genetic algorithms and classification to take a binary decision. Results focus on comparing performance loss when a database is evaluated with features selected on itself, or selection based in the other database. In general, complex classifiers tend to suffer higher losses, whereas simple classifiers, such as linear and quadratic detectors, offers less than a 10{\%} loss in most situations.}
      \field{booktitle}{ICPRAM 2017 - Proceedings of the 6th International Conference on Pattern Recognition Applications and Methods}
      \field{isbn}{9789897582226}
      \field{title}{{Acoustic detection of violence in real and fictional environments}}
      \field{year}{2017}
      \verb{doi}
      \verb 10.5220/0006195004560462
      \endverb
      \keyw{Audio processing,Feature selection,Fictional environment,Real environment,Violence detection}
    \endentry
    \entry{UnitedNations1989}{misc}{}
      \name{author}{1}{}{%
        {{hash=3cc54e2c0d69c1b76b4d49edb22dd6d1}{%
           family={{United Nations}},
           familyi={U\bibinitperiod}}}%
      }
      \strng{namehash}{3cc54e2c0d69c1b76b4d49edb22dd6d1}
      \strng{fullhash}{3cc54e2c0d69c1b76b4d49edb22dd6d1}
      \strng{bibnamehash}{3cc54e2c0d69c1b76b4d49edb22dd6d1}
      \strng{authorbibnamehash}{3cc54e2c0d69c1b76b4d49edb22dd6d1}
      \strng{authornamehash}{3cc54e2c0d69c1b76b4d49edb22dd6d1}
      \strng{authorfullhash}{3cc54e2c0d69c1b76b4d49edb22dd6d1}
      \field{sortinit}{4}
      \field{sortinithash}{11cdaee3b18e01d77f3f428b13c1fc76}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{{No Title}}
      \field{year}{1989}
      \field{pages}{11}
      \range{pages}{1}
    \endentry
    \entry{WHO2013}{book}{}
      \name{author}{1}{}{%
        {{hash=7847ad5f34be3d081b6fa006e3062025}{%
           family={{WHO. Department of Reproductive Health Research. London School of Hygiene and Tropical Medicine. South African Medical Research Council.}},
           familyi={W\bibinitperiod}}}%
      }
      \strng{namehash}{7847ad5f34be3d081b6fa006e3062025}
      \strng{fullhash}{7847ad5f34be3d081b6fa006e3062025}
      \strng{bibnamehash}{7847ad5f34be3d081b6fa006e3062025}
      \strng{authorbibnamehash}{7847ad5f34be3d081b6fa006e3062025}
      \strng{authornamehash}{7847ad5f34be3d081b6fa006e3062025}
      \strng{authorfullhash}{7847ad5f34be3d081b6fa006e3062025}
      \field{sortinit}{4}
      \field{sortinithash}{11cdaee3b18e01d77f3f428b13c1fc76}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The report presents the first global systematic review of scientific data on the prevalence of two forms of violence against women: violence by an intimate partner (intimate partner violence) and sexual violence by someone other than a partner (non-partner sexual violence). It shows, for the first time, global and regional estimates of the prevalence of these two forms of violence, using data from around the world. Previous reporting on violence against women has not differentiated between partner and non-partner violence.}
      \field{booktitle}{WHO}
      \field{title}{{WHO | Global and regional estimates of violence against women}}
      \field{year}{2013}
    \endentry
    \entry{EuropeanUnionAgencyforFundamentalRights2014}{book}{}
      \name{author}{1}{}{%
        {{hash=609c512976cff8b20bc8500b3d198999}{%
           family={{European Union Agency for Fundamental Rights}},
           familyi={E\bibinitperiod}}}%
      }
      \strng{namehash}{609c512976cff8b20bc8500b3d198999}
      \strng{fullhash}{609c512976cff8b20bc8500b3d198999}
      \strng{bibnamehash}{609c512976cff8b20bc8500b3d198999}
      \strng{authorbibnamehash}{609c512976cff8b20bc8500b3d198999}
      \strng{authornamehash}{609c512976cff8b20bc8500b3d198999}
      \strng{authorfullhash}{609c512976cff8b20bc8500b3d198999}
      \field{sortinit}{4}
      \field{sortinithash}{11cdaee3b18e01d77f3f428b13c1fc76}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Violence against women undermines women's core fundamental rights such as dignity, access to justice and gender equality. For example, one in three women has experienced physical and/or sexual violence since the age of 15; one in five women has experienced stalking; every second woman has been confronted with one or more forms of sexual harassment. What emerges is a picture of extensive abuse that affects many women's lives but is systematically under-reported to the authorities. The scale of violence against women is therefore not reflected by official data. This FRA survey is the first of its kind on violence against women across the 28 Member States of the European Union (EU). It is based on interviews with 42,000 women across the EU, who were asked about their experiences of physical, sexual and psychological violence, including incidents of intimate partner violence (‘domestic violence'). The survey also included questions on stalking, sexual harassment, and the role played by new technologies in women's experiences of abuse. In addition, it asked about their experiences of violence in childhood. Based on the detailed findings, FRA suggests courses of action in different areas that are touched by violence against women and go beyond the narrow confines of criminal law, ranging from employment and health to the medium of new technologies.}
      \field{booktitle}{Publications Office of the European Union,}
      \field{isbn}{9789292393427}
      \field{title}{{Violence against women : An EU-wide survey}}
      \field{year}{2014}
      \verb{doi}
      \verb 10.2811/62230
      \endverb
      \keyw{Violence against women,$\backslash$access to justice,attitudes and awareness,awareness of laws and political initiatives addres,awareness of selected organisations and specialise,carrying something for self-defence,characteristics of perpetrators,characteristics of victims,children's exposure to domestic violence,comprehensive and comparable data,confidence intervals and characteristics of the re,consequences,contact with police or other services,context,criminal law,details about intimate partner violence,details about non-partner violence,dignity,domestic violence,effect of the interview mode when asking sensitive,emotional responses,employment,fear of victimisation and its impact,forms of violence,fundamental rights,gender equality,health,impact of age,injuries,interviews,intimate partner violence,key results for selected respondent groups,methodology,national surveys on violence against women,needs of vitims,new technologies,official data,perceptions on frequency of violence against women,physical violence,prevalence rates,psychological consequences,psychological violence,relationship between violence in childhood and lat,relationship between worry and risk avoidance beha,respondant background variables,sexual harassment,sexual violence,stalking,survey,survey fieldwork outcomes,type of offender,under-reporting,violence in childhood,weighting,women's attitude towards doctors' role in identify,women's awareness of campaigns addressing violence,women's awareness of organisations and specialised,women's knowledge about other women victims of int,women's risk avoidance behaviour,worry about physical or sexual assault}
    \endentry
    \entry{Heise1999}{misc}{}
      \name{author}{3}{}{%
        {{hash=ce7fcd7bebad71e8d3deabf27bebf1c1}{%
           family={Heise},
           familyi={H\bibinitperiod},
           given={Lori},
           giveni={L\bibinitperiod}}}%
        {{hash=3ae4cda81b1a3fdef3b4c531498fc686}{%
           family={Ellsberg},
           familyi={E\bibinitperiod},
           given={Mary},
           giveni={M\bibinitperiod}}}%
        {{hash=a13c980bc429da246389ee92a8704824}{%
           family={Gottemoeller},
           familyi={G\bibinitperiod},
           given={Megan},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{d242cd889d17afcf342d3a6643767931}
      \strng{fullhash}{d242cd889d17afcf342d3a6643767931}
      \strng{bibnamehash}{d242cd889d17afcf342d3a6643767931}
      \strng{authorbibnamehash}{d242cd889d17afcf342d3a6643767931}
      \strng{authornamehash}{d242cd889d17afcf342d3a6643767931}
      \strng{authorfullhash}{d242cd889d17afcf342d3a6643767931}
      \field{sortinit}{4}
      \field{sortinithash}{11cdaee3b18e01d77f3f428b13c1fc76}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Population reports. Series L, Issues in world health}
      \field{issn}{01975838}
      \field{title}{{Ending violence against women.}}
      \field{year}{1999}
      \verb{doi}
      \verb 10.4324/9780429269516-5
      \endverb
    \endentry
    \entry{Beyer2015}{article}{}
      \name{author}{3}{}{%
        {{hash=e60c618abdddba17e71d62d1b079d742}{%
           family={Beyer},
           familyi={B\bibinitperiod},
           given={Kirsten},
           giveni={K\bibinitperiod}}}%
        {{hash=3efa62039ba92300edf22e3eb545939a}{%
           family={Wallis},
           familyi={W\bibinitperiod},
           given={Anne\bibnamedelima Baber},
           giveni={A\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
        {{hash=d4427826df3100e1283e9c9b0e8d8584}{%
           family={Hamberger},
           familyi={H\bibinitperiod},
           given={L.\bibnamedelimi Kevin},
           giveni={L\bibinitperiod\bibinitdelim K\bibinitperiod}}}%
      }
      \strng{namehash}{f45ddca38fedc418b6b10fbcdba44b7d}
      \strng{fullhash}{f45ddca38fedc418b6b10fbcdba44b7d}
      \strng{bibnamehash}{f45ddca38fedc418b6b10fbcdba44b7d}
      \strng{authorbibnamehash}{f45ddca38fedc418b6b10fbcdba44b7d}
      \strng{authornamehash}{f45ddca38fedc418b6b10fbcdba44b7d}
      \strng{authorfullhash}{f45ddca38fedc418b6b10fbcdba44b7d}
      \field{sortinit}{5}
      \field{sortinithash}{3c19c3776b658b3558e9e2e4840c01e2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Intimate partner violence (IPV) is an important global public health problem, affecting women across the life span and increasing risk for a number of unfavorable health outcomes. Typically conceptualized as a private form of violence, most research has focused on individual-level risk markers. Recently, more scholarly attention has been paid to the role that the residential neighborhood environment may play in influencing the occurrence of IPV. With research accumulating since the 1990s, increasing prominence of the topic, and no comprehensive literature reviews yet undertaken, it is time to take stock of what is known, what remains unknown, and the methods and concepts investigators have considered. In this article, we undertake a comprehensive, systematic review of the literature to date on the relationship between neighborhood environment and IPV, asking, “what is the status of scholarship related to the association between neighborhood environment and IPV occurrence?” Although the literature is young, it is receiving increasing attention from researchers in sociology, public health, criminology, and other fields. Obvious gaps in the literature include limited consideration of nonurban areas, limited theoretical motivation, and limited consideration of the range of potential contributors to environmental effects on IPV—such as built environmental factors or access to services. In addition, explanations of the pathways by which place influences the occurrence of IPV draw mainly from social disorganization theory that was developed in urban settings in the United States and may need to be adapted, especially to be useful in explaining residential environmental correlates of IPV in rural or non-U.S. settings. A more complete theoretical understanding of the relationship between neighborhood environment and IPV, especially considering differences among urban, semiurban, and rural settings and developed and developing country settings, will be necessary to advance research questions and improve policy and intervention responses to reduce the burden of IPV.}
      \field{issn}{15528324}
      \field{journaltitle}{Trauma, Violence, and Abuse}
      \field{title}{{Neighborhood Environment and Intimate Partner Violence: A Systematic Review}}
      \field{year}{2015}
      \verb{doi}
      \verb 10.1177/1524838013515758
      \endverb
      \keyw{community violence,cultural contexts,domestic violence}
    \endentry
    \entry{Salamon2017}{inproceedings}{}
      \name{author}{5}{}{%
        {{hash=6f29e3fb8b241a304c6a9778479b58d2}{%
           family={Salamon},
           familyi={S\bibinitperiod},
           given={Justin},
           giveni={J\bibinitperiod}}}%
        {{hash=440f2ec19b7e6bd89e9b0e137b96153e}{%
           family={MacConnell},
           familyi={M\bibinitperiod},
           given={Duncan},
           giveni={D\bibinitperiod}}}%
        {{hash=8aeab975280e4a1240315fc0e989af47}{%
           family={Cartwright},
           familyi={C\bibinitperiod},
           given={Mark},
           giveni={M\bibinitperiod}}}%
        {{hash=137297e0019ba55bca6f9b200782ae52}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Peter},
           giveni={P\bibinitperiod}}}%
        {{hash=dc1dca849ac4954f3a5b35d55cd40631}{%
           family={Bello},
           familyi={B\bibinitperiod},
           given={Juan\bibnamedelima Pablo},
           giveni={J\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
      }
      \strng{namehash}{9180d64e469adf6b63b10942a5d23f46}
      \strng{fullhash}{5e1fe6669f4ffd74d53112a012971882}
      \strng{bibnamehash}{5e1fe6669f4ffd74d53112a012971882}
      \strng{authorbibnamehash}{5e1fe6669f4ffd74d53112a012971882}
      \strng{authornamehash}{9180d64e469adf6b63b10942a5d23f46}
      \strng{authorfullhash}{5e1fe6669f4ffd74d53112a012971882}
      \field{sortinit}{5}
      \field{sortinithash}{3c19c3776b658b3558e9e2e4840c01e2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Sound event detection (SED) in environmental recordings is a key topic of research in machine listening, with applications in noise monitoring for smart cities, self-driving cars, surveillance, bioa-coustic monitoring, and indexing of large multimedia collections. Developing new solutions for SED often relies on the availability of strongly labeled audio recordings, where the annotation includes the onset, offset and source of every event. Generating such precise annotations manually is very time consuming, and as a result existing datasets for SED with strong labels are scarce and limited in size. To address this issue, we present Scaper, an open-source library for soundscape synthesis and augmentation. Given a collection of iso-lated sound events, Scaper acts as a high-level sequencer that can generate multiple soundscapes from a single, probabilistically defined, 'specification'. To increase the variability of the output, Scaper supports the application of audio transformations such as pitch shifting and time stretching individually to every event. To illustrate the potential of the library, we generate a dataset of 10,000 sound-scapes and use it to compare the performance of two state-of-The-Art algorithms, including a breakdown by soundscape characteristics. We also describe how Scaper was used to generate audio stimuli for an audio labeling crowdsourcing experiment, and conclude with a discussion of Scaper's limitations and potential applications.}
      \field{booktitle}{IEEE Workshop on Applications of Signal Processing to Audio and Acoustics}
      \field{isbn}{9781538616321}
      \field{title}{{Scaper: A library for soundscape synthesis and augmentation}}
      \field{year}{2017}
      \verb{doi}
      \verb 10.1109/WASPAA.2017.8170052
      \endverb
      \keyw{Soundscape,sound event detection,synthesis}
    \endentry
    \entry{Mapell2012}{misc}{}
      \name{author}{1}{}{%
        {{hash=3f02c19a413912033db647b913ccef77}{%
           family={Mapell},
           familyi={M\bibinitperiod},
           given={Val{é}rie},
           giveni={V\bibinitperiod}}}%
      }
      \strng{namehash}{3f02c19a413912033db647b913ccef77}
      \strng{fullhash}{3f02c19a413912033db647b913ccef77}
      \strng{bibnamehash}{3f02c19a413912033db647b913ccef77}
      \strng{authorbibnamehash}{3f02c19a413912033db647b913ccef77}
      \strng{authornamehash}{3f02c19a413912033db647b913ccef77}
      \strng{authorfullhash}{3f02c19a413912033db647b913ccef77}
      \field{sortinit}{5}
      \field{sortinithash}{3c19c3776b658b3558e9e2e4840c01e2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{{UPC-TALP database of isolated meeting-room acoustic events}}
      \field{year}{2012}
      \verb{urlraw}
      \verb http://catalog.elra.info/en-us/repository/browse/ELRA-S0268/
      \endverb
      \verb{url}
      \verb http://catalog.elra.info/en-us/repository/browse/ELRA-S0268/
      \endverb
    \endentry
    \entry{Foggia2015}{article}{}
      \name{author}{5}{}{%
        {{hash=761465c5516ba160c8add4d6165ceed5}{%
           family={Foggia},
           familyi={F\bibinitperiod},
           given={Pasquale},
           giveni={P\bibinitperiod}}}%
        {{hash=eef9a82a22125c381c2932d53b56b78c}{%
           family={Petkov},
           familyi={P\bibinitperiod},
           given={Nicolai},
           giveni={N\bibinitperiod}}}%
        {{hash=fa9310366f918cdf2d108c6116ebe16b}{%
           family={Saggese},
           familyi={S\bibinitperiod},
           given={Alessia},
           giveni={A\bibinitperiod}}}%
        {{hash=2a39af5e8ff61dd45815ce2652ffaafb}{%
           family={Strisciuglio},
           familyi={S\bibinitperiod},
           given={Nicola},
           giveni={N\bibinitperiod}}}%
        {{hash=9b180a65744f70a23bdcd75a767f4184}{%
           family={Vento},
           familyi={V\bibinitperiod},
           given={Mario},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{e3cc5901fc26a37b1821bc7b64b691fb}
      \strng{fullhash}{21181897ce2528f104c5444cf0344c1b}
      \strng{bibnamehash}{21181897ce2528f104c5444cf0344c1b}
      \strng{authorbibnamehash}{21181897ce2528f104c5444cf0344c1b}
      \strng{authornamehash}{e3cc5901fc26a37b1821bc7b64b691fb}
      \strng{authorfullhash}{21181897ce2528f104c5444cf0344c1b}
      \field{sortinit}{5}
      \field{sortinithash}{3c19c3776b658b3558e9e2e4840c01e2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In this paper we propose a novel method for the detection of audio events for surveillance applications. The method is based on the bag of words approach, adapted to deal with the specific issues of audio surveillance: the need to recognize both short and long sounds, the presence of a significant noise level and of superimposed background sounds of intensity comparable to the audio events to be detected. In order to test the proposed method in complex, realistic scenarios, we have built a large, publicly available dataset of audio events. The dataset has allowed us to evaluate the robustness of our method with respect to varying levels of the Signal-to-Noise Ratio; the experimentation has confirmed its applicability under real world conditions, and has shown a significant performance improvement with respect to other methods from the literature.}
      \field{issn}{01678655}
      \field{journaltitle}{Pattern Recognition Letters}
      \field{title}{{Reliable detection of audio events in highly noisy environments}}
      \field{year}{2015}
      \verb{doi}
      \verb 10.1016/j.patrec.2015.06.026
      \endverb
      \keyw{Audio surveillance,Bag of words,Event detection}
    \endentry
    \entry{Fagerlund2017}{misc}{}
      \name{author}{2}{}{%
        {{hash=d2017ae3d6f85b25f73b6d2aa2908eb1}{%
           family={Fagerlund},
           familyi={F\bibinitperiod},
           given={Eemi},
           giveni={E\bibinitperiod}}}%
        {{hash=c0f1b501dca2cfde0bc9c0803c7fdf85}{%
           family={Hiltunen},
           familyi={H\bibinitperiod},
           given={Aku},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{d74a4cad455393c2fef7dc47d5466888}
      \strng{fullhash}{d74a4cad455393c2fef7dc47d5466888}
      \strng{bibnamehash}{d74a4cad455393c2fef7dc47d5466888}
      \strng{authorbibnamehash}{d74a4cad455393c2fef7dc47d5466888}
      \strng{authornamehash}{d74a4cad455393c2fef7dc47d5466888}
      \strng{authorfullhash}{d74a4cad455393c2fef7dc47d5466888}
      \field{sortinit}{5}
      \field{sortinithash}{3c19c3776b658b3558e9e2e4840c01e2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{{TUT Rare sound events}}
      \field{year}{2017}
    \endentry
    \entry{Stowell2013}{misc}{}
      \name{author}{2}{}{%
        {{hash=720621ffa11137749699f824497e8e0b}{%
           family={Stowell},
           familyi={S\bibinitperiod},
           given={Dan},
           giveni={D\bibinitperiod}}}%
        {{hash=b068724f00d17295882e712edcc6315c}{%
           family={Benetos},
           familyi={B\bibinitperiod},
           given={Emmanouil},
           giveni={E\bibinitperiod}}}%
      }
      \strng{namehash}{d78ce7a9be213be35fbe009a089746ae}
      \strng{fullhash}{d78ce7a9be213be35fbe009a089746ae}
      \strng{bibnamehash}{d78ce7a9be213be35fbe009a089746ae}
      \strng{authorbibnamehash}{d78ce7a9be213be35fbe009a089746ae}
      \strng{authornamehash}{d78ce7a9be213be35fbe009a089746ae}
      \strng{authorfullhash}{d78ce7a9be213be35fbe009a089746ae}
      \field{sortinit}{5}
      \field{sortinithash}{3c19c3776b658b3558e9e2e4840c01e2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{{IEEE AASP Challenge: Detection and Classification of Acoustic Scenes and Events}}
      \field{year}{2013}
    \endentry
    \entry{Cakir2016}{misc}{}
      \name{author}{2}{}{%
        {{hash=8f2a35c32bdb4a55458172c6ed8caf7e}{%
           family={Cakir},
           familyi={C\bibinitperiod},
           given={Emre},
           giveni={E\bibinitperiod}}}%
        {{hash=6c32fc555cb6cb65401de698f7673917}{%
           family={Heittola},
           familyi={H\bibinitperiod},
           given={Toni},
           giveni={T\bibinitperiod}}}%
      }
      \strng{namehash}{5a2c1d7ee26d0f7f2f6a98522f665cef}
      \strng{fullhash}{5a2c1d7ee26d0f7f2f6a98522f665cef}
      \strng{bibnamehash}{5a2c1d7ee26d0f7f2f6a98522f665cef}
      \strng{authorbibnamehash}{5a2c1d7ee26d0f7f2f6a98522f665cef}
      \strng{authornamehash}{5a2c1d7ee26d0f7f2f6a98522f665cef}
      \strng{authorfullhash}{5a2c1d7ee26d0f7f2f6a98522f665cef}
      \field{sortinit}{5}
      \field{sortinithash}{3c19c3776b658b3558e9e2e4840c01e2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{{TUT-SED Synthetic}}
      \field{year}{2016}
    \endentry
    \entry{Demarty2015}{article}{}
      \name{author}{4}{}{%
        {{hash=1cfcfd6b95053490ce13f39a2f9fc831}{%
           family={Demarty},
           familyi={D\bibinitperiod},
           given={Claire\bibnamedelima H{é}l{è}ne},
           giveni={C\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
        {{hash=369e4a23a04ce26754dacdeb5204a807}{%
           family={Penet},
           familyi={P\bibinitperiod},
           given={C{é}dric},
           giveni={C\bibinitperiod}}}%
        {{hash=102f5f953b96751cafe841ae28608655}{%
           family={Soleymani},
           familyi={S\bibinitperiod},
           given={Mohammad},
           giveni={M\bibinitperiod}}}%
        {{hash=55a5f11a329674c150b3185dd236f3d8}{%
           family={Gravier},
           familyi={G\bibinitperiod},
           given={Guillaume},
           giveni={G\bibinitperiod}}}%
      }
      \strng{namehash}{485394e3fa224735d319c3edd4af4f5e}
      \strng{fullhash}{77a75ae3972b2c2bb343278394a24558}
      \strng{bibnamehash}{77a75ae3972b2c2bb343278394a24558}
      \strng{authorbibnamehash}{77a75ae3972b2c2bb343278394a24558}
      \strng{authornamehash}{485394e3fa224735d319c3edd4af4f5e}
      \strng{authorfullhash}{77a75ae3972b2c2bb343278394a24558}
      \field{sortinit}{5}
      \field{sortinithash}{3c19c3776b658b3558e9e2e4840c01e2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Content-based analysis to find where violence appears in multimedia content has several applications, from parental control and children protection to surveillance. This paper presents the design and annotation of the Violent Scene Detection dataset, a corpus targeting the detection of physical violence in Hollywood movies. We discuss definitions of physical violence and provide a simple and objective definition which was used to annotate a set of 18 movies, thus resulting in the largest freely-available dataset for such a task. We discuss borderline cases and compare with annotations based on a subjective definition which requires multiple annotators. We provide a detailed analysis of the corpus, in particular regarding the relationship between violence and a set of key audio and visual concepts which were also annotated. The VSD dataset results from two years of benchmarking in the framework of the MediaEval initiative. We provide results from the 2011 and 2012 benchmarks as a validation of the dataset and as a state-of-the-art baseline. The VSD dataset is freely available at the address:http://www.technicolor.com/en/innovation/research-innovation/scientific-data-sharing/violent-scenes-dataset.}
      \field{issn}{15737721}
      \field{journaltitle}{Multimedia Tools and Applications}
      \field{title}{{VSD, a public dataset for the detection of violent scenes in movies: design, annotation, analysis and evaluation}}
      \field{year}{2015}
      \verb{doi}
      \verb 10.1007/s11042-014-1984-4
      \endverb
      \keyw{Content-based analysis,Corpus design,Multimedia evaluation,Physical violence definition,Semantic audio concepts,Semantic video concepts,Violent scene detection}
    \endentry
    \entry{Gemmeke2017}{inproceedings}{}
      \name{author}{8}{}{%
        {{hash=91a565291c0e3c0bfd0bafb832bf2bf6}{%
           family={Gemmeke},
           familyi={G\bibinitperiod},
           given={Jort\bibnamedelima F.},
           giveni={J\bibinitperiod\bibinitdelim F\bibinitperiod}}}%
        {{hash=8c2dba7002458c6a90760f98d2e441f1}{%
           family={Ellis},
           familyi={E\bibinitperiod},
           given={Daniel\bibnamedelima P.W.},
           giveni={D\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
        {{hash=62ea759066a9bbd8dd7fb84cf45bc2f7}{%
           family={Freedman},
           familyi={F\bibinitperiod},
           given={Dylan},
           giveni={D\bibinitperiod}}}%
        {{hash=ff1b8e2ee4f94eecee46d9c11902ccae}{%
           family={Jansen},
           familyi={J\bibinitperiod},
           given={Aren},
           giveni={A\bibinitperiod}}}%
        {{hash=ece9c0f3c018e20ef7ede094b10059fb}{%
           family={Lawrence},
           familyi={L\bibinitperiod},
           given={Wade},
           giveni={W\bibinitperiod}}}%
        {{hash=2f9e6cba84a097d64d79fb6451a2d846}{%
           family={Moore},
           familyi={M\bibinitperiod},
           given={R.\bibnamedelimi Channing},
           giveni={R\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
        {{hash=fc103b936ff1d8fa40cfdc0b5ee79f52}{%
           family={Plakal},
           familyi={P\bibinitperiod},
           given={Manoj},
           giveni={M\bibinitperiod}}}%
        {{hash=02ca56291c902c02cf20caa730c79895}{%
           family={Ritter},
           familyi={R\bibinitperiod},
           given={Marvin},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{1f38ee742789c0f86200fcbd9bbdd127}
      \strng{fullhash}{b358d3108cf3fe1ec468f471471cf9c2}
      \strng{bibnamehash}{c73dd6c6deec934e5e3b65d9e22bdbdb}
      \strng{authorbibnamehash}{c73dd6c6deec934e5e3b65d9e22bdbdb}
      \strng{authornamehash}{1f38ee742789c0f86200fcbd9bbdd127}
      \strng{authorfullhash}{b358d3108cf3fe1ec468f471471cf9c2}
      \field{sortinit}{5}
      \field{sortinithash}{3c19c3776b658b3558e9e2e4840c01e2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Audio event recognition, the human-like ability to identify and re-late sounds from audio, is a nascent problem in machine percep-tion. Comparable problems such as object detection in images have reaped enormous benefits from comprehensive datasets – principally ImageNet. This paper describes the creation of Audio Set, a large-scale dataset of manually-annotated audio events that endeavors to bridge the gap in data availability between image and audio research. Using a carefully structured hierarchical ontology of 632 audio classes guided by the literature and manual curation, we collect data from human labelers to probe the presence of specific audio classes in 10 second segments of YouTube videos. Segments are proposed for labeling using searches based on metadata, context (e.g., links), and content analysis. The result is a dataset of unprecedented breadth and size that will, we hope, substantially stimulate the de-velopment of high-performance audio event recognizers.}
      \field{booktitle}{ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings}
      \field{isbn}{9781509041176}
      \field{issn}{15206149}
      \field{title}{{Audio Set: An ontology and human-labeled dataset for audio events}}
      \field{year}{2017}
      \verb{doi}
      \verb 10.1109/ICASSP.2017.7952261
      \endverb
      \keyw{Audio event detection,audio databases,data collection,sound ontology}
    \endentry
    \entry{Fonseca2017}{inproceedings}{}
      \name{author}{9}{}{%
        {{hash=65e2e2a8e414294d5c501fee29cb321c}{%
           family={Fonseca},
           familyi={F\bibinitperiod},
           given={Eduardo},
           giveni={E\bibinitperiod}}}%
        {{hash=b6a381b9b1b8ef5372019dcb3faf2d5d}{%
           family={Pons},
           familyi={P\bibinitperiod},
           given={Jordi},
           giveni={J\bibinitperiod}}}%
        {{hash=3a9953d4026ca5d333db1136bd19e2bd}{%
           family={Favory},
           familyi={F\bibinitperiod},
           given={Xavier},
           giveni={X\bibinitperiod}}}%
        {{hash=cdf5acb3a05edeecfb76307b9c727574}{%
           family={Font},
           familyi={F\bibinitperiod},
           given={Frederic},
           giveni={F\bibinitperiod}}}%
        {{hash=ad7d1c5f77687a9eb63c48597b5503c7}{%
           family={Bogdanov},
           familyi={B\bibinitperiod},
           given={Dmitry},
           giveni={D\bibinitperiod}}}%
        {{hash=8632f2fb7b3ab06a821fd5aae9887567}{%
           family={Ferraro},
           familyi={F\bibinitperiod},
           given={Andres},
           giveni={A\bibinitperiod}}}%
        {{hash=a60ff36288eaeef4d2b3958e6e6ea73e}{%
           family={Oramas},
           familyi={O\bibinitperiod},
           given={Sergio},
           giveni={S\bibinitperiod}}}%
        {{hash=ee5bc53a7f69f1ecc5d21d77fccbd50b}{%
           family={Porter},
           familyi={P\bibinitperiod},
           given={Alastair},
           giveni={A\bibinitperiod}}}%
        {{hash=f69acf45f53ac7c6a66d991fef16781a}{%
           family={Serra},
           familyi={S\bibinitperiod},
           given={Xavier},
           giveni={X\bibinitperiod}}}%
      }
      \strng{namehash}{d66b2acc37f093f9c748fc4efec407ae}
      \strng{fullhash}{de432f92663322d3479409d5243b91ed}
      \strng{bibnamehash}{23c9d9a68e7445ef1010c445771a7420}
      \strng{authorbibnamehash}{23c9d9a68e7445ef1010c445771a7420}
      \strng{authornamehash}{d66b2acc37f093f9c748fc4efec407ae}
      \strng{authorfullhash}{de432f92663322d3479409d5243b91ed}
      \field{sortinit}{5}
      \field{sortinithash}{3c19c3776b658b3558e9e2e4840c01e2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Openly available datasets are a key factor in the advancement of data-driven research approaches, including many of the ones used in sound and music computing. In the last few years, quite a number of new audio datasets have been made available but there are still major shortcomings in many of them to have a significant research impact. Among the common shortcomings are the lack of transparency in their creation and the difficulty of making them completely open and sharable. They often do not include clear mechanisms to amend errors and many times they are not large enough for current machine learning needs. This paper introduces Freesound Datasets, an online platform for the collaborative creation of open audio datasets based on principles of transparency, openness, dynamic character, and sustainability. As a proof-of-concept, we present an early snapshot of a large-scale audio dataset built using this platform. It consists of audio samples from Freesound organised in a hierarchy based on the AudioSet Ontology. We believe that building and maintaining datasets following the outlined principles and using open tools and collaborative approaches like the ones presented here will have a significant impact in our research community.}
      \field{booktitle}{Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017}
      \field{isbn}{9789811151798}
      \field{title}{{Freesound datasets: A platform for the creation of open audio datasets}}
      \field{year}{2017}
    \endentry
    \entry{Hearst1992}{inproceedings}{}
      \name{author}{1}{}{%
        {{hash=45330e5d45157c38c5519d8cdb6cb1d8}{%
           family={Hearst},
           familyi={H\bibinitperiod},
           given={Marti\bibnamedelima A.},
           giveni={M\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
      }
      \strng{namehash}{45330e5d45157c38c5519d8cdb6cb1d8}
      \strng{fullhash}{45330e5d45157c38c5519d8cdb6cb1d8}
      \strng{bibnamehash}{45330e5d45157c38c5519d8cdb6cb1d8}
      \strng{authorbibnamehash}{45330e5d45157c38c5519d8cdb6cb1d8}
      \strng{authornamehash}{45330e5d45157c38c5519d8cdb6cb1d8}
      \strng{authorfullhash}{45330e5d45157c38c5519d8cdb6cb1d8}
      \field{sortinit}{6}
      \field{sortinithash}{57e57fb8451e7fcfa45d1e069f6d3136}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexicosyntactic patterns that are easily recognizable, that occur frequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest that other lexical relations will also be acquirable in this way. A subset of the acquisition algorithm is implemented and the results are used to augment and critique the structure of a large hand-built thesaurus. Extensions and applications to areas such as information retrieval are suggested. 1 Introduction Currently there is much interest in the automatic acquisition of lexical syntax and semantics, with the goal of building up large lexicons for natural language processing. Projects ...}
      \field{title}{{Automatic acquisition of hyponyms from large text corpora}}
      \field{year}{1992}
      \verb{doi}
      \verb 10.3115/992133.992154
      \endverb
    \endentry
    \entry{Singhal2012}{misc}{}
      \name{author}{1}{}{%
        {{hash=e842c74a8f00853e7e3ea135b41b55bd}{%
           family={Singhal},
           familyi={S\bibinitperiod},
           given={Amit},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{e842c74a8f00853e7e3ea135b41b55bd}
      \strng{fullhash}{e842c74a8f00853e7e3ea135b41b55bd}
      \strng{bibnamehash}{e842c74a8f00853e7e3ea135b41b55bd}
      \strng{authorbibnamehash}{e842c74a8f00853e7e3ea135b41b55bd}
      \strng{authornamehash}{e842c74a8f00853e7e3ea135b41b55bd}
      \strng{authorfullhash}{e842c74a8f00853e7e3ea135b41b55bd}
      \field{sortinit}{6}
      \field{sortinithash}{57e57fb8451e7fcfa45d1e069f6d3136}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{{Introducing the Knowledge Graph: things, not strings}}
      \field{year}{2012}
    \endentry
    \entry{SoundUnderstandinggroup2017}{misc}{}
      \name{author}{1}{}{%
        {{hash=3d2dcf87c7fd114cc351a9ff1df8af4d}{%
           family={{Sound Understanding group}},
           familyi={S\bibinitperiod}}}%
      }
      \strng{namehash}{3d2dcf87c7fd114cc351a9ff1df8af4d}
      \strng{fullhash}{3d2dcf87c7fd114cc351a9ff1df8af4d}
      \strng{bibnamehash}{3d2dcf87c7fd114cc351a9ff1df8af4d}
      \strng{authorbibnamehash}{3d2dcf87c7fd114cc351a9ff1df8af4d}
      \strng{authornamehash}{3d2dcf87c7fd114cc351a9ff1df8af4d}
      \strng{authorfullhash}{3d2dcf87c7fd114cc351a9ff1df8af4d}
      \field{sortinit}{6}
      \field{sortinithash}{57e57fb8451e7fcfa45d1e069f6d3136}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{{AudioSet}}
      \field{year}{2017}
    \endentry
    \entry{Hershey2017}{inproceedings}{}
      \name{author}{13}{}{%
        {{hash=50935d083470bdca0850f858e6ad1004}{%
           family={Hershey},
           familyi={H\bibinitperiod},
           given={Shawn},
           giveni={S\bibinitperiod}}}%
        {{hash=b51a895a91010cdbc2ea5e0bf1a06ac5}{%
           family={Chaudhuri},
           familyi={C\bibinitperiod},
           given={Sourish},
           giveni={S\bibinitperiod}}}%
        {{hash=8c2dba7002458c6a90760f98d2e441f1}{%
           family={Ellis},
           familyi={E\bibinitperiod},
           given={Daniel\bibnamedelima P.W.},
           giveni={D\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
        {{hash=91a565291c0e3c0bfd0bafb832bf2bf6}{%
           family={Gemmeke},
           familyi={G\bibinitperiod},
           given={Jort\bibnamedelima F.},
           giveni={J\bibinitperiod\bibinitdelim F\bibinitperiod}}}%
        {{hash=ff1b8e2ee4f94eecee46d9c11902ccae}{%
           family={Jansen},
           familyi={J\bibinitperiod},
           given={Aren},
           giveni={A\bibinitperiod}}}%
        {{hash=2f9e6cba84a097d64d79fb6451a2d846}{%
           family={Moore},
           familyi={M\bibinitperiod},
           given={R.\bibnamedelimi Channing},
           giveni={R\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
        {{hash=fc103b936ff1d8fa40cfdc0b5ee79f52}{%
           family={Plakal},
           familyi={P\bibinitperiod},
           given={Manoj},
           giveni={M\bibinitperiod}}}%
        {{hash=47098052dc9c4b8483027d40d8f7f872}{%
           family={Platt},
           familyi={P\bibinitperiod},
           given={Devin},
           giveni={D\bibinitperiod}}}%
        {{hash=7195887e1fff23cbe4d4606880c8f0da}{%
           family={Saurous},
           familyi={S\bibinitperiod},
           given={Rif\bibnamedelima A.},
           giveni={R\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=d133fd26959a532ebf4f625c31493878}{%
           family={Seybold},
           familyi={S\bibinitperiod},
           given={Bryan},
           giveni={B\bibinitperiod}}}%
        {{hash=af3eb849c7bb824718da580e0514e62b}{%
           family={Slaney},
           familyi={S\bibinitperiod},
           given={Malcolm},
           giveni={M\bibinitperiod}}}%
        {{hash=79806915dad52902a35bf2249c791110}{%
           family={Weiss},
           familyi={W\bibinitperiod},
           given={Ron\bibnamedelima J.},
           giveni={R\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=aed46e9ead80441ec8b5d09f2a94c753}{%
           family={Wilson},
           familyi={W\bibinitperiod},
           given={Kevin},
           giveni={K\bibinitperiod}}}%
      }
      \strng{namehash}{c07131980526e0c1f2c2c20bfaef3e2e}
      \strng{fullhash}{8b57a353f15cb4b8d68b57e6fc08092a}
      \strng{bibnamehash}{93d0f26fe9a4a5f86c5b824b2a2d1aea}
      \strng{authorbibnamehash}{93d0f26fe9a4a5f86c5b824b2a2d1aea}
      \strng{authornamehash}{c07131980526e0c1f2c2c20bfaef3e2e}
      \strng{authorfullhash}{8b57a353f15cb4b8d68b57e6fc08092a}
      \field{sortinit}{6}
      \field{sortinithash}{57e57fb8451e7fcfa45d1e069f6d3136}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Convolutional Neural Networks (CNNs) have proven very effective in image classification and show promise for audio. We use various CNN architectures to classify the soundtracks of a dataset of 70M training videos (5.24 million hours) with 30,871 video-level labels. We examine fully connected Deep Neural Networks (DNNs), AlexNet [1], VGG [2], Inception [3], and ResNet [4]. We investigate varying the size of both training set and label vocabulary, finding that analogs of the CNNs used in image classification do well on our audio classification task, and larger training and label sets help up to a point. A model using embeddings from these classifiers does much better than raw features on the Audio Set [5] Acoustic Event Detection (AED) classification task.}
      \field{booktitle}{ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings}
      \field{eprinttype}{arXiv}
      \field{isbn}{9781509041176}
      \field{issn}{15206149}
      \field{title}{{CNN architectures for large-scale audio classification}}
      \field{year}{2017}
      \verb{doi}
      \verb 10.1109/ICASSP.2017.7952132
      \endverb
      \verb{eprint}
      \verb 1609.09430
      \endverb
      \keyw{Acoustic Event Detection,Acoustic Scene Classification,Convolutional Neural Networks,Deep Neural Networks,Video Classification}
    \endentry
    \entry{VideoUnderstandingGroup2017}{misc}{}
      \name{author}{1}{}{%
        {{hash=e8d2840c46d2b2c276e5e6ca5eb536f6}{%
           family={{Video Understanding Group}},
           familyi={V\bibinitperiod}}}%
      }
      \strng{namehash}{e8d2840c46d2b2c276e5e6ca5eb536f6}
      \strng{fullhash}{e8d2840c46d2b2c276e5e6ca5eb536f6}
      \strng{bibnamehash}{e8d2840c46d2b2c276e5e6ca5eb536f6}
      \strng{authorbibnamehash}{e8d2840c46d2b2c276e5e6ca5eb536f6}
      \strng{authornamehash}{e8d2840c46d2b2c276e5e6ca5eb536f6}
      \strng{authorfullhash}{e8d2840c46d2b2c276e5e6ca5eb536f6}
      \field{sortinit}{6}
      \field{sortinithash}{57e57fb8451e7fcfa45d1e069f6d3136}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{{YouTube-8M}}
      \field{year}{2017}
    \endentry
    \entry{GoogleResearch2015}{article}{}
      \name{author}{1}{}{%
        {{hash=81e64b4a249abb83bba2de4e32497d0c}{%
           family={GoogleResearch},
           familyi={G\bibinitperiod}}}%
      }
      \strng{namehash}{81e64b4a249abb83bba2de4e32497d0c}
      \strng{fullhash}{81e64b4a249abb83bba2de4e32497d0c}
      \strng{bibnamehash}{81e64b4a249abb83bba2de4e32497d0c}
      \strng{authorbibnamehash}{81e64b4a249abb83bba2de4e32497d0c}
      \strng{authornamehash}{81e64b4a249abb83bba2de4e32497d0c}
      \strng{authorfullhash}{81e64b4a249abb83bba2de4e32497d0c}
      \field{sortinit}{6}
      \field{sortinithash}{57e57fb8451e7fcfa45d1e069f6d3136}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{TensorFlow [1] is an interface for expressing machine learn-ing algorithms, and an implementation for executing such al-gorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of hetero-geneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learn-ing systems into production across more than a dozen areas of computer science and other fields, including speech recogni-tion, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the Ten-sorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.}
      \field{eprinttype}{arXiv}
      \field{journaltitle}{Google Research}
      \field{title}{{TensorFlow: Large-scale machine learning on heterogeneous systems}}
      \field{year}{2015}
      \verb{eprint}
      \verb arXiv:1603.04467v2
      \endverb
    \endentry
    \entry{ImageNet2014}{misc}{}
      \name{author}{1}{}{%
        {{hash=b318879f822314efe94c2f096d06465c}{%
           family={ImageNet},
           familyi={I\bibinitperiod}}}%
      }
      \strng{namehash}{b318879f822314efe94c2f096d06465c}
      \strng{fullhash}{b318879f822314efe94c2f096d06465c}
      \strng{bibnamehash}{b318879f822314efe94c2f096d06465c}
      \strng{authorbibnamehash}{b318879f822314efe94c2f096d06465c}
      \strng{authornamehash}{b318879f822314efe94c2f096d06465c}
      \strng{authorfullhash}{b318879f822314efe94c2f096d06465c}
      \field{sortinit}{6}
      \field{sortinithash}{57e57fb8451e7fcfa45d1e069f6d3136}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{{Results for ILSVRC2014}}
      \field{year}{2014}
    \endentry
    \entry{Simonyan2015}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=9d16b7284df92c9adaee86c37ab992df}{%
           family={Simonyan},
           familyi={S\bibinitperiod},
           given={Karen},
           giveni={K\bibinitperiod}}}%
        {{hash=c72fc39e94030f67717052309266a44d}{%
           family={Zisserman},
           familyi={Z\bibinitperiod},
           given={Andrew},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{25d2f3c4577a6632d37f0126cc781232}
      \strng{fullhash}{25d2f3c4577a6632d37f0126cc781232}
      \strng{bibnamehash}{25d2f3c4577a6632d37f0126cc781232}
      \strng{authorbibnamehash}{25d2f3c4577a6632d37f0126cc781232}
      \strng{authornamehash}{25d2f3c4577a6632d37f0126cc781232}
      \strng{authorfullhash}{25d2f3c4577a6632d37f0126cc781232}
      \field{sortinit}{6}
      \field{sortinithash}{57e57fb8451e7fcfa45d1e069f6d3136}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.}
      \field{booktitle}{3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings}
      \field{eprinttype}{arXiv}
      \field{title}{{Very deep convolutional networks for large-scale image recognition}}
      \field{year}{2015}
      \verb{eprint}
      \verb 1409.1556
      \endverb
    \endentry
    \entry{Levoy2012}{misc}{}
      \name{author}{3}{}{%
        {{hash=1315bf00f5fa82534be18df443fc255c}{%
           family={Levoy},
           familyi={L\bibinitperiod},
           given={Marc},
           giveni={M\bibinitperiod}}}%
        {{hash=b92eb745b074c8f7a07a44c26ffc3543}{%
           family={Dektar},
           familyi={D\bibinitperiod},
           given={Katie},
           giveni={K\bibinitperiod}}}%
        {{hash=d9920ffa161bb88a78dfee84636fb7be}{%
           family={Adams},
           familyi={A\bibinitperiod},
           given={Andrew},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{4d74d0f4dd3790b8b4c936a0a028f35e}
      \strng{fullhash}{4d74d0f4dd3790b8b4c936a0a028f35e}
      \strng{bibnamehash}{4d74d0f4dd3790b8b4c936a0a028f35e}
      \strng{authorbibnamehash}{4d74d0f4dd3790b8b4c936a0a028f35e}
      \strng{authornamehash}{4d74d0f4dd3790b8b4c936a0a028f35e}
      \strng{authorfullhash}{4d74d0f4dd3790b8b4c936a0a028f35e}
      \field{sortinit}{7}
      \field{sortinithash}{c818dd9105a2852444fc9f5e145c294e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{{Spatial Convolution}}
      \field{year}{2012}
      \verb{urlraw}
      \verb https://graphics.stanford.edu/courses/cs178/applets/convolution.html
      \endverb
      \verb{url}
      \verb https://graphics.stanford.edu/courses/cs178/applets/convolution.html
      \endverb
    \endentry
    \entry{Jabbar2015}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=24bb986f645e282ec2b5895a9d5da9be}{%
           family={Jabbar},
           familyi={J\bibinitperiod},
           given={Haider\bibnamedelima Khalaf},
           giveni={H\bibinitperiod\bibinitdelim K\bibinitperiod}}}%
        {{hash=23e3d7dfa6202663e7696690db14b050}{%
           family={Khan},
           familyi={K\bibinitperiod},
           given={Rafiqul\bibnamedelima Zaman},
           giveni={R\bibinitperiod\bibinitdelim Z\bibinitperiod}}}%
      }
      \strng{namehash}{907aadb37fc60b0f083f20db847f6e26}
      \strng{fullhash}{907aadb37fc60b0f083f20db847f6e26}
      \strng{bibnamehash}{907aadb37fc60b0f083f20db847f6e26}
      \strng{authorbibnamehash}{907aadb37fc60b0f083f20db847f6e26}
      \strng{authornamehash}{907aadb37fc60b0f083f20db847f6e26}
      \strng{authorfullhash}{907aadb37fc60b0f083f20db847f6e26}
      \field{sortinit}{7}
      \field{sortinithash}{c818dd9105a2852444fc9f5e145c294e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Machine learning is an important task for learning artificial neural networks, and we find in the learning one of the common problems of learning the Artificial Neural Network (ANN) is over-fitting and under-fitting to outlier points. In this paper we performed various methods in avoiding over-fitting and under-fitting; that is penalty and early stopping methods. A comparative study has been presented for the aforementioned methods to evaluate their performance within a range of specific parameters such as; speed of training, over-fitting and under-fitting avoidance, difficulty, capacity, time of training, and their accuracy. Besides these parameters we have included comparison between over-fitting and under-fitting. We found the early stopping method as being better as compared to the penalty method, as it can avoid over- fitting and under-fitting with respect to validation time. Besides we find that Under-fitting neural networks perform poorly on both training and test sets, but Over-fitting networks may do very well on training sets though terribly on test sets.}
      \field{title}{{Methods to Avoid Over-Fitting and Under-Fitting in Supervised Machine Learning (Comparative Study)}}
      \field{year}{2015}
      \verb{doi}
      \verb 10.3850/978-981-09-5247-1_017
      \endverb
    \endentry
    \entry{Kaski2011}{article}{}
      \name{author}{2}{}{%
        {{hash=a4e041fb1be6fc5ec0f43802c95087d2}{%
           family={Kaski},
           familyi={K\bibinitperiod},
           given={Samuel},
           giveni={S\bibinitperiod}}}%
        {{hash=a14051cefe8817b817d29ffd2a5aa7ad}{%
           family={Peltonen},
           familyi={P\bibinitperiod},
           given={Jaakko},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{e3f85673334154343d5bc138a399ef77}
      \strng{fullhash}{e3f85673334154343d5bc138a399ef77}
      \strng{bibnamehash}{e3f85673334154343d5bc138a399ef77}
      \strng{authorbibnamehash}{e3f85673334154343d5bc138a399ef77}
      \strng{authornamehash}{e3f85673334154343d5bc138a399ef77}
      \strng{authorfullhash}{e3f85673334154343d5bc138a399ef77}
      \field{sortinit}{7}
      \field{sortinithash}{c818dd9105a2852444fc9f5e145c294e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Dimensionality reduction is one of the basic operations in the toolbox of data analysts and designers of machine learning and pattern recognition systems. Given a large set of measured variables but few observations, an obvious idea is to reduce the degrees of freedom in the measurements by representing them with a smaller set of more condensed variables. Another reason for reducing the dimensionality is to reduce computational load in further processing. A third reason is visualization. {©} 2006 IEEE.}
      \field{issn}{10535888}
      \field{journaltitle}{IEEE Signal Processing Magazine}
      \field{title}{{Dimensionality reduction for data visualization}}
      \field{year}{2011}
      \verb{doi}
      \verb 10.1109/MSP.2010.940003
      \endverb
      \keyw{Data models,Data visualization,Information retrieval,Machine learning,Manifolds,Probabilistic logic,Visualization}
    \endentry
    \entry{Abdi2010}{misc}{}
      \name{author}{2}{}{%
        {{hash=c580255ca139872e5f15d7fe98f8fcfc}{%
           family={Abdi},
           familyi={A\bibinitperiod},
           given={Herv{é}},
           giveni={H\bibinitperiod}}}%
        {{hash=b973996d01dd9df2c765d72c94a70678}{%
           family={Williams},
           familyi={W\bibinitperiod},
           given={Lynne\bibnamedelima J.},
           giveni={L\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \strng{namehash}{b2f7e8590d9978f96852b19f6522b6ef}
      \strng{fullhash}{b2f7e8590d9978f96852b19f6522b6ef}
      \strng{bibnamehash}{b2f7e8590d9978f96852b19f6522b6ef}
      \strng{authorbibnamehash}{b2f7e8590d9978f96852b19f6522b6ef}
      \strng{authornamehash}{b2f7e8590d9978f96852b19f6522b6ef}
      \strng{authorfullhash}{b2f7e8590d9978f96852b19f6522b6ef}
      \field{sortinit}{7}
      \field{sortinithash}{c818dd9105a2852444fc9f5e145c294e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Principal component analysis (PCA) is amultivariate technique that analyzes a data table in which observations are described by several inter-correlated quantitative dependent variables. Its goal is to extract the important information from the table, to represent it as a set of new orthogonal variables called principal components, and to display the pattern of similarity of the observations and of the variables as points in maps. The quality of the PCA model can be evaluated using cross-validation techniques such as the bootstrap and the jackknife. PCA can be generalized as correspondence analysis (CA) in order to handle qualitative variables and as multiple factor analysis (MFA) in order to handle heterogeneous sets of variables. Mathematically, PCA depends upon the eigen-decomposition of positive semidefinite matrices and upon the singular value decomposition (SVD) of rectangular matrices. {©} 2010 John Wiley {\&} Sons, Inc.}
      \field{booktitle}{Wiley Interdisciplinary Reviews: Computational Statistics}
      \field{issn}{19395108}
      \field{title}{{Principal component analysis}}
      \field{year}{2010}
      \verb{doi}
      \verb 10.1002/wics.101
      \endverb
    \endentry
    \entry{AmatRodrigo2017}{misc}{}
      \name{author}{1}{}{%
        {{hash=ed02ac72acfb015a248e2cba0202677d}{%
           family={{Amat Rodrigo}},
           familyi={A\bibinitperiod},
           given={Joaqu{í}n},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{ed02ac72acfb015a248e2cba0202677d}
      \strng{fullhash}{ed02ac72acfb015a248e2cba0202677d}
      \strng{bibnamehash}{ed02ac72acfb015a248e2cba0202677d}
      \strng{authorbibnamehash}{ed02ac72acfb015a248e2cba0202677d}
      \strng{authornamehash}{ed02ac72acfb015a248e2cba0202677d}
      \strng{authorfullhash}{ed02ac72acfb015a248e2cba0202677d}
      \field{sortinit}{7}
      \field{sortinithash}{c818dd9105a2852444fc9f5e145c294e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{{An{á}lisis de Componentes Principales (Principal Component Analysis, PCA) y t-SNE}}
      \field{year}{2017}
    \endentry
    \entry{Hinton2003}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=9a8750ccdb2a4cf14d2655face1ce016}{%
           family={Hinton},
           familyi={H\bibinitperiod},
           given={Geoffrey},
           giveni={G\bibinitperiod}}}%
        {{hash=709affbec617f6fb034be46e27fa4e34}{%
           family={Roweis},
           familyi={R\bibinitperiod},
           given={Sam},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{ccd03e0d3825ac0bbd9c5ec8e4ff4089}
      \strng{fullhash}{ccd03e0d3825ac0bbd9c5ec8e4ff4089}
      \strng{bibnamehash}{ccd03e0d3825ac0bbd9c5ec8e4ff4089}
      \strng{authorbibnamehash}{ccd03e0d3825ac0bbd9c5ec8e4ff4089}
      \strng{authornamehash}{ccd03e0d3825ac0bbd9c5ec8e4ff4089}
      \strng{authorfullhash}{ccd03e0d3825ac0bbd9c5ec8e4ff4089}
      \field{sortinit}{7}
      \field{sortinithash}{c818dd9105a2852444fc9f5e145c294e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We describe a probabilistic approach to the task of placing objects, described by high-dimensional vectors or by pairwise dissimilarities, in a low-dimensional space in a way that preserves neighbor identities. A Gaussian is centered on each object in the high-dimensional space and the densities under this Gaussian (or the given dissimilarities) are used to define a probability distribution over all the potential neighbors of the object. The aim of the embedding is to approximate this distribution as well as possible when the same operation is performed on the low-dimensional "images" of the objects. A natural cost function is a sum of Kullback-Leibler divergences, one per object, which leads to a simple gradient for adjusting the positions of the low-dimensional images. Unlike other dimensionality reduction methods, this probabilistic framework makes it easy to represent each object by a mixture of widely separated low-dimensional images. This allows ambiguous objects, like the document count vector for the word "bank", to have versions close to the images of both "river" and "finance" without forcing the images of outdoor concepts to be located close to those of corporate concepts.}
      \field{booktitle}{Advances in Neural Information Processing Systems}
      \field{isbn}{0262025507}
      \field{issn}{10495258}
      \field{title}{{Stochastic neighbor embedding}}
      \field{year}{2003}
    \endentry
    \entry{VanDerMaaten2008}{article}{}
      \name{author}{2}{}{%
        {{hash=a33ababcca5b83a7777452957fb2eef2}{%
           family={{Van Der Maaten}},
           familyi={V\bibinitperiod},
           given={Laurens},
           giveni={L\bibinitperiod}}}%
        {{hash=9a8750ccdb2a4cf14d2655face1ce016}{%
           family={Hinton},
           familyi={H\bibinitperiod},
           given={Geoffrey},
           giveni={G\bibinitperiod}}}%
      }
      \strng{namehash}{db0349d236640a0f89628c7e075d2cf7}
      \strng{fullhash}{db0349d236640a0f89628c7e075d2cf7}
      \strng{bibnamehash}{db0349d236640a0f89628c7e075d2cf7}
      \strng{authorbibnamehash}{db0349d236640a0f89628c7e075d2cf7}
      \strng{authornamehash}{db0349d236640a0f89628c7e075d2cf7}
      \strng{authorfullhash}{db0349d236640a0f89628c7e075d2cf7}
      \field{sortinit}{7}
      \field{sortinithash}{c818dd9105a2852444fc9f5e145c294e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We present a new technique called "t-SNE" that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.}
      \field{issn}{15324435}
      \field{journaltitle}{Journal of Machine Learning Research}
      \field{title}{{Visualizing data using t-SNE}}
      \field{year}{2008}
      \keyw{Dimensionality reduction,Embedding algorithms,Manifold learning,Multidimensional scaling,Visualization}
    \endentry
    \entry{Wattenberg2016}{article}{}
      \name{author}{3}{}{%
        {{hash=c66133f96104e3c0dcd2f73e11469ab0}{%
           family={Wattenberg},
           familyi={W\bibinitperiod},
           given={Martin},
           giveni={M\bibinitperiod}}}%
        {{hash=d9ad2ba70a84203f22b0dab9080eedfc}{%
           family={Viegas},
           familyi={V\bibinitperiod},
           given={Fernanda},
           giveni={F\bibinitperiod}}}%
        {{hash=7201a73ba867797477903e1eec2b65e4}{%
           family={Johnson},
           familyi={J\bibinitperiod},
           given={Ian},
           giveni={I\bibinitperiod}}}%
      }
      \strng{namehash}{a59d8b98f80ac712b9ab7240b7403c98}
      \strng{fullhash}{a59d8b98f80ac712b9ab7240b7403c98}
      \strng{bibnamehash}{a59d8b98f80ac712b9ab7240b7403c98}
      \strng{authorbibnamehash}{a59d8b98f80ac712b9ab7240b7403c98}
      \strng{authornamehash}{a59d8b98f80ac712b9ab7240b7403c98}
      \strng{authorfullhash}{a59d8b98f80ac712b9ab7240b7403c98}
      \field{sortinit}{8}
      \field{sortinithash}{07edf88d4ea82509b9c4b4d13f41c452}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Distill}
      \field{title}{{How to Use t-SNE Effectively}}
      \field{year}{2016}
      \verb{doi}
      \verb 10.23915/distill.00002
      \endverb
      \verb{urlraw}
      \verb http://distill.pub/2016/misread-tsne
      \endverb
      \verb{url}
      \verb http://distill.pub/2016/misread-tsne
      \endverb
    \endentry
    \entry{Scikit-learn}{misc}{}
      \name{author}{1}{}{%
        {{hash=ae928b07781d0af9cd222cc0af04ec88}{%
           family={Scikit-learn},
           familyi={S\bibinithyphendelim l\bibinitperiod}}}%
      }
      \strng{namehash}{ae928b07781d0af9cd222cc0af04ec88}
      \strng{fullhash}{ae928b07781d0af9cd222cc0af04ec88}
      \strng{bibnamehash}{ae928b07781d0af9cd222cc0af04ec88}
      \strng{authorbibnamehash}{ae928b07781d0af9cd222cc0af04ec88}
      \strng{authornamehash}{ae928b07781d0af9cd222cc0af04ec88}
      \strng{authorfullhash}{ae928b07781d0af9cd222cc0af04ec88}
      \field{sortinit}{8}
      \field{sortinithash}{07edf88d4ea82509b9c4b4d13f41c452}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{{Metrics and scoring: quantifying the quality of predictions}}
      \verb{urlraw}
      \verb https://scikit-learn.org/stable/modules/model{\_}evaluation.html
      \endverb
      \verb{url}
      \verb https://scikit-learn.org/stable/modules/model%7B%5C_%7Devaluation.html
      \endverb
    \endentry
    \entry{Mishra2018}{misc}{}
      \name{author}{1}{}{%
        {{hash=a9097cfddbfd8854e2aa89fa5f47b0c7}{%
           family={Mishra},
           familyi={M\bibinitperiod},
           given={Aditya},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{a9097cfddbfd8854e2aa89fa5f47b0c7}
      \strng{fullhash}{a9097cfddbfd8854e2aa89fa5f47b0c7}
      \strng{bibnamehash}{a9097cfddbfd8854e2aa89fa5f47b0c7}
      \strng{authorbibnamehash}{a9097cfddbfd8854e2aa89fa5f47b0c7}
      \strng{authornamehash}{a9097cfddbfd8854e2aa89fa5f47b0c7}
      \strng{authorfullhash}{a9097cfddbfd8854e2aa89fa5f47b0c7}
      \field{sortinit}{8}
      \field{sortinithash}{07edf88d4ea82509b9c4b4d13f41c452}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{{Metrics to Evaluate your Machine Learning Algorithm}}
      \field{year}{2018}
      \verb{urlraw}
      \verb https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234
      \endverb
      \verb{url}
      \verb https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234
      \endverb
    \endentry
    \entry{Kruger2018}{article}{}
      \name{author}{1}{}{%
        {{hash=98e13143be3453854b82e7cd0e56ba9d}{%
           family={Kr{ü}ger},
           familyi={K\bibinitperiod},
           given={Frank},
           giveni={F\bibinitperiod}}}%
      }
      \strng{namehash}{98e13143be3453854b82e7cd0e56ba9d}
      \strng{fullhash}{98e13143be3453854b82e7cd0e56ba9d}
      \strng{bibnamehash}{98e13143be3453854b82e7cd0e56ba9d}
      \strng{authorbibnamehash}{98e13143be3453854b82e7cd0e56ba9d}
      \strng{authornamehash}{98e13143be3453854b82e7cd0e56ba9d}
      \strng{authorfullhash}{98e13143be3453854b82e7cd0e56ba9d}
      \field{sortinit}{8}
      \field{sortinithash}{07edf88d4ea82509b9c4b4d13f41c452}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{As computers are becoming more and more a part of our everyday life, the vision of Mark Weiser about ubiquitous computing becomes true. One of the core tasks of such devices is to assist the users in achieving their goals. To do this, the assistive system has to have knowledge about the current situation as well as the user's goal. Such knowledge allows the assistive system to provide strategies to support the users in achieving their goals beginning from the current situation. A GPS navigation device is a simple, yet well known instance of such an assistive system. It recommends a route based on the current location and the manually specified goal. Obviously, effective assistance can only be provided if accurate knowledge about the user's situation and his goal is available. This requires to reason about the actions of the user and to cope with uncertainties that are inherent to human behaviour. The problem becomes even harder, as in real world settings, users cannot be observed directly but through sensors that introduce noise and ambiguity as additional sources of uncertainty. Several applications in the literature showed that probabilistic methods can be used to infer the required information from sensor data. However, massive amounts of training data are needed in order to train classifiers to achieve good recognition rates. This is expensive and prevents trained models from being reused. Recently, researchers employed models of human behaviour in order to reduce the need for training data. These models are generalisable -- they allow the specification of human behaviour without the need for training samples. To this end, these models can be reused in different settings. While these models allow the synthesis of probabilistic models, only few attempts have been made to assess their capabilities with respect to low level sensors such as accelerometers. In fact, different researchers stated that inferring high level knowledge about the user from low level sensor data is an open research topic. To address the above problems, objective of this thesis is to answer the question "how to achieve efficient sensor-based reconstruction of causal structures of human behaviour in order to provide assistance?". To achieve that, in the first step the meaning of this question is analysed and requirements for an inference system are derived. A review of the literature is then conducted and a meta analysis is performed to assess the capabilities of the different approaches and the complexity of their evaluation setting. The results of this analysis show that none of the approaches from the literature satisfies all requirements. To answer the research question, the concept of CCBM is introduced. CCBM allows the specification of human behaviour by means of preconditions and effects and employs Bayesian filtering techniques to reconstruct action sequences from noisy and ambiguous sensor data. Furthermore, a novel approximative inference algorithm -- the Marginal Filter -- is introduced. The Marginal Filter is specifically tailored for categorical state spaces, which are generated by CCBM. To investigate the capabilities with respect to recognition performance and reusability, different experiments are then conducted. Each experiment addresses different aspects of the research question. A detailed analysis of the results of these experiments shows that CCBM is able to achieve good recognition rates. Moreover, the MF is shown to outperform the standard method for approximative Bayesian inference -- the Particle Filter. Furthermore, it is shown that CCBM satisfies the requirements.}
      \field{journaltitle}{ResearchGate}
      \field{title}{{Activity, Context, and Plan Recognition with Computational Causal Behaviour Models}}
      \field{year}{2018}
    \endentry
  \enddatalist
\endrefsection
\endinput

