% !TeX spellcheck = en_GB

	\label{chapter:our-approach-for-avd}
	
	For our final approach when implementing this system of acoustic gender-based violence detection we first need to select the classes we are going to use for the classification so as to obtain the corresponding embedding features with \acrshort{vgg}ish from the audio data, obtained from the database explained in section \ref{section:audioset}. Then, a supervised multiclass classification is performed for the different events and, finally, the soft output from this step is used as an input for a binary classifier so we can differentiate the audio from violent and non-violent.
	
	In this chapter we want to make clear why we think that the \acrshort{vgg}ish network is a good option for our problem in \ref{section:why-vggish}. Then, the procedure when starting to prepare the data and to extract the feature embeddings with this model is included in \ref{section:our-approach}. Also, the demonstration of the similarity between both types of embeddings previously mentioned in \ref{subsection:data-access} is described in \ref{subsection:exploring-differences-between-two-types-of-data-access}. Finally, the last section \ref{section:reason-methodology} includes an explanation of the whole system that we use for the experiments explained in chapter \ref{chapter:experiments}.

\section{Why \acrshort{vgg}ish}
\label{section:why-vggish}

	For our goal, one of the toughest tasks consisted on the selection of data that properly adapted to our problem and its preparation so as to obtain features that allowed us to characterize every acoustic event from a violent point of view. Since our first efforts of finding an available dataset containing enough violent scenes was driving us to a dead end, we decided to take advantage of a huge database which let us rethink the standpoint about how we were going to address the problem. As it was mentioned in subsection \ref{subsection:our-point-of-view}, one of the main questions was how to define the term violence for each victim depending on  her certain situation. After finding the Audio Set database, previously explained in section \ref{section:audioset}, with all its variety of samples, we had a wide range of audio data to work with. This is how we came up with the system explained below in \ref{subsection:violent-classes}.

	At this point, not only we had an idea but we had already found a data resource to start with. However, the issue was related to what kind of features could be extracted in order to categorize events from different nature with a unique violent label. An acceptable conception of the term violence could be expected to cover all kinds of impulsive events such as hits, smashes, gunshots, yells, etc. Also, we would like to introduce sounds that were likely to happen in a domestic environment within a tense atmosphere as children crying, dog barking or glass breaking. We also wanted to take into account the possibility of including other cases not usually considered violent a priori. For example, the sound of keys jangling or the noise produces by a shaver machine. These situations may be too particular and just would be present in few uses, but this is how we understand the problem given the advice of the social sciences experts in the UC3M4Safety team.

	So, our first intention was to apply some audio processing techniques to extract low-level features, as the ones previously explained in \ref{subsection:features-and-methods}. Even though there are plenty of previous works and a lot of tools to work in this way, it was not sure which path should we take in order to decide what features better fitted our task. Apart from this, since the database had such a big volume it would have supposed an enormous cost of time to compute features every time we wanted to try new type of categories. Moving on, by following the advances on finding new level features already mentioned in \ref{subsection:features-and-methods}, we decided to investigate new methods of extraction based on the use of \acrlong{nn} models. Nevertheless, even thought the features obtained in this case had been more appropriated, the time consumption of training a big model was one of the aspects that did not totally convince us.

	The previous selection of Audio Set as our dataset allowed us to get to know the \acrshort{vgg}ish model proposed by Google researchers for feature extraction. This system loads the parameters already learnt from training with another huge dataset as YouTube-8M. This is possible due to apply transfer learning idea, explained above in subsection \ref{subsection:transfer-learning}, that consists of leveraging features or weights extracted from certain models and use them in simpler ways for different tasks \cite{Sarkar2018}, so all the computational cost and training time is not a problem any longer. Finally, we decided to put in practice this pre trained embedding extractor by loading the given parameters so as to obtain our final input representations.

\section{Our approach}
\label{section:our-approach}

	In order to start describing our approach, we will first explain in \ref{subsection:input-data} how we obtained the input data to work with by using the resources previously explained in \ref{section:audioset} and \ref{section:feature-extractor}. Then, we will move to the implementation of the whole model in chapter \ref{chapter:experiments}.

\subsection{Input data}
\label{subsection:input-data}

	Different phases took place when trying to obtain all the necessary data from the YouTube videos specified in the database. We will explain them from the first step of deciding which classes better fit our problem to the last part in which the desired embeddings are achieved.

\subsubsection{Violent classes}
\label{subsection:violent-classes}

	In subsection \ref{subsection:our-point-of-view} we mentioned our idea about giving the victim a choice of defining her own perception of violence, so the final implementation can adapt to her situation in a better way. To do so, we have taken advantage of the ontology provided by the Audio Set creators we explained in subsection \ref{subsecition:ontology}. 

	Our system has been implemented based on the idea of using the parent-children relationship among the different nodes to ease the process of selecting among the 632 classes. It must go through all the branches so as to offer the victim the possibility of choosing any of the audio event categories. However, instead of consider each label individually, this starts the way from the parent classes down to the children ones.
	
	% Flowchart of user interact system
	\begin{wrapfigure}[18]{R}[-18mm]{0.4\textwidth}
		\centering
		\captionsetup{justification=centering}
		\includegraphics[scale=0.4]{select-vio-cls}
		\caption{Flowchart about selecting violent classes}
		\label{fig:mesh3}
	\end{wrapfigure}
	
	
	Let's say we begin from the class "Human sounds" that is the top level of all sounds emitted by humans contained in the dataset. The system will ask the user if she wants to advance in that direction, i.e., to go across the branches that belong to that part of the tree. If the answer is positive, it would go for the next class, that in this case it would be "Human Speech". It will advance this way until there were no children nodes in the actual class. When this happens, the user will be asked to add the label to the record of \textit{violent classes}, that will be saved in a text file so they can be read by other parts of the model further on. If the user does not want to go deeper, she will be asked to add the current class to the record. If the answer is "No", then they system will jump to the next sibling category. The corresponding flowchart is shown in figure \ref{fig:mesh3}.
	
	This way of flowing through the different classes allows to skip those parts that are not related to our problem. For example, as we can see in figure \ref{fig:mesh1}, one of the top parent labels is "Natural sounds", that relates to sounds from weather phenomena. In most of the cases, these classes will not be selected so the whole branch can be skipped. 

\subsubsection{Downloading videos}

	The following step consists in achieving audio files that belong to the chosen labels. For this purpose, we have made use of the \acrshort{csv} files that were explained in subsection \ref{subsection:data-access}. For each included video, we took its ID and build the corresponding YouTube \acrshort{url}. Once downloaded, we trimmed the file considering the onset and offset and, finally, converted it to audio format (\acrshort{wav}). In our script, we can pass as a parameter the identifier of the desired classes in comma-separated format and either the number of videos per class for a balanced set or a total number of downloads for an unbalanced set. However, there might be some errors when obtaining all the data: the two most common cases are due to lack of enough videos of the desired type in the dataset or because the video is not available anymore on YouTube. When this happens, a message is shown to the user.
	
	It it also worth mentioning that throughout the developing of this downloading task, a script has been coded to achieve the whole dataset in both formats, video and audio, for future works. We are not going to specify anything else since this feature was not finally used.

\subsection{Extracting embeddings}
\label{subsection:extracting-embeddings}

	At this point, all the desired data has been already downloaded to extract the embedded features that will be used to train the model. For this part, we have used the \acrshort{vgg}ish network explained in subsection \ref{subsection:vggish}. Since the audio files duration is usually 10 s, and the embedding extractor gives as a result a vector of size $1 \times 128$ for each second, we will obtain a $10 \times 128$ feature matrix composed by values within the range 0 - 255. Therefore, our input feature matrix will have a size of $(number\ of\ audios) \times 10 \times 128$. The corresponding labels will be stored in a vector of size $(number\ of\ samples)$.
	
	There are some points about the data obtained in this step that should be commented. One of them is about the length of the audio files. As it was previously indicated in \ref{subsection:what-is-audioset}, most of them last 10 s because the creators decided to set this duration for the audio events, but this can change if the video is originally shorter. For these situations, since the model is configured to have an input of $10 \times 128$, the embedding matrix of the shorter clip must be padded with zero-rows to achieve the required dimensions. Even though this does not happen very often, there might be some silent segments that will be labelled with the category of the rest of the audio. A solution for this problem explained later in \ref{section:input-data-preparation} was implemented.
	
	The other case is related to what was explained in \ref{subsection:data-access}. For the recent explanation of how to extract the input features from the audio files we have utilized the first manner of accessing the data, i.e., by reading the \acrshort{csv} files with the videos information before downloading. There is a second option of using the already extracted embedding features. However, these do not look exactly the same when comparing them to the ones obtained by running the \acrshort{vgg}ish with the downloaded videos. This difference is because the implementation of the given code differs from their internal production system in computing issues such as underlying libraries in the installation of \acrshort{vgg}ish and hardware equipment. In spite of this, the result in classification tasks are expected to be equivalent. In order to prove this assumption, we decided to try a small system with both kinds of data. The reason for this check is that although, for this work, the development of the methodologies could be made by using the first and faster manner of accessing the data, for the real implementation in the pendant device within the EMPATIA project (out of our scope) only the second way will be feasible.

\subsection{Assessing the differences between the two types of data access}
\label{subsection:exploring-differences-between-two-types-of-data-access}

	In order to check what is explained above in \ref{subsection:extracting-embeddings}, we have decided to run a little experiment in which a toy classification is performed. Also, we wanted to visualize the different features to check if we could appreciate common patterns by using the \acrfull{tsne} algorithm, which will be explained later in this section.

	Our first step consisted in determining a subset extracted from the original dataset. We thought about choosing for this small application a set composed by three classes that could be considered violent and other three that were non-violent. Apart from this, we paid attention to the number of samples per category to pick some classes over others. Finally, we ended up picking up the labels detailed in table \ref{table:4} and a number of 80 samples for each of them, which led us to a total of 480.
	
	% Table for chosen classes
	\begin{table}[h]
		\begin{center}
			\begin{tabular}{||m{7em} | m{23em} ||}
				\hline
				\textbf{Class} & \textbf{Description} \\
				\hline\hline
				Screaming & A sharp, high-pitched human vocalization; often an instinctive action indicating fear, pain, surprise, joy, anger, etc. Verbal content is absent or overwhelmed, unlike Shout and Yell. \\
				\hline
				Crying, sobbing & Sound associated with the shedding of tears in response to an emotional state, arising from slow but erratic inhalation, occasional instances of breath holding and muscular tremor. \\
				\hline
				Gunshot, gunfire & The sound of the discharge of a firearm, or multiple such discharges. \\
				\hline
				Animal & All sound produced by the bodies and actions of non-human animals. \\
				\hline
				Engine & The sound of a machine designed to produce mechanical energy. Combustion engines burn a fuel to create heat, which then creates a force. Electric motors convert electrical energy into mechanical motion. Other classes of engines include pneumatic motors and clockwork motors. \\
				\hline
				Printer & Sounds of a computer peripheral which makes a persistent human readable representation of graphics or text on paper or similar physical media. \\
				\hline
			\end{tabular}
		\end{center}
		\caption{Chosen classes for a small classification. \textit{Screaming}, \textit{Crying, sobbing} and \textit{Gunshot, gunfire} are considered as the violent ones.}
		\label{table:4}
	\end{table}
	
	For the classification task, we decided to create a small \acrshort{cnn} composed by few layers. Since our input data are matrices of shape $10 \times 128$, these were treated as images so the model was built with layers that perform spatial convolution \cite{Levoy2012}. The architecture is detailed in figure \ref{fig:mesh5}. We used a small kernel size of $3 \times 3$, zero padding so as not to change the shape of the output and a activation of function \acrshort{relu}. Two dense\footnote{Dense is a also a common form to refer to \acrlong{fc} layers. Since each neuron is fed with an input from all the neurons in the previous layer we can say that they are densely connected \cite{Rampurawala2019}} layers are added at the end, first one with also \acrshort{relu} as activation function and the second one with \textit{softmax} to perform the classification, and as many filters as the number of classes.
	
	In order to measure the results, since our subset is balanced, we could have evaluated our model by computing the accuracy and the confusion matrix. More information about these metrics can be found in the appendix \ref{appendix:metrics}. In figure \ref{fig:mesh7}, the four confusion matrices corresponding to the training and test phases for both types of data are shown. Also, in table \ref{table:5}, it is included the accuracy for each case. The results are more accurate when training with the embeddings extracted directly from the audio files we downloaded. When obtaining them from the \acrlong{tf} files, they value of the metrics indicate a worse performance. However, we opted for this second manner because the performance difference is not too big and it requires  much less computational cost and time loss.
	
	% Four confussion matrices
	\begin{figure}[htb]
		% Whole figure
		\captionsetup{justification=centering}
		\begin{subfigure}[b]{\textwidth}
			% Start with figure wav
			\centering
			\captionsetup{justification=centering}
			\includegraphics[width=0.5\linewidth]{wav_train}%
			\includegraphics[width=0.5\linewidth]{wav_test}
			\subcaption{Confusion matrices when embeddings are extracted from audio files}
		\end{subfigure}
		\vskip\baselineskip
		% Start with figure tfrecord
		\begin{subfigure}[b]{\textwidth}
			\centering
			\captionsetup{justification=centering}
			\includegraphics[width=0.5\linewidth]{tfrecord_train}%
			\includegraphics[width=0.5\linewidth]{tfrecord_test}
			\subcaption{Confusion matrices when embeddings are taken from .\textit{tfrecord} files}
		\end{subfigure}
		\caption{Confusion matrices}
		\label{fig:mesh7}
	\end{figure}
	
	% Accuracy values train and test
	\begin{table}[h]
		\begin{center}
			\centering
			\begin{tabular}{|| m{7em} | m{7em} | m{7em} ||}
				\hline
				format / subset & \textbf{Train} & \textbf{Test} \\
				\hline\hline
				\textbf{Audio file} & 1.00 & 0.70 \\
				\hline
				\textbf{.\textit{tfrecord}} & 0.98 & 0.64 \\
				\hline
			\end{tabular}
		\end{center}
		\caption{Accuracy values for audio and .\textit{tfrecord} files}
		\label{table:5}
	\end{table}
	
			% Architecture of model of user interact system
	\begin{wrapfigure}[15]{R}{0.4\textwidth}
		\centering
		\captionsetup{justification=centering}
		\includegraphics[scale=0.6]{try-wav-tf}
		\caption{Architecture to see how the different embeddings work}
		\label{fig:mesh5}
	\end{wrapfigure}
	
	For the training phases, it can be appreciated that there may be an overfitting since the accuracy is perfect. This means that the \acrshort{nn} stop improving its capacity of learning how to solve the problem in a certain moment of the training task. Instead, it does learn some behaviour pattern that the training data follows. This impacts negatively in the model since the new data that the system will have to learn from will look different and will not follow these same rules \cite{Jabbar2015}. In spite of this result, we did not give it so much importance since we just wanted to do a quick check of the similarity between the two types of data which can be appreciated due to the similarity of both metrics results.
	
\subsubsection{Dimensionality reduction for visualization}
\label{subsection:dimensionality-reduction-for-visualization}

	Apart from the classification exercise, we wanted to see if by plotting the data samples we were able to identify or appreciate some common patterns. In our problem, each of our samples is characterized by a matrix of features with 128 columns, which means that we were working with data belonging to a high-dimensional space. Visualizing this type of data has always been a case of study for many different fields. Plenty of methods have been published so as to find a solution for this task. Some of the most accepted consist on reducing the dimensionality of the data so this can be transformed from the high-dimensional space to a lower one and can be visualize in a common scatter plot of 2D or 3D. These techniques rely on the idea that within a multivariate sample denoted as $x_i = [x_{i1},..., x_{in}]^T$ and considered to be a point that corresponds to a $n-dimensionality$ space, a $d-dimensionality$ space can be found, so that $d < n$. %, in which the data \doubt{is included}. 
	If this is possible, then the observations can be transformed to this lower dimensional space $d$ without any loses \cite{Kaski2011}. 
	
	One of the most common and well-known dimensionality reduction methods is the one known as \acrfull{pca}. This follows the idea previously explained. It specifically aims at extracting the \textit{important} information from the original data and transform them into a set formed by orthogonal variables which are actually known as principal components. This is done by multiplying the matrix data $X$ by a projection matrix $Q$ that contains the coefficients of the linear combinations that allow the conversion. The projections must be orthogonal to each other and they represent the data maximum variance in descending order, being the first component the one with largest variance \cite{Abdi2010}. In fact, each of the projections correspond to an eigenvector in descending order following the value of the eigenvalue. So, the first component will be the eigenvector with the highest eigenvalue. It has been proved to be one of the most appropriated options in dimensionality reduction nowadays. Its use is completely accepted and it is implemented in many famous software libraries. However, it presents some limitations. One of them consists on just considering linear combinations of the original data. When the relation is non-linear, a dimensionality reduction with this technique may result in a loss of information \cite{AmatRodrigo2017}. 
	
	For these cases, other methods have been developed such as \acrfull{tsne}. This algorithm appears as an extension of the previously published \acrfull{sne} \cite{Hinton2003}. Both are based on the same idea of a new way of measuring the similarity between samples. Instead of comparing two observations, let's call them $x_i$ and $y_j$, by computing the euclidean between them, this is done by calculating the conditional probability $p_{j|i}$ of $x_j$ being picked as a neighbour of $x_i$ considering that the samples belong to a Gaussian distribution centered at $x_i$. This depends on how far the samples are from each other, i.e., it is high when they are close and minimum when there are totally separated \cite{VanDerMaaten2008}. Apart from this, two analogous observations are created in the subspace of lower-dimensionality, $y_i$ and $y_j$, and conditional probability $q_{j|i}$ is computed in this situation. It is important that, for $y_i$ and $y_j$ to be faithful representations of $x_i$ and $x_j$, both conditional probabilities must be equal.
	
	In order to calculate the probabilities, a crucial factor is the variance of the Gaussian distribution. There is not a unique valid choice for this parameter, so \acrshort{tsne} performs a binary search so as to find the optimal one \cite{AmatRodrigo2017}. This is also influenced by what is called the perplexity. This can be defined as an assumption of the number of adjacent neighbours for each point. It is a value that is fixed by the user, but it usually belongs to a range from 5 to 50 \cite{VanDerMaaten2008}. 
	
	There are several considerations that should be known before looking at a representation of data from this algorithm \cite{Wattenberg2016}. Actually, it is not an easy task to understand this kind of plots, since the distance between points in the new subspace are not related to the real euclidean distance, which has been denoted as "The Crowding Problem" \cite{VanDerMaaten2008}. This means that the groups cannot be interpreted as real collections of data in the original dimension. However, in order not to misunderstand the data distribution, a couple of visualizations varying the parameters usually tends to be done so that the conclusions can be based on more than one result. In figures \ref{fig:mesh8} and \ref{fig:mesh52}, we show ten \acrshort{tsne} outputs, five for each type of data for diverse values of perplexity from 2 to 50, respectively.
	
	% Four confussion matrices
	\begin{figure}[H]
		% Whole figure
		\captionsetup{justification=centering}
		\begin{subfigure}[b]{\textwidth}
			% Start with figure wav
			\centering
			\captionsetup{justification=centering}
			\includegraphics[width=0.5\linewidth]{wav-perp-2}%
			\includegraphics[width=0.5\linewidth]{wav-perp-5}%
		\end{subfigure}
		\vskip\baselineskip
		% Start with figure tfrecord
		\begin{subfigure}[b]{\textwidth}
			\centering
			\captionsetup{justification=centering}
			\includegraphics[width=0.5\linewidth]{wav-perp-20}%
			\includegraphics[width=0.5\linewidth]{wav-perp-30}%
		\end{subfigure}
		% 
		\begin{subfigure}[b]{\textwidth}
			\centering
			\captionsetup{justification=centering}
			\includegraphics[width=0.5\linewidth]{wav-perp-50}
		\end{subfigure}
		\caption{t-SNE results for wav format with a legend that shows the labels of the corresponding samples in the original 128D space}
		\label{fig:mesh8}
	\end{figure}
	
	The selection of the perplexity value depends on the number of observations per class \cite{Wattenberg2016}. Since our subset has 80 samples for each category, we can consider that a proper value is 20 or 30. As we can see, a boundary cannot be extracted among the different labels, but we can see some grouping patterns in the data that correspond to the original labelling and helps us to confirm the similarity of the two different given types. It is true that this method should never be used as an algorithm for clustering itself, but it is a good resource as a backing strategy to other results, as in this case. 
	
	% Four confussion matrices
	\begin{figure}[H]
		% Whole figure
		\captionsetup{justification=centering}
		\begin{subfigure}[b]{\textwidth}
			% Start with figure wav
			\centering
			\captionsetup{justification=centering}
			\includegraphics[width=0.5\linewidth]{TF-perp-2}%
			\includegraphics[width=0.5\linewidth]{TF-perp-5}%
		\end{subfigure}
		\vskip\baselineskip
		% Start with figure tfrecord
		\begin{subfigure}[b]{\textwidth}
			\centering
			\captionsetup{justification=centering}
			\includegraphics[width=0.5\linewidth]{TF-perp-20}%
			\includegraphics[width=0.5\linewidth]{TF-perp-30}%
		\end{subfigure}
		% 
		\begin{subfigure}[b]{\textwidth}
			\centering
			\captionsetup{justification=centering}
			\includegraphics[width=0.5\linewidth]{TF-perp-50}
		\end{subfigure}
		\caption{t-SNE results for .\textit{tfrecord} format with a legend that shows the labels of the corresponding samples in the original 128D space}
		\label{fig:mesh52}
	\end{figure}

\section{Our system}
\label{section:our-system}
	
	In this section we are going to make a small explanation of the systems we have implemented. As explained at the beginning of this chapter, our mainly objective in this work is to perform a binary classification in which we can distinguish between violent and non-violent events. Before achieving this final result, a couple of steps take place from which we can make some pre processing of the data and also extract some conclusions about the behaviour of the embeddings selected. More information for the implementation will be found along chapter \ref{chapter:experiments}.
	
	First, we need to select the classes we are going to work with. For that purpose we use the system implemented explained in subsection \ref{subsection:violent-classes}. After these we end up with a couple of violent and non-violent classes that will be used in the classification task.
	
	Then, due to the unbalanced nature of the database we have chosen, some of these classes are more populated than others. We decided to solve this problem by using a data augmentation technique called \acrshort{smote} which generates samples for the minority classes in a synthetic way. A more detailed explanation can be found in \ref{subsection:smote}. We actually did not have a very wide range of possibilities when choosing if a synthetic manner of creating new data was the best idea. Since we are not working with original audio files for this work, we are just taking the already extracted embeddings provided in .\textit{tfrecord} format by the Audio Set website \cite{SoundUnderstandinggroup2017}, we could not apply some of the techniques of deforming or transforming the original signal, as previously explained in \ref{subsection:data-augmentation}. \acrshort{smote} is a common technique for this task with a robust algorithm that seems a good election.
	
	Then, after dividing the data in the corresponding sets for the classification, we wanted to see how was the behaviour of the embeddings in different ways. Then, the final decision was to perform a multiclass classification, as the small experiment described in \ref{subsection:exploring-differences-between-two-types-of-data-access},  with different techniques and see how they work with this type of data. In the following list, the algorithms already explained theoretically in section \ref{section:methodology} and \ref{subsection:ann-cnn} are included with the reason of why choosing them:
	
	\begin{itemize}
		\item We decided to use \acrshort{svm} since we wanted to use a classifier algorithm that differs from neural models. This technique has been used largely in the literature \cite{Fu2011} and seems one of the most appropriated options when performing classification tasks. We also used this algorithm to perform the binary classification.
		\item Another option was to use the simple \acrshort{cnn} model explained in figure \ref{fig:mesh5}. Since the data we are working with is a 2\acrshort{d} matrix for every segment of audio, we wanted to check the result of a convolution operation around each segment. The number of channels indicated in this case is 1, so every instance is considered as a single input "image".
		\item As a last option, we wanted to try the concept of a \acrshort{lstm}, since this method takes as an advantage the fact of the input being a sequence. We could have used simpler \acrshort{rnn} but we decided to try this method since the concept they present are mainly the same.
	\end{itemize}
	
	These three methods are used to perform a multi classification task for all the samples collected within the classes chosen. The output we get are the probabilities for each sample to belong to each class, i.e. the soft outputs. To evaluate the performance of this task, we convert them to hard values by choosing the higher probability. This will be also mentioned later in section \ref{section:input-data-preparation}. Actually, this is due to the softmax layer we include at the end of \acrshort{cnn} and \acrshort{lstm}. For the \acrshort{svm}, the outputs are naturally hard, we just need to indicate that we want to get soft ones in the prediction stage when declaring the model. So, depending on which classification task we are performing, we indicate hard or soft.
	
	For the binary classification, we use the soft probabilities mentioned above as input features for the binary \acrshort{svm}. In this case, we get the hard outputs \textit{0} or \textit{1}, depending if the observation is non-violent or violent. In figure \ref{fig:mesh54}, a block diagram of the whole system is included. This shows the whole treatment of the data since being an audio recording until the output of the classification.
	
	\begin{figure}[H]
		\centering
		\captionsetup{justification=centering}
		\includegraphics[scale=0.5]{block-diagram-whole-model}
		\caption{Block diagram of the whole model including both types of classification. We do not extract the embeddings, but this is a diagram of the whole model considering that we did extract them.}
		\label{fig:mesh54}
	\end{figure} 




















