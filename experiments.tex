% !TeX spellcheck = en_GB
\label{chapter:experiments}

	In this chapter we are going to explain the final models we chose for our work and the input data preparation to feed these models with. For the different solutions, we have used the algorithms previously explained in the methodology section \ref{section:methodology}. Below, it is presented a list including the 6 implementations compared:
	
	\begin{itemize}
		\item \acrshort{svm} classifier for multiclass classification
		\item \acrshort{svm} multiclass + \acrshort{svm} for a final binary classification
		\item \acrshort{lstm} for multiclass classification
		\item \acrshort{lstm} multiclass + \acrshort{svm} for a final binary classification
		\item \acrshort{cnn} for multiclass classification
		\item \acrshort{cnn} multiclass + \acrshort{svm} for a final binary classification
	\end{itemize}
	

\section{Input data preparation}
\label{section:input-data-preparation}

	For the proposed experiments, we have decided to build a small dataset with the embeddings extracted form the .\textit{tfrecord} files that belong to 14 classes: half of them \textit{violent} and the other half, \textit{non-violent}. To find these, we first did a run on the simple user-interaction program that is explained in \ref{subsection:violent-classes} as if we were gender-based violence victims. As we said, a total of 28 classes were selected. From this set, we picked 7 as the violent classes. The other 7 were chosen just by looking at the ontology to provide negative classes.
	
	It is important to mention that this collection of data is composed by samples with just one label assigned. As explained before in \ref{section:audioset}, the Audio Set database is highly unbalanced. Some of the most appropriated classes to be considered violence are scarcely populated. When doing the selection of categories, apart from paying attention to the own meaning of the class, we also checked the number of samples. For the non-violent type, this was not a problem, since we took some of the most populated labels. However, in the violent case, we had to deal with the requirement of representing violence and also having enough observations. So, due to these limitations, we could not obtain a naturally balanced set. In table \ref{table:7}, the 14 selected labels are shown. %that were selected divided in violent and non-violent. 
	In figure \ref{fig:mesh14}, a bar plot shows the number of samples obtained per class right after the selection. 
	
	\begin{table}[ht]
		\centering
		\begin{tabular}{|| m{10em} | m{10em} ||}
			\hline
			\textbf{Violent} & \textbf{Non-violent} \\
			\hline\hline
			Baby cry, infant cry & Printer \\
			\hline
			Slap, smack & Music \\
			\hline
			Screaming & Speech \\
			\hline
			Machine gun & Vehicle \\
			\hline
			Breaking & Animal \\
			\hline
			Slam & Dishes, pots, and pans \\
			\hline
			Yell & Wind \\
			\hline
		\end{tabular}
	\caption{Relation of \textit{violent} and \textit{non-violent} selected classes}
	\label{table:7}
	\end{table}
	
	\begin{figure}[t]
		\centering
		\captionsetup{justification=centering}
		\includegraphics[scale=0.5]{samples-class-experiments}
		\caption{Bar plot that shows the number of samples for each of the selected classes. Clearly, the violent categories are much less populated than the others, that did not have to much any semantic criteria}
		\label{fig:mesh14}
	\end{figure}

	Some initial preprocessing steps were performed before passing the data to the different models. As mentioned in \ref{subsection:extracting-embeddings}, we had to deal with the zero-filling problem, which means that some of the rows of the embeddings matrices are completely zero because the duration of the original video is less than 10s. As a solution, we decided to substitute the zero numbers in all the data with the machine epsilon\footnote{The machine epsilon value is considered the smallest value that satisfies $1 + \epsilon_{match} > 1$. It is the difference between one and the next closest number that is representable as a machine value \cite{Kaw}} value. 
	
	However, in the two models that use \acrshort{svm} for the multiclass classification a different solution was proposed. This algorithm needs the input data matrix to be in the form [$number\ of\ samples\ \times\ number\ of\ features$], which differs from the originally shape of our data, [$number\ of\ samples\ \times\ number\ of\ seconds\ \times\ number\ of\ features$]. For this reason, we decided to reshape our data to the required form which resulted in a matrix of shape [$(number\ of\ samples\ \times\ number\ of\ seconds)\ \times\ number\ of\ features$]. With this conversion, instead of working with full audio instances, the data samples became the seconds of those instances. In order to remove zero data, we first checked the amount of zero-rows in every class. Since it was not a very significant portion of the data, we took them out of the dataset. 
	
	Once we had our data with all non-zero values, we needed to convert the unbalanced set to balanced. First, for the classification task, we divided our data into train, validation and test subsets, using a 20\% for last one. Then, we decided to exclude half of the samples from the most populated classes from the train and validation sets %half of the samples from the most populated classes 
	\footnote{\textit{Speech}, \textit{Music}, \textit{Vehicle} and \textit{Animal}}. In figure \ref{fig:mesh15}, subfigure (a) shows the distribution of data per class in the dataset for the models that use \acrshort{svm} multiclass classifier. In subfigure (b), the distribution for the other four cases is shown. Then, we wanted to generate new data for those with less observations by applying the data augmentation technique \acrshort{smote}, explained in subsection \ref{subsection:smote}, in order to generate samples until equalize the most populated category. Just the train and validation sets were subjected to this conversion process, leaving the test set with the original number of embeddings in their natural a priori state. The final shape of the input data for each of the experiments is shown in table \ref{table:8}.
	
	\begin{figure}[H]
		% Whole figure
		\captionsetup{justification=centering}
		\begin{subfigure}[b]{\textwidth}
			% Start with figure wav
			\centering
			\captionsetup{justification=centering}
			\includegraphics[width=0.5\linewidth]{tr_val_unbal_svm}%
			\includegraphics[width=0.5\linewidth]{tst_unbal_svm}
			\subcaption{This are the resulting subsets after downsampling the most populated classes and removing the zero-rows for the experiments that involve an SVM classifier}
		\end{subfigure}
		\vskip\baselineskip
		% Start with figure tfrecord
		\begin{subfigure}[b]{\textwidth}
			\centering
			\captionsetup{justification=centering}
			\includegraphics[width=0.5\linewidth]{tr_val_unbal}%
			\includegraphics[width=0.5\linewidth]{tst_unbal}
			\subcaption{This are the resulting sets after downsampling the train and validation sets. The test set remains the same after the split.}
		\end{subfigure}
		
		\caption{Number of observations for train, validation and test subsets used in the different experiments}
		\label{fig:mesh15}
	\end{figure}
	
	
	\begin{table}[H]
		\centering
		\begin{tabular}{|| m{5em} | m{9em} | m{9em} | m{9em} ||}
			\hline
			& \textbf{SVM multi and \acrshort{svm} + \acrshort{svm} binary} & \textbf{\acrshort{lstm} multi and \acrshort{lstm} + \acrshort{svm} binary} & \textbf{\acrshort{cnn} multi and \acrshort{cnn} + \acrshort{svm} binary}  \\
			\hline\hline
			\textbf{Train and validation} & 17645 (1993 per class) & 2800 (200 per class) & 2800 (200 per class) \\
			\hline
			\textbf{Test} & 6898 & 699 & 699 \\
			\hline                    
		\end{tabular}
		\caption{Train, validation and test set for different experiments}
		\label{table:8}
	\end{table}

\section{Implementations and results}

	For all the experiments, as mentioned above, the dataset was split by randomly selecting a 20\% of the data for the testing procedure. The other part was divided into train and validation subsets with a \acrlong{kfold} technique, with 10 folds, i.e. 10-fold cross-validation. A more detailed explanation about this resampling technique can be found in appendix \ref{appendix:kfold}. The average and standard deviation of the results from the different folds were obtained for train and validation. Then, a final measurement was performed for the test set. 
	
	The way of checking the model performance was by finding the accuracy and confusion matrix, whose explanation can be found in the Appendix \ref{appendix:metrics}. For the cross-validation procedure, a matrix with the average values\footnote{In the average matrices, the results for each cell are shown just with one decimal in order to make a better and more comfortable visualization. However, the colour bar on the right of the plots must be taking into account since there might be some cells in which the value shown is $0.0$ but it is actually greater.} was obtained and also one for the standard deviation,\footnote{In the standard deviation matrices the results are shown multiplied by $10^{2}$ in order to see the value in the different cells of the matrix} so the estimation of the model is represented for the corresponding subsets. Finally, a last evaluation of the model is performed by checking the accuracy just once with the test set. The testing step is performed by using the model for which the highest value of accuracy was obtained during the validation process.
	
	Also, for the test set, in order to show an orientation of a binary solution performed with the original embeddings, the multiclass matrix result is adapted to a binary approach by building a new matrix that relates the number of predicted labels for the violent and non-violent classes. This way, we want to give an idea of how a binary classification directly performed with data labeled with just these two tags would be ended.
	
	In order to compare the results for the three sets, an error bar plot is also shown for each implementation. This includes the average value of the accuracy for the train and validation set and the accuracy obtained the only time the model was evaluated with the test set. These are shown in a two decimals format for a better visualization. About the standard deviations, they are also included in the plot if they are high enough. Sometimes, the values obtained are too small to make a good visualization of them so they are just commented.
	
	As mentioned above, we have developed a total of six final implementations. Three of them for a multiclass classification problem that consists of predicting the right label for each sample within the fourteen labels already explained. The other three are basically \acrshort{svm} binary classifiers that take as input the output probabilities of the multiclass classifications. The purpose of these three is to adapt the model to the final objective of creating a system to distinguish between violent and non-violent scenarios. In figure \ref{fig:mesh17}, two block diagrams are included in order to represent the flow in the multiclass and binary classification approaches.
	
	\begin{figure}[ht]
		% Whole figure
		\captionsetup{justification=centering}
		\begin{subfigure}[b]{\textwidth}
			% Start with figure wav
			\centering
			\captionsetup{justification=centering}
			\includegraphics[scale=0.65]{multiclass-classifier}
			\caption{General model for the the three multiclass classification approaches.}
		\end{subfigure}
		\vskip\baselineskip
		% Start with figure tfrecord
		\begin{subfigure}[b]{\textwidth}
			\centering
			\captionsetup{justification=centering}
			\includegraphics[scale=0.7]{mutli+svm-binary}
			\caption{Block diagram that shows the concatenation of multiclass classifier and the SVM binary classifier.}
		\end{subfigure}
		
		\caption{Confusion matrices for SVM multiclass classification for train and validation sets}
		\label{fig:mesh17}
	\end{figure}

	As previously explained in \ref{section:our-system}, as output of the multiclass systems, the soft values were obtained. These are the probabilities of a certain sample to belong to each class, being the highest value the one that usually corresponds to the correct predicted label. So this is exactly what we did: we took the highest probability and consider it the predicted category in order to transform the resulting outputs into hard format. This is the \textit{\^{Y}} in the figure. 
	
	For the binary approach, we just took the original soft outputs, \textit{Y multi}, to feed the binary classifier in order to be trained. The model for which the best accuracy value is obtained in the validation set was saved so as to, finally, use it in the testing process.
	
\subsection{Implementation 1: \acrshort{svm} classifier for a multiclass classification}
\label{subsection:implementation-1}

	We decided to establish a baseline by making a first experiment based on a \acrshort{svm} classifier that is used for a multiclass classification. In this case, we wanted to check the results of the performance of one of the most employed techniques in this filed different from \acrshort{nn}. It is true that this method is originally designed for binary problems but, as explained in \ref{subsection:svm}, we took advantage of the multiclass algorithm. To define our model, we used mainly the default parameters. In this work, we have not investigated which type of kernel could better fit our problem, however we found in the literature that the default option of \acrshort{rbf} is the most appropriated for real world applications \cite{Prajapati2010}. 
	
	Below, we can find the results for the different sets. In figure \ref{fig:mesh18}, the confusion matrices for the train and validation sets are shown. In figure \ref{fig:mesh19}, the confusion matrix for the evaluation on the test set is included. Finally, in the bar plot of figure \ref{fig:mesh20}, the accuracy values for the three sets are represented.
	
	\begin{figure}[ht]
		\hspace*{-2cm}
		% Whole figure
		\captionsetup{justification=centering}
		\begin{subfigure}[b]{\textwidth}
			% Start with figure wav
			\captionsetup{justification=centering}
			\includegraphics[width=0.6\linewidth]{svm-multi-cm-tr-av}%
			\includegraphics[width=0.6\linewidth]{svm-multi-cm-tr-std}
			\subcaption{Average and standard deviation for the train set}
		\end{subfigure}
		\vskip\baselineskip
		% Start with figure tfrecord
		\hspace*{-2cm}
		\begin{subfigure}[b]{\textwidth}
			\captionsetup{justification=centering}
			\includegraphics[width=0.6\linewidth]{svm-multi-cm-val-av}%
			\includegraphics[width=0.6\linewidth]{svm-multi-cm-val-std}
			\subcaption{Average and standard deviation for the validation set}
		\end{subfigure}
		
		\caption{Confusion matrices for SVM multiclass classification for train and validation sets}
		\label{fig:mesh18}
	\end{figure}

	\begin{figure}[H]
		\centering
		\captionsetup{justification=centering}
		\includegraphics[width=0.6\linewidth]{svm-multi-cm-tst}
		\caption{Confusion matrix for the test set for the SVM multiclass classification}
		\label{fig:mesh19}
	\end{figure}

	\begin{figure}[H]
		\centering
		\captionsetup{justification=centering}
		\includegraphics[width=0.55\linewidth]{svm-multierrorbar}
		\caption{Accuracy values for the three sets for the SVM multiclass classification}
		\label{fig:mesh20}
	\end{figure}

	The accuracy values are a $89\% \pm 0.087\%$ for the training set, a $79\% \pm 0.56\%$ for the validation and $69\%$ for test. This can also be appreciated in the average confusion matrices. The diagonal for the training and the validation stands out the other cells but they do not present a clear sense of overfitting due to they are not completely uniform. In the test results, we can see greater values in the true positive sections for the most populated classes, mentioned above in section \ref{section:input-data-preparation}. This makes sense since the test set has more samples in these categories. We can consider a good result for this case. The accuracy of the final testing process is not much lower than the one obtained for the validation set. It would be a good option to populate more the violent labels, but we did not want to use synthetic data in the final evaluation.
	
	In the table \ref{table:11}, a binary grouping considering the violent and non-violent classes is included. This was generated by taking the violent samples predicted as violence, even though the prediction did not match the specific class, it just had to be violent. Same for non-violent events. Also, the rest of the observations were grouped as predicted violent label for non-violent samples and vice versa.
	
	\begin{table}[H]
	\begin{center}
		\begin{tabular}{| m{6em} || m{6em} | m{6em} ||}
			\hline
			True/Predicted & \textbf{Non-violent} & \textbf{Violent} \\
			\hline\hline
			\textbf{Non-violent} & 5274 & 485 \\
			\hline
			\textbf{Violent} & 269 & 870 \\
			\hline
		\end{tabular}
	\end{center}
	\caption{Adaptation of the results for SVM multiclass classifier to binary}
	\label{table:11}
	\end{table}

	The accuracy value for this table is $89.07\%$, which was calculated by using the indicated formula in appendix \ref{appendix:metrics}. A grate difference is notable between the true positive section of the non-violent label with respect to the other parts of the matrix. However, we can considered an accurate result since plenty of the samples belong to true positive regions for all the classes in the confusion matrix for the test set in figure \ref{fig:mesh19}.

\subsection{Implementation 2: \acrshort{svm} multiclass classification and \acrshort{svm} binary classification}

	The \acrshort{svm} multiclass model is then incorporated to the previous stage of the preprocessing. So, it predicts the multiclass probability for each value, what can also be interpreted as it is converting the data to a new feature space before the binary classification.
	
	The results in accuracy and confusion matrices for the different evaluations can be found below. Figure \ref{fig:mesh23} includes the average and standard deviation of confusion matrices for train and validation sets. In figure \ref{fig:mesh24}, the confusion matrix for test is included. Then, in figure \ref{fig:mesh25}, the accuracy values for the three sets are shown.


	\begin{figure}[H]
		\hspace*{-2cm}
		\centering
		\captionsetup{justification=centering}
		\includegraphics[width=0.6\linewidth]{svm-svm-cm-tr-av}%
		\includegraphics[width=0.6\linewidth]{svm-svm-cm-val-av}
		\caption{Confusion matrix for the train and validation sets for the SVM multiclass + SVM binary classification}
		\label{fig:mesh23}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\captionsetup{justification=centering}
		\includegraphics[width=0.6\linewidth]{svm-svm-cm-tst}
		\caption{Confusion matrix for the test set for the SVM multiclass + SVM binary classification}
		\label{fig:mesh24}
	\end{figure}

	In this performing model a more accurate interpretation can be obtained from paying attention to both metrics. In the accuracy bar plot, we can see that the values are $96\% \pm 0.024\%$ for the train set, a $96\% \pm 0.021\%$ for the validation set and a $90\%$ for the test. At first, it can be interpreted that the results are really good. This can be due to the high number of samples for a binary classification. Considering the test value of 90\%, we can see that the merit of the good performance is thank to the good classification of the non-violent observations, while the result on differentiating the violent classes is not as good since just the 67\% of the labels were correctly predicted. For the train and validation sets, the results are pretty good for both types, being a 96\% for both cases. Again, the fact of creating a big amount of the samples artificially for the training and validating parts is given a considerable difference in the results respect to the testing. However, this output also allows to see that with a more balanced test set the performance would be more even in the tree sets, as it happened with the multiclass task explained before, and that those observations which belong to the original embeddings show a satisfying outcome.

	\begin{figure}[H]
		\centering
		\captionsetup{justification=centering}
		\includegraphics[width=0.55\linewidth]{svm-binaryerrorbar}
		\caption{Accuracy values for the three sets for the SVM multiclass + SVM binary classification}
		\label{fig:mesh25}
	\end{figure}

	
\subsection{Implementation 3: \acrshort{lstm} for multiclass classification}

	\begin{wrapfigure}[12]{r}{0.35\textwidth}
		\centering
		\captionsetup{justification=centering}
		\includegraphics[width=0.5\linewidth]{lstm}
		\caption{LSTM architecture}
		\label{fig:mesh26}
	\end{wrapfigure}
	
	For this implementation we have decided to use a network composed by a total of three layers: two \acrshort{lstm} and a final \acrlong{fc} with a softmax activation function for the classification task. The first two \acrshort{lstm} layers are set with a drop-out of 0.05 and a recurrent dropout of 0.35. For the first one, the number of units was set to 128, in order not to change the dimensions of its output. In the second layer, it was set to 32 layers so as to reduce the dimensionality before the final prediction in the dense layer. In figure \ref{fig:mesh26}, an schema of the model is shown.
	
	The function minimized in the process was the categorical cross-entropy, which is a common way to evaluate multiclass classification problems. A more detailed explanation of this function can be found in appendix \ref{appendix:categorical-cross-entropy}. With respect to the training process, a number of 50 epochs were used and also a batch size of 32 samples. Also, a \acrshort{nadam} optimizer was used and a learning rate of $2e^{-3}$. An optimization algorithm is in charge of updating the internal parameters for a better performance and to minimize the loss function. The epochs is a hyperparameter that establishes how many times the learning algorithm pass through the entire train set in the training process. Finally, the batch size is the one that defines the number of observations the algorithm works through before assigns the internal parameters an updated value \cite{Browniee2018a}
	
	Below, the results for the classification are included. In figure \ref{fig:mesh27}, the average and standard deviation matrices for the train and validation sets are shown. Then, in figure \ref{fig:mesh28}, the confusion matrix for the testing process is included. In figure \ref{fig:mesh29}, a bar plot shows the accuracy for every set with their standard deviation values.
	
	\begin{figure}[H]
		% Whole figure
		\captionsetup{justification=centering}
		\hspace*{-3.5cm}
		\begin{subfigure}[b]{\textwidth}
			\centering
			\captionsetup{justification=centering}
			\includegraphics[width=0.6\linewidth]{lstm-multi-cm-tr-av}%
			\includegraphics[width=0.6\linewidth]{lstm-multi-cm-tr-std}
			\subcaption{Average and standard deviation for the train set}
		\end{subfigure}
		\vskip\baselineskip
		\hspace*{-3.5cm}
		\begin{subfigure}[b]{\textwidth}
			\centering
			\captionsetup{justification=centering}
			\includegraphics[width=0.6\linewidth]{lstm-multi-cm-val-av}%
			\includegraphics[width=0.6\linewidth]{lstm-multi-cm-val-std}
			\subcaption{Average and standard deviation for the validation set}
		\end{subfigure}
		\caption{Confusion matrices for LSTM multiclass classification for train and validation sets}
		\label{fig:mesh27}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\captionsetup{justification=centering}
		\includegraphics[width=0.6\linewidth]{lstm-multi-cm-tst}
		\caption{Confusion matrix for the test set for the LSTM multiclass classification}
		\label{fig:mesh28}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\captionsetup{justification=centering}
		\includegraphics[width=0.55\linewidth]{lstm-multierrorbar}
		\caption{Accuracy values for the three sets for the LSTM multiclass classification}
		\label{fig:mesh29}
	\end{figure}

	For this model, we obtained an accuracy of $79\% \pm 2.28\%$ for the training set, a $73\% \pm 2.41\%$ for the validation and a $61\%$ for the test. This can be due to the possible confusion regions mainly between classes such as \textit{Screaming} and \textit{Baby cry, infant cry}, and \textit{Wind} and \textit{Vehicle}, whose standard deviation values are the most emphasized. Also, a prominent failing region is the cell \textit{Slam}/\textit{Vehicle}, showing a not expected behaviour. About the \textit{Yell} class, there is a big misclassification that results in a majority of predicted \textit{Music} labels. The other violent classes have fewer correct predictions but also due to the fewer samples that belong to them in the test subset. We can discard heavy overfitting since the values of accuracy in the training and validation sets with respect to the test one are not too different. %away from each other.
	
	In table \ref{table:13}, the adaptation to the binary problem is included. The accuracy for this case is $88.27\%$. Of course, the true positive region with more samples is again the one that corresponds to non-violent. The observations placed in the false negative cell for the violent class can be a consequence of the misclassification of the samples form \textit{Yell} as \textit{Music}. Other cases of failed prediction as \textit{Breaking} for \textit{Slam} do not affect this matrix since the confusion occurs between violent classes.
	
	\begin{table}[H]
		\begin{center}
			\begin{tabular}{| m{6em} || m{6em} | m{6em} ||}
				\hline
				True/Predicted & \textbf{Non-violent} & \textbf{Violent} \\
				\hline\hline
				\textbf{Non-violent} & 517 & 62 \\
				\hline
				\textbf{Violent} & 20 & 100 \\
				\hline
			\end{tabular}
		\end{center}
		\caption{Adaptation of the results for LSTM multiclass classifier to binary}
		\label{table:13}
	\end{table}
	
	
\subsection{Implementation 4: \acrshort{lstm} multiclass + \acrshort{svm} for a final binary classification}
	
	For this case, the predicted probabilities obtained from the \acrshort{lstm} multiclass classifier are passed as input to the \acrshort{svm} for the binary classification. 
	
	The accuracy value for the different sets are included. Figure \ref{fig:mesh30} shows the average and standard deviation of the confusion matrices for the train and validation sets. In figure \ref{fig:mesh31}, the confusion matrix for the test set is also included, and in figure \ref{fig:mesh32}, the error bar can be seen with the accuracy values for each of the sets.
	
	\begin{figure}[H]
		\hspace*{-1.3cm}
		\centering
		\captionsetup{justification=centering}
		\includegraphics[width=0.55\linewidth]{lstm-svm-cm-tr-av}%
		\includegraphics[width=0.55\linewidth]{lstm-svm-cm-val-av}
		\caption{Average and standard deviation for the train and validation set for the LSTM multiclass + SVM binary classification}
		\label{fig:mesh30}
	\end{figure}

	\begin{figure}[H]
		\centering
		\captionsetup{justification=centering}
		\includegraphics[width=0.55\linewidth]{lstm-svm-cm-tst}
		\caption{Confusion matrix for the test set for the LSTM multiclass + SVM binary classification}
		\label{fig:mesh31}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\captionsetup{justification=centering}
		\includegraphics[width=0.55\linewidth]{lstm-svmerrorbar}
		\caption{Accuracy values for the three sets for the LSTM multiclass + SVM binary classification}
		\label{fig:mesh32}
	\end{figure}

	As shown in the bar plot, the values of the accuracy for train and validation are $94\% \pm 0.1\%$ and $94\% \pm 1\%$, respectively. The average matrices are really good as well. The no variation between them could be understood as a symptom of overfitting. However, the accuracy for testing is $90\%$, which is similar to the train and test results. The interesting point can be read in the confusion matrix for the test set. The classification of the violent classes is almost as good as the one for the non-violent ones, despite of the less amount of samples in these categories. 
	
\subsection{Implementation 5: \acrshort{cnn} for multiclass classification}

	For this case, we have implemented a \acrshort{cnn} model based on the architecture presented in figure \ref{fig:mesh5} in subsection \ref{subsection:exploring-differences-between-two-types-of-data-access} for the small experiment to check the similarity of the embeddings from the .\textit{tfrecord} files and the naturally audio files.
	
	In the configuration of the network, the same hyperparameters from implementation 3 are used for the learning process: a categorical cross-entropy for the loss function, a \acrshort{nadam} optimizer, a number of 50 epochs and a batch size of 32 samples. We have kept the same value for this model since the number of observations is the same and the depth of the network is not very great. A learning rate of $2e^{-3}$ was also used. 
	
	In figure \ref{fig:mesh33}, the average and standard deviation matrices for the train and validation sets are shown. In figure \ref{fig:mesh34}, the confusion matrix for the test set. The bar plot in \ref{fig:mesh35}, shows the accuracy for every set with their standard deviation values.
	
	\begin{figure}[H]
		\begin{center}
		% Whole figure
		\captionsetup{justification=centering}
		\begin{subfigure}[b]{\textwidth}
			% Start with figure wav
			\hspace*{-2cm}
			\centering
			\captionsetup{justification=centering}
			\includegraphics[width=0.6\linewidth]{cnn-multi-cm-tr-av}%
			\includegraphics[width=0.6\linewidth]{cnn-multi-cm-tr-std}
			\subcaption{Average and standard deviation for the train set}
		\end{subfigure}
		\vskip\baselineskip
		% Start with figure tfrecord
		\begin{subfigure}[b]{\textwidth}
			\hspace*{-2cm}
			\centering
			\captionsetup{justification=centering}
			\includegraphics[width=0.6\linewidth]{cnn-multi-cm-val-av}%
			\includegraphics[width=0.6\linewidth]{cnn-multi-cm-val-std}
			\subcaption{Average and standard deviation for the validation set}
		\end{subfigure}
		\caption{Confusion matrices for CNN multiclass classification for train and validation sets}
		\label{fig:mesh33}
		\end{center}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\captionsetup{justification=centering}
		\includegraphics[width=0.65\linewidth]{cnn-multi-cm-tst}
		\caption{Confusion matrix for the test set for the CNN multiclass classification}
		\label{fig:mesh34}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\captionsetup{justification=centering}
		\includegraphics[width=0.55\linewidth]{cnn-multierrorbar}
		\caption{Accuracy values for the three sets for the CNN multiclass classification}
		\label{fig:mesh35}
	\end{figure}

	The values of the accuracy obtained for this implementation are $98\% \pm 1\%$ for the train set, $72\% \pm 3\%$ for validation and $65\%$ for the test set. In the error bar in figure \ref{fig:mesh35}, the difference between the red bar for train is considerable meaningful with respect to the other two. Also, the confusion matrix for train shows a very well marked diagonal, while the other two are not that perfect. In the validation set, there are some misclassifications but still the equality among cells is present. Some confusion regions can be found again between \textit{Screaming} and \textit{Baby cry, infant cry} or \textit{Wind} and \textit{Vehicle}. In the test set, we can see that the diagonal does not follow a regular shape, being this one the worst result of the experiments.
	
	As in the previous cases, the table to see the problem from a binary point of view is included in \ref{table:13}. For this case the value of the accuracy is $90\%$. Again this result is kind of delicate. The failed predictions happen between, for example, \textit{Screaming} and \textit{Baby cry, infant cry}, and also between \textit{Breaking} and \textit{Slam}. Errors that this matrix does not show.
	
	\begin{table}[H]
	\begin{center}
		\begin{tabular}{| m{6em} || m{6em} | m{6em} ||}
		\hline
		True/Predicted & \textbf{Non-violent} & \textbf{Violent} \\
		\hline\hline
		\textbf{Non-violent} & 531 & 48 \\
		\hline
		\textbf{Violent} & 22 & 98 \\
		\hline
		\end{tabular}
	\end{center}
	\caption{Adaptation of the results for CNN multiclass classifier to binary}
	\label{table:12}
	\end{table}

\subsection{Implementation 6: \acrshort{cnn} multiclass + \acrshort{svm} for a final binary classification}

	In this last approach we wanted to use the predictions from the \acrshort{cnn} to feed the binary classifier \acrshort{svm}.
	
	Figure \ref{fig:mesh36} shows the average and standard deviation of the confusion matrices for the train and validation sets. Figure \ref{fig:mesh37} includes the confusion matrix for the test set. Finally, figure \ref{fig:mesh38} shows the error bar in which the accuracy values for the three test can be seen.
	
	\begin{figure}[H]
		\hspace*{-1.7cm}
		\centering
		\captionsetup{justification=centering}
		\includegraphics[width=0.6\linewidth]{cnn-svm-cm-tr-av}%
		\includegraphics[width=0.6\linewidth]{cnn-svm-cm-val-av}
		\caption{Average and standard deviation for the train and validation set for the CNN multiclass + SVM binary classification}
		\label{fig:mesh36}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\captionsetup{justification=centering}
		\includegraphics[width=0.6\linewidth]{cnn-svm-cm-tst}
		\caption{Confusion matrix for the test set for the CNN multiclass + SVM binary classification}
		\label{fig:mesh37}
	\end{figure}
	
	
	\begin{figure}[H]
		\centering
		\captionsetup{justification=centering}
		\includegraphics[width=0.55\linewidth]{cnn-svmerrorbar}
		\caption{Accuracy values for the three sets for the CNN multiclass + SVM binary classification}
		\label{fig:mesh38}
	\end{figure}

	The values of the accuracy are $98\% \pm 0.32\%$, $98\% \pm 3\%$ and $90\%$ for train, validation and test set, respectively. Again the results are similar to the ones obtained in the experiment 4. In this case, the classification of the violent classes are good enough as well for the test. However, as seen in experiment 5, there are plenty of misclassifications for the test set but some of them happen between classes of the same kind, so this failures do not have an effect on the binary problem. Also, others have been mixed with categories from the other group, this is would be the reason of the lower output. 
	
\section{Comparison and results}
	
	The idea behind the different implementations was to check the usefulness of the embeddings extracted with \acrshort{vgg}ish for a violent/non-violent classification problem. We wanted to study the performance of different learning techniques to see how this type of data worked with them. To do so, we first ran a typical algorithm for classification that does not involve \acrlong{nn}s, an \acrshort{svm}. Then, we thought it could be a good idea to try neural models with different core concepts to exploit the nature of our data. The dimensions of our input allowed the use of the \acrshort{cnn}, while the sequential character of an audio instance makes the \acrshort{lstm} a possible approach. In this section, we are going to compare the six different results dividing the explanations according to the type classification: multiclass or binary. 
	
	For the multiclass problem, the results are shown again in table \ref{table:9}.
	
	\begin{table}[h!]
		\begin{center}
			\resizebox{0.7\columnwidth}{!}{%
			\begin{tabular}{|| m{5em} | m{7em} | m{7em} | m{7em} ||}
				\hline
				& \textbf{Train} & \textbf{Validation} & \textbf{Test} \\
				\hline\hline
				\textbf{SVM} & $92\% \pm 0.087\%$ & $86\% \pm 0.56\%$ & $72\%$ \\
				\hline
				\textbf{\acrshort{lstm}} & $79\% \pm 2.28\%$ & $73\% \pm 2.41\%$ & $61\%$ \\
				\hline
				\textbf{\acrshort{cnn}} & $98\% \pm 1\%$ & $72\% \pm 3\%$ & $65\%$ \\
				\hline
			\end{tabular}
		}
		\end{center}
		\caption{Accuracy results for the three different algorithms and the three sets for the multiclass approach}
		\label{table:9}
	\end{table}

	Initially, the best performance for this case is the one done by with \acrshort{svm}. It is expectable that this algorithm achieves a good result due to the way it works. Basically, it expresses the different data samples in a feature space in which the optimal boundary is computed, as explained in subsection \ref{subsection:svm}. However, this one is not totally comparable with the other two \acrshort{lstm} and \acrshort{cnn}, since the number of observations to feed the model is much greater because of the conversions explained in the subsection \ref{section:input-data-preparation}. This way, the method has more examples to learn how the different classes are distributed.
	
	For the \acrshort{cnn} approach, the results are maybe the least satisfactory. The system presents a clear overfitting due to the big difference of the results for within the three sets. It is learning the data in the training stage and not being able to generalize, so it does not perform a good enough classification for validation and test. Also, considering the errors in figure \ref{fig:mesh34}, the mismatches are the greatest of the three experiments.
	
	In the \acrshort{lstm} approach, the results are numerically the worst but also the ones that make more sense. The system does not present an overfitting since the accuracies for validation and test are similar to the one for train. Also, most of the errors that can be read in the confusion matrix in figure \ref{fig:mesh31} are understandable since the audio data for those classes involved in the confusion regions may look similar. For example, a recording that belongs to \textit{Baby cry, infant cry} is likely to be similar to one from \textit{Screaming}. The same explanation could be used for \textit{Wind} and \textit{Vehicle}, since they usually have sounds that belong to scenes in a rush. Just some cases are more weird as the misclassification of \textit{Yell} for \textit{Music}, or \textit{Screaming} for \textit{Slap, smack}.
	
	Let's look at some violent categories to check their performance. 
	
	\begin{itemize}
		\item The error when predicting \textit{Music} for \textit{Yell} samples appears in all the models in a more or less clear way. Maybe, the original audio data from which the embeddings were extracted had common similarities. It would not be weird that many samples from \textit{Music} had the presence of shouts and yells.
		\item The confusion region between \textit{Screaming} and \textit{Baby cry, infant cry} comes out in all matrices. This makes sense, since the semantics of these two type of audio events is quite similar.
		\item The predictions for \textit{Slam} vary in all the models obtaining half of the samples correctly labeled for the \acrshort{svm} approach. In the case of the \acrshort{lstm} the true positive region just counts with $30\%$ of the data but the rest is misclassified among other violent categories. For the \acrshort{cnn}, this class is a complete failure being most of it classified as \textit{Speech}.
		\item In the case of \textit{Machine gun} it has a pretty good performance for all the three models, getting a $80\%$ for \acrshort{lstm}, a $70\%$ for \acrshort{cnn} and a $60\%$ in the case of \acrshort{svm} in the true positive regions. The rest of them are shared uniformly along all the other classes.
		\item In the case of \textit{Slap, smack}, it is predicted in a similar way for all the models as well, achieving a $50\%$ of the samples right classified.
		\item The class \textit{Breaking} has not showed a really good result for \acrshort{lstm} and \acrshort{cnn} being $40\%$ of the samples predicted as \textit{Slam}. However, for \acrshort{svm} a $30\%$ was set as true positives being the rest of the samples uniformly distributed along the other classes, mainly in \textit{Animal} and \textit{Printer}.
	\end{itemize}
	
   The conclusion we can extract from this comparison is that the models which shows much better results are the \acrshort{lstm} and \acrshort{svm}. We can see that the classes have similar results for all the methods being the \acrshort{svm} the one that stands out. In the \acrshort{lstm}, it calls the attention the region between \textit{Wind}, \textit{Vehicle} and \textit{Slam} and also the amount of samples from \textit{Yell} predicted as \textit{Music}. We could establish that maybe the \acrshort{cnn} is not the best option due to the broken diagonal that it presents. Also, a general good performance of the non-violent classes appears in all the matrices. This may be due to the more number of samples that belong to these classes within in the test set. It is worth to mention that the quality of the labels previously described in \ref{subsection:labels-quality}, can also affect the resulting values. For example, \textit{Yell} presents a quality of the $50\%$ and also \textit{Slam} has a label quality of $20\%$. This could be taken into account for future implementations.
   
   For the binary problem, the results are included in table \ref{table:10}. Also the confusion matrices for test are shown again in figure \ref{fig:mesh46}, since they are really relevant for the final explanation.
	
	\begin{table}[h!]
		\begin{center}
			\resizebox{0.7\columnwidth}{!}{%
			\begin{tabular}{|| m{5em} | m{7em} | m{7em} | m{7em} ||}
				\hline
				& \textbf{Train} & \textbf{Validation} & \textbf{Test} \\
				\hline\hline
				\textbf{SVM} & $96\% \pm 0.024\%$ & $96\% \pm 0.021\%$ & $90\%$ \\
				\hline
				\textbf{\acrshort{lstm}} & $ 94\% \pm 0.1\%$ & $95\% \pm 1\%$ & $90\%$ \\
				\hline
				\textbf{\acrshort{cnn}} & $98\% \pm 0.32\%$ & $98\% \pm 3\%$ & $90\%$ \\
				\hline
			\end{tabular}%
		}
		\end{center}
		\caption{Accuracy results for the three different algorithms and the three sets for the binary approach}
		\label{table:10}
	\end{table}
	
	\begin{figure}[H]
		\begin{center}
			% Whole figure
			\captionsetup{justification=centering}
			\begin{subfigure}[b]{\textwidth}
				\centering
				\captionsetup{justification=centering}
				\includegraphics[scale=0.45]{svm-svm-cm-tst}
				\subcaption{SVM multiclass + SVM binary}
			\end{subfigure}
			\vskip\baselineskip
			% 
			\begin{subfigure}[b]{\textwidth}
				\centering
				\captionsetup{justification=centering}
				\includegraphics[scale=0.45]{lstm-svm-cm-tst}
				\subcaption{LSTM multiclass + SVM binary}
			\end{subfigure}
			%
			\begin{subfigure}[b]{\textwidth}
				\centering
				\captionsetup{justification=centering}
				\includegraphics[scale=0.45]{cnn-svm-cm-tst}
				\subcaption{CNN multiclass + SVM binary}
			\end{subfigure}
			\caption{Test confusion matrices}
			\label{fig:mesh46}
		\end{center}
	\end{figure}

	The actual solution of the problem would be this binary classification that allows us to distinguish between non-violent and violent events. The results apparently are really good if we look at the accuracy values, but the interpretability is much more clear when reading the confusion matrices. 
	
	In order to interpret the result for the \acrshort{svm}, we can read the confusion matrix in figure \ref{fig:mesh19} to check how the multiclass classification performed and related to the output obtained from the binary classifier. For this case, certain classes have can be predicted within the range of false negatives as non-violent categories, such as \textit{Yell} confused with \textit{Music}. Also, a couple of categories are predicted as \textit{Speech} and some others as \textit{Animal}. If we interpret this errors in a binary context, we can say that the system is taking advantage of the higher a priory of non-violent classes without learning the true distinctions. %they can be resumed by saying that violent labels are predicted as non-violent. 
	This fact is reflected in the only $67\%$ of true positives for the violent label in figure \ref{fig:mesh46} \textit{(a)}. 
	
	In the \acrshort{cnn} approach, the results are numerically smaller but the classification of the violent instances is much better. Almost a $80\%$ of these samples were correctly classified. Again, if we look at figure \ref{fig:mesh34}, some classes are confused within the two different binary labels. A $30\%$ of instances from \textit{Slam} have been wrongly identified as \textit{Speech}. The same happens with \textit{Yell} and \textit{Music} as in the case of \acrshort{svm}. So, these can affect the output of this binary approach and the result of the test confusion matrix. There is also a big mistake in predicting \textit{Slam} for samples that belong to \textit{Breaking}, but this fact does not involve errors in the binary classification.
	
	Finally, the \acrshort{lstm} is the method for which a better result is obtained in this problem. In the confusion matrix in figure \ref{fig:mesh28}, as explained above, we can see some misclassifications but they happen between data of the same type, except between \textit{Yell} and \textit{Music}. In a real application this would be a more reasonable result, in order to detect a violent situation without depending too much on the exact event occurred just considering if it is violent or not. This is why the percentage of true positive samples for the violent class is the best for this model with a $86\%$. We can say that this is the best approach from all of our experiments. %the final experiment of the work is the one that involves \acrshort{lstm} as multiclass classifier.

		
	